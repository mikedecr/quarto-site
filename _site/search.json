[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "welcome",
    "section": "",
    "text": "I used to be a political scientist.  Now I do statistics and software development in high-frequency / low-latency trading."
  },
  {
    "objectID": "blog/latex_colors/index.html",
    "href": "blog/latex_colors/index.html",
    "title": "Treating \\(\\mathrm{\\LaTeX}\\) like a programming language: the case of color-coded equations",
    "section": "",
    "text": "\\(\\newcommand{latex}{\\mathrm{\\LaTeX}}\\)\n\\(\\latex\\) can be a headache. The syntax is clunky. What seems like “normal” usage requires a lot of frustrating patterns. I see academics and other researchers complain about it, and I usually agree with the spirit of the complaints.\nBut I don’t always agree. I often feel like many researchers don’t get the most out of \\(\\latex\\) (hereafter “LaTeX” or “TeX”). When I used to write more LaTeX in graduate school, I knew I didn’t. I mean, I was superficially fine at it. I was capable of injecting statistical results into documents, managing citations and cross-references, and so on. But TeX requires a lot of shoeleather work—boilerplate code to itemize and enumerate, manage environments, typeset math—and I didn’t know how to criticize that very well. Like many others, I would repeating my code a lot, get trapped in irritating patterns, and feel the age of the language constantly.\nBut you get older, you (hopefully) get better at programming, and you realize what you were being silly about. LaTeX is a programming language. You can do ordinary programming language things with it, like save variables and write functions. And then you can turn those variables and functions into interfaces that let you work with nicer abstractions with greater efficiency. And these interfaces can make LaTeX more joyful.\nThis has been on my mind for a little while, but recently I was talking with Jordan Nafa about color-coding different parts of some equations. So that will be our example for this post: color-coding an equation. We require no deep LaTeX expertise, external packages, or complicated programming concepts. Just a little ordinary programming thinking to turn LaTeX’s built-in color tools into something more practical and friendly."
  },
  {
    "objectID": "blog/latex_colors/index.html#color-fundamentals-in-latex",
    "href": "blog/latex_colors/index.html#color-fundamentals-in-latex",
    "title": "Treating \\(\\mathrm{\\LaTeX}\\) like a programming language: the case of color-coded equations",
    "section": "Color fundamentals in LaTeX",
    "text": "Color fundamentals in LaTeX\nLike many things in LaTeX, the built-in experience of color-control is a little clunky. You can change text color with \\color{}, which naturally takes some color argument. The language provides builtin keywords like red and blue, which are a bit harsh on the eyes, but others like violet, teal, and maroon are fine. But the color options aren’t the problem. The problem is the interface, which works like this.\nAny time you change a color, you change it indefinitely (up to some scope change, more on that in a second). Here is an example:\n\nWe start with some normal text.\n\\color{red} And now the text is red indefinitely.\n\nWhen I render this in the browser:\n\\[\n\\text{\n    We start with some normal text.\n    \\color{red} And now the text is red indefinitely.\n}\n\\]\nYou may have encountered similar behavior with altering text size.\n\nWe start with some normal text.\n\\huge And now the text is huge indefinitely.\n\n\\[\n\\text{\n    We start with some normal text.\n    \\huge And now the text is huge indefinitely.\n}\n\\]\nWe can extert some control over the “indefinite” application of these settings by introducing some scope, for instance with curly braces.\n\nWe start with some normal text. \n{ \\color{red} But the } redness is { \\color{red} contained inside curly braces}.\n\n\\[\n\\text{\nWe start with some normal text.\n{ \\color{red} But the } redness is { \\color{red} contained inside curly braces}.\n}\n\\]\nThis isn’t the worst but it doesn’t feel too comfortable. It would feel better to control colors more like this:\n\\red{But the} redness is \\red{contained}\nwhich makes the whole experience feel more intuitive, function-oriented and declarative. We will work toward something that feels similar (not identical) to this."
  },
  {
    "objectID": "blog/latex_colors/index.html#doing-better-with-an-equation-example",
    "href": "blog/latex_colors/index.html#doing-better-with-an-equation-example",
    "title": "Treating \\(\\mathrm{\\LaTeX}\\) like a programming language: the case of color-coded equations",
    "section": "Doing better, with an equation example",
    "text": "Doing better, with an equation example\nWe will work with an example equation that has certain terms that we want make different colors. More specifically, we want to map certain “semantic features” of the equation to certain colors in a consistent way. And we would like the interface to be minimally burdensome; I don’t want to have to type (or think) too much to get nice effects.\nLet’s meet our equation and its “semantics”. By “semantics” I mean that the terms in the equation have some additional meaning in addition to the math itself. In this example the semantics refer to the levels at which terms are indexed. We collect data on some individuals subscripted \\(i\\), who are located within groups \\(g\\), and measured across time units subscripted \\(t\\). We model some outcome \\(y_{it}\\) as varying across individuals within groups and over time, \\[\\begin{align}\n    y_{it} &= \\alpha + \\mu_{i} + \\gamma_{g[i]} + \\tau_{t} + \\varepsilon_{it}\n\\end{align}\\] where \\(\\alpha\\) is a constant term, \\(\\mu_{i}\\) is a term that is fixed for an individual \\(i\\) across repeated observations, \\(\\gamma_{g[i]}\\) is a group-specific that is fixed across time for all \\(i\\) in group \\(g\\), \\(\\tau_{t}\\) is a time-unit effect that is fixed for all individuals. and \\(\\varepsilon_{it}\\) is random error. These units of measurement—units, groups, time periods—are the semantics that we want to map to colors.\nWe have already identified one problem: we don’t want colors to apply indefinitely. This means that in order to turn “off” a color, I either have to explicitly call \\color{black} again, or I have to scope the color e.g. with curly braces.\n\n% back to black\ny_{it} &= \\alpha + \\color{violet} \\mu_{i} \\color{black} \n          + \\gamma_{g[i]} + \\tau_{t} + \\varepsilon_{it} \\\\\n\n% use scope\ny_{it} &= \\alpha + {\\color{violet} \\mu_{i}} \n          + \\gamma_{g[i]} + \\tau_{t} + \\varepsilon_{it}\n\n\\[\\begin{align}\n    y_{it} &= \\alpha + \\color{violet} \\mu_{i} \\color{black} + \\gamma_{g[i]} + \\tau_{t} + \\varepsilon_{it} \\\\\n    y_{it} &= \\alpha + {\\color{violet} \\mu_{i}} + \\gamma_{g[i]} + \\tau_{t} + \\varepsilon_{it}\n\\end{align}\\]\nUsing the curly braces is definitely safer than \\color{black}; I don’t want to assume that we always want to return to black. But managing the curly braces yourself can be cumbersome if you aren’t used to writing LaTeX that way already. I don’t write with that style, so I don’t want to burden myself with unusual patterns.1\nSo to improve things, we will introduce a function that, at first, will not feel like much of an improvement. But we discuss it to highlight both how we can modify interfaces with pretty simple tools and why we may want to do that. So, consider a function called setcolor, which takes two arguments: a color code and the text you want apply the color to locally.\n\n% notice the extra {} braces in the definition\n\\newcommand{\\setcolor}[2]{ {\\color{#1} {#2}} } \n\n% apply function to the equation\ny_{it} &= \\alpha + \\setcolor{violet}{\\mu_{i}} \n          + \\tau_{t} + \\varepsilon_{it}\n\n\\[\\begin{align}\n    \\newcommand{\\setcolor}[2]{ {\\color{#1} {#2}} }\n    y_{it} &= \\alpha + \\setcolor{violet}{\\mu_{i}} + \\gamma_{g[i]} + \\tau_{t} + \\varepsilon_{it}\n\\end{align}\\]\nWhy do I say that this function may not feel like much of an improvement? It has some drawbacks: it really isn’t any “faster” to type \\setcolor{violet}{\\mu_{i}} than it is to type {\\color{violet} \\mu_{i}}. It actually has more characters and just as many curly braces. But it is better in at least two important respects that we should care about when we write code. One, we made the problem of locally scoping the color inherent to the function instead of being procedurally managed ad hoc outside of the function. This is good because it makes the whole thing more bug-proof. It also makes the interface feel more naturally function-like: we achieve a coherent result by calling a function with a predictable interface, then our work is done. No managing other special characters in the language as a side concern. It is easier to remember one thing (use a function) than it is to remember two things (use a function oh and also manage the weird scope). So we get a safer function with a more recognizable interface. Not so bad!\nBut we aren’t done there. We complete the interface by using this function to map semantics to colors directly. We create a function called \\unitfx{} which applies the same color to any term in the equation that semantically refers to unit-level effects. Same for a functions called groupfx{}, \\timefx{}, and so on. We also throw in a generic \\param{} function for other terms.\n\n\\newcommand{unitfx}[1]{\\setcolor{violet}{#1}}\n\\newcommand{groupfx}[1]{\\setcolor{green}{#1}}\n\\newcommand{timefx}[1]{\\setcolor{orange}{#1}}\n\\newcommand{param}[1]{\\setcolor{maroon}{#1}}\n\nSo as long as we define these color commands in one place, all the hard work is done. All downstream calls to these functions are simple. Just wrap a term in the equation inside of the function corresponding to its semantic.\n\ny_{it} &= \\param{\\alpha} + \\unitfx{\\mu_{i}}\n          + \\groupfx{\\gamma_{g[i]}} + \\timefx{\\tau_{t}}\n          + \\param{\\varepsilon_{it}}\n\n\\[\\begin{align}\n    \\newcommand{unitfx}[1]{\\setcolor{violet}{#1}}\n    \\newcommand{groupfx}[1]{\\setcolor{green}{#1}}\n    \\newcommand{timefx}[1]{\\setcolor{orange}{#1}}\n    \\newcommand{param}[1]{\\setcolor{maroon}{#1}}\n    y_{it} &= \\param{\\alpha} + \\unitfx{\\mu_{i}} + \\groupfx{\\gamma_{g[i]}} + \\timefx{\\tau_{t}} + \\param{\\varepsilon_{it}}\n\\end{align}\\]"
  },
  {
    "objectID": "blog/latex_colors/index.html#conclusion",
    "href": "blog/latex_colors/index.html#conclusion",
    "title": "Treating \\(\\mathrm{\\LaTeX}\\) like a programming language: the case of color-coded equations",
    "section": "Conclusion",
    "text": "Conclusion\nSo that’s it. Our solution meaningfully improves the color experience in LaTeX using just five lines of code: one to create a helper function and four more to create some color mappings. The helper function let us change the interface to color control in the general case. And the color mappings let us apply the new interface to create simple key-value pairs that map a semantic to a color. And bonus: the interface is also safer because managing the scope of a color modification requires no more work for the user.\nWe also do this without introducing any dependencies. We could still load outside packages to access more color values, but that choice doesn’t bear on the general interface or vice-versa. That modularity is a good design feature.\nAnd all because we thought about a LaTeX problem like a programming problem."
  },
  {
    "objectID": "blog/latex_colors/index.html#footnotes",
    "href": "blog/latex_colors/index.html#footnotes",
    "title": "Treating \\(\\mathrm{\\LaTeX}\\) like a programming language: the case of color-coded equations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlthough I could totally see, for example, Lisp users feeling quite comfortable with that style. It feels more like the (function arg1 arg2) syntax.↩︎"
  },
  {
    "objectID": "blog/nonflat_implied_priors/index.html",
    "href": "blog/nonflat_implied_priors/index.html",
    "title": "Non-flat implications of flat priors",
    "section": "",
    "text": "When many researchers first encounter Bayesian methods, they are nervous that their choice of prior distribution will ruin their posterior inferences. Estimation under non-Bayesian methods feels “automatic”, but Bayesian methods bear this additional risk of bad priors.1 In this new uncertain territory, it initially feels safer to prefer flatter, more diffuse priors in your model. How bad can flat priors be if, in the extreme, you merely get the same answer that you would have gotten without Bayes?\nThis blog post discuss some areas where this reasoning breaks down. One, we discuss the fact that if some parameter has a prior, functions of that parameter have their own “implied” priors. And depending on the nature of those functions, the implied prior may take a form that the researcher does not expect when they first specify the prior on the original parameter.2 A flat prior in one part of the model often leads to non-flat priors over downstream quantities. The default tendency to prefer flatness for all priors cannot avoid this. Two, we see how the parameters and functions-on-parameters in our model depend on the parameterization of the model. Different parameterizations naturally expose different parameters, and some parameters are easier to reason about than others. Parameterization affects whether a prior’s “flatness” is at all related to a prior’s “informativeness”."
  },
  {
    "objectID": "blog/nonflat_implied_priors/index.html#implied-priors",
    "href": "blog/nonflat_implied_priors/index.html#implied-priors",
    "title": "Non-flat implications of flat priors",
    "section": "Implied priors",
    "text": "Implied priors\nLet’s say we have a model parameter, \\(\\theta\\), and we represent our uncertainty about this parameter with a probability distribution. As far as our model is concerned, therefore, \\(\\theta\\) is a random variable. If we pass \\(\\theta\\) to some function \\(f\\), then the value \\(f(\\theta)\\) is also a random variable with its own distribution. We can call the distribution of \\(f(\\theta)\\) its “implied prior”. You may hear others call it an “induced” prior.\nWe can construct an easy example by linearly transforming a Normal/Gaussian distribution. Even if you don’t do Bayesian statistics, the math here will probably be recognizable nonetheless, so it’s a good example for building intuition. Say that \\(\\theta\\) is a standard Normal variable (mean of 0, standard deviation of 1) and then define \\(f(\\theta) = \\mu + \\sigma \\theta\\). The result will be that \\(f(\\theta)\\) is distributed Normal with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). We can simulate this to convince ourselves that this is true. I will use R for this, fixing the values of \\(\\mu\\) and \\(\\sigma\\).\nFirst, I will load some packages.\n\nlibrary(\"latex2exp\")\nlibrary(\"magrittr\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\n\n\n# unlikely to work on your machine :)\ntheme_set(theme_gray(base_family = \"InconsolataGo Nerd Font Complete\"))\n\n\n# helpful theme functions\nremove_y_axis &lt;- function() {\n    theme(axis.text.y = element_blank(),\n          axis.ticks.y = element_blank())\n}\nremove_grid &lt;- function() {\n    theme(panel.grid = element_blank())\n}\n\nHere we create the function f, generate some Normal draws, and then apply f to those draws.\n\nsigma &lt;- 1 / 2\nmu &lt;- 6\nf &lt;- function(x) { mu + sigma * x }\n\nd = tibble::tibble(\n    theta = rnorm(10000, mean = 0, sd = 1),\n    f_theta = f(theta)\n)\n\nprint(d)\n## # A tibble: 10,000 × 2\n##      theta f_theta\n##      &lt;dbl&gt;   &lt;dbl&gt;\n##  1  0.0593    6.03\n##  2 -0.268     5.87\n##  3 -0.987     5.51\n##  4 -1.06      5.47\n##  5  0.0496    6.02\n##  6  1.39      6.69\n##  7  0.287     6.14\n##  8  0.226     6.11\n##  9 -1.39      5.30\n## 10  0.103     6.05\n## # ℹ 9,990 more rows\n\nWe plot the Normal draws alongside the implied distribution of f_theta.\n\n# partial histogram w/ fixed args\nphist &lt;- purrr::partial(geom_histogram, bins = 100, boundary = 0)\n# this alone is sufficient to make the point\nhistograms &lt;- ggplot(d) +\n    phist(aes(x = theta), fill = \"gray\") +\n    phist(aes(x = f_theta), fill = \"violet\")\n\n# but I make it pretty with labels and things\nhistograms +\n    annotate(\"text\", x = 0, y = 200, label = TeX('\\\\theta')) +\n    annotate(\"text\", x = 6, y = 200, label = TeX(\"f(\\\\theta)\")) +\n    annotate(\"segment\", x = 0.25, xend = 5.5, y = 150, yend = 150,\n             arrow = arrow(length = unit(0.3, 'cm'))) +\n    annotate(\"text\", x = 3, y = 150, label = \"f\", vjust = -0.5) +\n    labs(x = NULL, y = \"Prior samples\") +\n    scale_x_continuous(breaks = seq(-10, 10, 2))\n\n\n\n\n\n\n\n\nFor Normal random variables, linear transformation have familiar effects. Add a constant, the mean increases by the same constant. Multiply by a constant, the standard deviation increases by the same factor.\nThis paricular implied prior is something that Bayesians routinely take advantage of. It is common to want a \\(X = \\mathrm{Normal}(\\mu, \\sigma)\\) prior where the values of \\(\\mu\\) and \\(\\sigma\\) are also unknown parameters. Rather than specify that prior directly, it is often easier to sample \\(z = \\mathrm{Normal}(0, 1)\\) and compute \\(X = \\mu + z\\sigma\\) after the fact. This is called a “non-centered” parameterization. It is useful because it de-correlates the Normal random variation (the \\(z\\) component) from the variation in the priors for \\(\\mu\\) and \\(\\sigma\\). This can improve the performance of Markov chain Monte Carlo sampling without losing any information about the \\(X\\) quantity you ultimately want."
  },
  {
    "objectID": "blog/nonflat_implied_priors/index.html#implied-priors-for-nonlinear-functions",
    "href": "blog/nonflat_implied_priors/index.html#implied-priors-for-nonlinear-functions",
    "title": "Non-flat implications of flat priors",
    "section": "Implied priors for nonlinear functions",
    "text": "Implied priors for nonlinear functions\nNonelinear functions will stretch or shrink their input values in non-constant ways, which matters big time for implied prior densities. To see why, let’s think about prior density in itself. Say we have some probability mass of 1 that is uniformly distributed in some fixed interval of length \\(L\\). The density will be \\(1 / L\\). What would happen if we shrunk the interval by half? The total mass in our interval stays the same, but the density would double to \\(1 / (L / 2)\\) because we have to fit just as much probability mass into an interval of half the volume. If we stretched the interval by a factor of two, the volume would also increase, but because we have constant probability mass, the density decreases to \\(1 / 2L\\).\nA nonlinear transformation will apply this stretching and shrinking non-uniformly over the interval. Some regions of parameter space will be stretched (density decreases) and others will be compressed (density increases). As a result, a prior distribution that has a recognizable shape may be unrecognizable after a nonlinear transformation is applied to its support.\nI have an example of my own research with Ken Mayer on voter identification requirements in Wisconsin, which I will simplify for the sake of exposition. Let’s say that we have a voter registration file containing all registered voters in 2016 for a certain county. The voter registration file says whether each registered voter casted a vote in 2016 (but vote choice is anonymous). In this county, \\(N\\) of the registrants in the file did not vote. How many of these nonvoters would have voted if not for the state’s voter identification requirement?\nThere are two parameters that would be helpful to know. First, not every nonvoter in the voter file was eligible to vote at their registered address at the time of the election. Many registrants move away, die, change their names, etc., so only some fraction of the \\(N\\) nonvoters (say, \\(\\varepsilon \\in [0, 1]\\)) were eligible to vote as registered in 2016. So that’s one parameter; call it the “eligibity rate”. Next, of the \\(N \\times \\varepsilon\\) eligible nonvoters, another proportion \\(\\pi \\in [0, 1]\\) would have voted if not for the voter identification requirement. Call this the “vote prevention rate”. Our quantity of interest is the total number of voters who would have otherwise voted, which is calculated as \\(N\\varepsilon\\pi\\), and we only know \\(N\\).\nThis means \\(\\varepsilon\\) and \\(\\pi\\) are parameters to be estimated. Let’s say we give these parameters independent, flat priors on the \\([0,1]\\) interval; what would be the implied prior for \\(N\\varepsilon\\pi\\)? Let’s simulate it again. We use a \\(\\mathrm{Beta}(1,1)\\) prior for each probability parameter. The Beta family is commonly used to model unknown probabilities3, and the \\(\\mathrm{Beta}(1, 1)\\) prior is a special case that gives flat density to all probability values. But when we multiply all of these terms together, the result will not be flat at all.\n\n# This is a real value from the data\nN &lt;- 229625\n\n# simulate Beta priors for pi and epsilon\n# calculate N . eps . pi\nv &lt;- tibble::tibble(\n  pi = rbeta(100000, 1, 1),\n  epsilon = rbeta(100000, 1, 1),\n  pop_estimate = N * epsilon * pi\n) \n\nprint(v)\n## # A tibble: 100,000 × 3\n##        pi epsilon pop_estimate\n##     &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n##  1 0.129  0.287          8512.\n##  2 0.360  0.757         62562.\n##  3 0.0607 0.191          2658.\n##  4 0.879  0.798        161070.\n##  5 0.0813 0.227          4235.\n##  6 0.280  0.978         62793.\n##  7 0.821  0.651        122760.\n##  8 0.952  0.613        134013.\n##  9 0.923  0.00709        1503.\n## 10 0.973  0.605        135287.\n## # ℹ 99,990 more rows\n\n\nv |&gt;\n    pivot_longer(cols = everything(), names_to = \"param\") %&gt;%\n    mutate(\n        param_label = case_when(param == \"epsilon\" ~ \"Eligibility rate\",\n                                param == \"pi\" ~ \"Vote prevention rate\",\n                                param == \"pop_estimate\" ~ \"Would-be voters\")\n    ) %&gt;%\n    ggplot() +\n        aes(x = value) +\n        facet_grid(. ~ param_label, scales = \"free\") +\n        geom_histogram(alpha = 0.7, boundary = 0, bins = 50) +\n        scale_x_continuous(labels = scales::comma) +\n        labs(x = NULL, y = \"Frequency\") +\n        remove_y_axis() +\n        remove_grid()\n\n\n\n\n\n\n\n\nWe started with flat priors for \\(\\varepsilon\\) and \\(\\pi\\), but the implied prior for our quantity of interest is far from flat. If I had simply plopped this graphic into the paper and asserted that it was our prior for this quantity, we would have been in trouble over how opinionated it appears. But it is simply the mechanical result of multiplying two independent probability values with vague priors. The prior is still uninformative, but it’s a mistake to think that flatness is the same thing."
  },
  {
    "objectID": "blog/nonflat_implied_priors/index.html#parameterization-parameterization",
    "href": "blog/nonflat_implied_priors/index.html#parameterization-parameterization",
    "title": "Non-flat implications of flat priors",
    "section": "Parameterization, parameterization",
    "text": "Parameterization, parameterization\nNow that we have seen a \\(\\mathrm{Beta}(1,1)\\) prior, this would be a good time to talk about parameterization.\nLet’s say we observe a binary variable \\(y_{i}\\) for a set of observations indexed \\(i\\), and \\(y_{i}\\) takes the value 1 with probability \\(\\pi\\) and 0 otherwise. We give \\(\\pi\\) a flat \\(\\mathrm{Beta}(1,1)\\) prior, so our model looks like this: \\[\\begin{align}\n    y_{i} &\\sim \\mathrm{Bernoulli}(\\pi) \\\\\n    \\pi &\\sim \\mathrm{Beta}(1,1).\n\\end{align}\\] But in applied modeling, we often model probabilities on the logit scale. So we can introduce the log-odds (logit) parameter \\(\\eta = \\log\\left(\\frac{\\pi}{1 - \\pi}\\right)\\) and rewrite the model equivalently like so: \\[\\begin{align}\n    y_{i} &\\sim \\mathrm{Bernoulli}(\\pi) \\\\\n    \\pi &= \\frac{1}{1 + e^{-\\eta}}\n\\end{align}\\] Only now we have to set a prior for \\(\\eta\\) instead of \\(\\pi\\). If we wanted our prior to be flat for the probability parameter \\(\\pi\\), what prior would this be for the log-odds \\(\\eta\\)? I will simulate this by drawing \\(\\pi\\) from a flat Beta prior and calculating \\(\\eta\\) for each draw.\n\nps = tibble::tibble(\n    pi = rbeta(1000000, 1, 1),\n    eta = log(pi / (1 - pi))\n)\n\n\nps |&gt;\n    tidyr::pivot_longer(cols = everything(), names_to = \"param\") %&gt;%\n    ggplot() +\n        aes(x = value) +\n        facet_grid(. ~ forcats::fct_rev(param), scales = \"free\", labeller = label_parsed) +\n        geom_histogram(bins = 100, boundary = 0, alpha = 0.7) +\n        remove_y_axis() +\n        remove_grid() +\n        labs(y = \"Freqency\", x = \"Parameter value\")\n\n\n\n\n\n\n\n\nSo a flat prior for a probability implies a very non-flat prior for the logit probability. It turns out that the prior on the logit scale is a \\(\\mathrm{Logistic}(0, 1)\\) prior, which feels right when you remember that the inverse link function for logistic regression is the standard Logistic distribution function. We would find a similar result modeling the probability with a probit model instead: we would need a \\(\\mathrm{Normal}(0,1)\\) prior on the probit scale to induce a flat prior on the probability scale.\nThe lesson here is that priors only have meaning relative to some model parameterization. If there are many ways to rewrite the likelihood for a model, then a flat prior in one parameterization is unlikely to be flat for all parameterizations. The consequence is that not all flat priors will be “noninformative”, and plenty of noninformative priors won’t look flat. It all depends on what a parameter value means in specific parameter space."
  },
  {
    "objectID": "blog/nonflat_implied_priors/index.html#nothing-is-safe-estimated-causal-effects-are-functions-of-parameters",
    "href": "blog/nonflat_implied_priors/index.html#nothing-is-safe-estimated-causal-effects-are-functions-of-parameters",
    "title": "Non-flat implications of flat priors",
    "section": "Nothing is safe: estimated causal effects are functions of parameters",
    "text": "Nothing is safe: estimated causal effects are functions of parameters\nSocial scientists in the causal inference space often try to take a stance of principled agnosticism. They want to impose few assumptions, invoke models only minimally, and use simple estimators based on differences in means. But not even differences in means are safe.\nWe have an experiment with two groups, a control (0) and a treatment (1), with a binary outcome variable \\(y\\). The groups have unkown means (success probabilities) that we represent with \\(\\mu_{0}\\) and \\(\\mu_{1}\\), so the true treatment effect is \\(\\tau = \\mu_{1} - \\mu_{0}\\). It’s common among social scientists to estimate this as a regression with an indicator for treatment status: \\[\\begin{align}\n    y_{i} = \\mu_{0} + \\tau z_{i} + e_{i}\n\\end{align}\\] where \\(Z_{i}\\) is a treatment indicator and \\(e_{i}\\) is random error. Bayesians, always thinking about parameterization, would prefer to estimate the means directly: \\[\\begin{align}\n    y_{i} = (1 - z_{i})\\mu_{0} + z_{i}\\mu_{1} + e_{i}\n\\end{align}\\] because it is easier to give each group mean the same prior when the model is parameterized directly in terms of means. It would be pretty confusing to set one prior for the control mean and another prior for the difference in means, which is itself a function of the control mean. So let’s go with the second parameterization.\nBecause this is causal inference and we want to be principled agnostics, we employ some default thinking and give each group mean a flat prior, estimating the difference in means after we have the means. But if each group mean has a flat prior, what would be the implied prior for the difference in means? Not flat! We simulate another example, giving a flat prior to each mean and pushing the uncertainty into the treatment effect.\n\n# simulate means and calculate difference\nrct &lt;- tibble::tibble(\n  mu_0 = rbeta(1000000, 1, 1), \n  mu_1 = rbeta(1000000, 1, 1),\n  trt = mu_1 - mu_0\n)\n\nprint(rct)\n## # A tibble: 1,000,000 × 3\n##     mu_0   mu_1      trt\n##    &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n##  1 0.316 0.0316 -0.284  \n##  2 0.327 0.329   0.00182\n##  3 0.184 0.851   0.667  \n##  4 0.596 0.0169 -0.579  \n##  5 0.696 0.561  -0.135  \n##  6 0.171 0.908   0.737  \n##  7 0.747 0.764   0.0173 \n##  8 0.318 0.382   0.0638 \n##  9 0.891 0.768  -0.123  \n## 10 0.779 0.0576 -0.721  \n## # ℹ 999,990 more rows\n\n\nrct |&gt;\n    tidyr::pivot_longer(cols = everything(), names_to = \"param\") %&gt;%\n    ggplot() +\n        aes(x = value) +\n        facet_grid(\n            . ~ param,\n            scales = \"free\",\n            labeller = as_labeller(c(\"mu_0\" = \"Control mean\",\n                                     \"mu_1\" = \"Treatment mean\",\n                                     \"trt\" = \"Treatment effect\"))\n        ) +\n        geom_histogram(boundary = 0, alpha = 0.7, bins = 50) +\n        labs(y = \"Frequency\", x = \"Parameter value\") +\n        remove_y_axis() +\n        remove_grid()\n\n\n\n\n\n\n\n\nSo we started with a flat prior on our group means, and we ended up with a non-trivial prior for the treatment effect. How did this happen? Averaging over my prior uncertainty in both groups, my expected difference in means ought to be mean zero (naturally). But more than that, we get a mode at zero because there are many more ways to obtain mean differences near zero than differences far from zero. We get big differences (near -1 or 0) only when both means are far apart, which isn’t as likely to happen randomly as two means that are a little closer together in the prior.\nIf we really wanted a flat prior on the treatment effect, what we would be saying is that big treatment effects are just as likely as small treatment effects. But this doesn’t make sense. If we devise a reasonable prior for our control group mean, and then we encode a “null” view into the treatment mean that it has the same prior as the control group, then the notion that “big treatment effects are just as likely as small treatment effects” is simply untenable. We should prefer to set reasonable priors for our grouop means directly and let the treatment effect prior take care of itself."
  },
  {
    "objectID": "blog/nonflat_implied_priors/index.html#let-go-of-the-need-for-flatness",
    "href": "blog/nonflat_implied_priors/index.html#let-go-of-the-need-for-flatness",
    "title": "Non-flat implications of flat priors",
    "section": "Let go of the need for flatness",
    "text": "Let go of the need for flatness\nThese implications feel strange at first, but they are all around us whether we notice them or not. The flatness of a prior (or any shape, flat or not) is a relative feature of a model parameterization or a quantity of interest, not an absolute one. Inasmuch as we believe priors are at work even when we don’t want to think about them—i.e. we accept Bayesian models as generalizations of likelihood models—we should respect how transforming a likelihood affects which parameters are exposed to the researcher, and which spaces those parameters are defined in. We should know that flat priors aren’t necessarily vague, and vague priors aren’t necessarily flat. What we’re seeing here is that flatness begets non-flatness in tons of scenarios, but that is totally ordinary and nothing to be worried about."
  },
  {
    "objectID": "blog/nonflat_implied_priors/index.html#footnotes",
    "href": "blog/nonflat_implied_priors/index.html#footnotes",
    "title": "Non-flat implications of flat priors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor academic social scientists, at least, for whom unregularized least-squares or maximum likelihood estimation still dominate. Once you begin caring about regularization to prevent overfitting your model to your data, non-Bayesian inference quickly stops being automatic. But that’s academic for you: the luxury of never actually having to pay your loss function.↩︎\nThe shape of an implied prior can be quite useful in practice, when you know what you are getting into. For example, common methods for regularizing a linear model, like ridge and LASSO regression, have (loosely) analogous Bayesian prior distributions. Topic for a future post maybe.↩︎\nEven in non-Bayesian inference, the Clopper-Pearson confidence interval is based on Beta distributions.↩︎"
  },
  {
    "objectID": "blog/usable_repos/index.html",
    "href": "blog/usable_repos/index.html",
    "title": "Replication code should be more usable.",
    "section": "",
    "text": "My mind has been on replication archives lately. I often go digging in other peoples’ projects for data to practice some new statistical skill or another. And I have been digging in my own projects lately to refactor some code for a dormant academic project. In both of these situations I am interfacing with some else’s code (me today ≠ me in the past), and in both situations I am having a bad time.\nThe academic community has been increasingly interested in replication archives since they realized that a lot of public research is, err, systematically untrustworthy. Formal requirements (from journals) and informal pressures (from other researchers) in academia are increasingly requiring authors to prepare these repositories for new projects, which seems good on the whole. But if you ever go digging in these republication archives, you quickly realize that just because authors provide data and code doesn’t mean that the experience is all that helpful. But why? You can see the data, maybe the code runs without errors…What’s the problem?\nMy main argument is that the code was not really designed to be understood or manipulated by other people. If it were, the code would not look the way it does.\nNow, I’m not an academic anymore, so don’t have much stake in this. But I do write quantitative research code in a collaborative environment at work all the time, with the intention that my code will be consumed and repurposed by others. As it turns out, writing code under these conditions changes how you write that code, improves the reliability of your work, and has positive effects on the community in which your code is deliberated and consumed. This blog post will attempt to distill some of the things I have learned into discrete, attainable lessons for writing academic research code.\nBefore I get too concrete though, I make a normative argument for caring about this at all. Changing the way code is written takes effort, and I want to argue why that effort would be justified."
  },
  {
    "objectID": "blog/usable_repos/index.html#why-do-so-many-research-projects-replication-archives-hurt",
    "href": "blog/usable_repos/index.html#why-do-so-many-research-projects-replication-archives-hurt",
    "title": "Replication code should be more usable.",
    "section": "",
    "text": "My mind has been on replication archives lately. I often go digging in other peoples’ projects for data to practice some new statistical skill or another. And I have been digging in my own projects lately to refactor some code for a dormant academic project. In both of these situations I am interfacing with some else’s code (me today ≠ me in the past), and in both situations I am having a bad time.\nThe academic community has been increasingly interested in replication archives since they realized that a lot of public research is, err, systematically untrustworthy. Formal requirements (from journals) and informal pressures (from other researchers) in academia are increasingly requiring authors to prepare these repositories for new projects, which seems good on the whole. But if you ever go digging in these republication archives, you quickly realize that just because authors provide data and code doesn’t mean that the experience is all that helpful. But why? You can see the data, maybe the code runs without errors…What’s the problem?\nMy main argument is that the code was not really designed to be understood or manipulated by other people. If it were, the code would not look the way it does.\nNow, I’m not an academic anymore, so don’t have much stake in this. But I do write quantitative research code in a collaborative environment at work all the time, with the intention that my code will be consumed and repurposed by others. As it turns out, writing code under these conditions changes how you write that code, improves the reliability of your work, and has positive effects on the community in which your code is deliberated and consumed. This blog post will attempt to distill some of the things I have learned into discrete, attainable lessons for writing academic research code.\nBefore I get too concrete though, I make a normative argument for caring about this at all. Changing the way code is written takes effort, and I want to argue why that effort would be justified."
  },
  {
    "objectID": "blog/usable_repos/index.html#getting-oriented",
    "href": "blog/usable_repos/index.html#getting-oriented",
    "title": "Replication code should be more usable.",
    "section": "Getting oriented",
    "text": "Getting oriented\n\nThis isn’t about “good” code vs. “bad” code.\nLike you, I don’t have a defensible theory about what makes code Good for any enduring understanding of Good. “Good” code seems like a contested and highly contextual idea. Code may have “good properties”, but certain features of code design also present important trade-offs, so goodness is understood relative to our goals. So maybe we should start by talking about what we want academic code to do for us.\n\n\nWhat is the point of the code repository for academic projects?\nI will discuss two broad models for code repositories, an archive model and a product model. I am making these up as I go, but you will get the idea.\nThe archive model. This is what most academic repositories are doing right now, I would guess. We call these code repositories “replication archives” because that’s what we think we want them to do: replicate and archive. The model is focused on the “validity” of the code—does it correctly reproduce the results in the writing, tables, and figures (replication) in a robust and enduring way (archiving).\nThese are important goals, but focusing exclusively on them has some predictable side-effects. The repositories end up serving as an audit trail for the paper, so the code is designed around the rhetoric of the paper instead of around the operations that the code performs. We see files named things like 01_setup, 02_clean-data, 03_model, 04_tables-figs, 05_robustness and so on. Even supposing that all of the code runs just fine, all we can really do with the code is rebuild the paper exactly as it is. If you could draw a web of all the interlocking pieces of this project, it would be a long, single strand from start to finish, highly dependent from one script to the next (perhaps intended to be run in the same R/Python session), with no feasible way to swap out any components for other components or modify them in isolation. And crucially, if we wanted to use the data or some of the code from this project for some other purpose, we would have to mangle the project in order to extract the components we wanted.\nThe product model. The product model organizes its code as if to offer it as a product for someone else to use. The code is a key part of the research package, and researchers care that it is a good experience, just as they care about the quality of their writing. An important assertion here is that there is value in the code even if the paper did not exist. If a project proposes a method to measure or estimate a key variable, those methods are valuable outside of the context of the written paper, so part of the product of the project is to make these tools available and usable for other researchers to benefit from. Projects that propose a new model make this model available for others to use with a practical interface. Indeed, there may be a notion of interface as distinct from infrastructure; the code that drives the project at a high level is uniform and understandable, and the ugly code that does heavy lifting might be encapsulated away from the interface. And better yet, the important infrastructure pieces are modularly designed: they may combine at the locus of the project, but the components can exist without reference to one another and could be exchanged for other components that perform similar functions.\n\n\nIncreasing the amount of shared / re-used code would be good for research credibility and knowledge accumulation\nPosting code online is not the same as making it usable. I suspect many social scientists put code online without really wanting others to dig into it. But there are real services that academics could provide for each other more regularly if they were more generous with the way their code is designed. Political scientists in particular use a lot of the same data sources and do a lot of similar transformations on it, but why all the wasted effort? I quite admire Steven V. Miller’s peacesciencer package for R, a set of tools whose rationale seems to be that there’s no sense in having everybody duplicate each other’s work to shape the data into common formats for analysis in political science. I gotta say, I agree.\nBut it doesn’t stop at broad-based datasets or data-shaping tools. Think about every paper you have ever read that proposes a new way to measure something. Did those authors provide a code module to do the appropriate calculations on new data of appropriate shape? Did those authors provide an interface for validating these measures against alternatives? I suspect that in most cases the answer is no. The authors may put their code online, but it isn’t online so that you can use it. Not really.\nI don’t want to point fingers, so I will use some examples from my own academic work to show that I, too, wasn’t thinking enough about this. In my dissertation, I built a group-level measurement model to estimate latent political ideology in partisan groups at subnational units of aggregation, and I wrote a series of Stan files to iterate on a few approaches to it. Other projects I have seen before and after I finished my thesis have built similar models. Did I package my models into a tool that they could use? No, I did not. I also implemented a reweighting method for aggregated survey data that I first saw in a paper from nearly a decade ago. This is probably a pretty broadly applicable correction for similar data, but did I provide a little module for others to apply the same calculations on data that wasn’t exactly my own? Nah. I designed a Bayesian implementation of the Acharya, Blackwell, and Sen formulation of sequential-g estimator that addresses some of the things that Bayesians would care about in situations like that, and I would even say I was pretty proud of it. But I wasn’t proud enough to share a generic version that others could use and adapt for their purposes.\nYou get the idea.\nIt just makes me worry that when we advertise our work as tools that others could use, we do not really mean it. I worry that phrases like “We propose a method…”, or “we provide an approach…” are only things we were trained to say to make it sound like we are contributing tools for the community. But we are not doing the work that would make those tools available for others. The code that goes into the paper repository is for ourselves, because the goal is getting our paper over the finish line. The code is just another part of the game.\nThere recently was a post on the Gelman blog that stuck out to me about cumulative science and re-using each other’s work. Here is an excerpt that gives you the idea:\n\nHow could a famous study sit there for 20 years with nobody trying to replicate it? […] Pamela said this doesn’t happen as often in biology. Why? Because in biology, when one research team publishes something useful, then other labs want to use it too. Important work in biology gets replicated all the time—not because people want to prove it’s right, not because people want to shoot it down, not as part of a “replication study,” but just because they want to use the method. So if there’s something that everybody’s talking about, and it doesn’t replicate, word will get out.\nThe way she put it is that biology is a cumulative science.\n\nThinking about how this applies to our code, it is clear that there is more than a vague moral value to posting usable code for others. There is scientific value. And it is interesting to me that after all the commotion about replication and running code without errors, there is comparatively little discussion about the scientific value of the code outside of the narrow, narrow context of the paper it is written for."
  },
  {
    "objectID": "blog/usable_repos/index.html#what-can-be-done",
    "href": "blog/usable_repos/index.html#what-can-be-done",
    "title": "Replication code should be more usable.",
    "section": "What can be done?",
    "text": "What can be done?\nNow that I am done complaining, we can talk about recommendations. I will focus mainly on two concepts, interface and modularity, from a couple different angles. Interface refers to the way other people use your tools. Modularity describes how those tools are combined for easier development (for your own benefit) and exporting (for others’ benefit).\n\nInterfaces for living projects, not memorials to dead projects\nWhenever I want to download a project’s data and code, I only want to get it from a version control platform like Github. I want to fork the project, see the history, get the intended file structure, and maybe even contribute to the project by opening issues or pull requests. I never want to go to Harvard Dataverse. I’m sure Dataverse was a great idea when it first hit the scene, but by today’s standards, it feels like it is chasing yesterday’s problems, like the problem of “pure replication”. But I think the credibility problems in social science warrant more than old-world-style replication. We should be looking for platforms that accommodate and encourage sharing, re-use, mutual contribution, and stress-testing by others.\n\n\nInterface vs. Infrastructure\nThis is a pretty common distinction you hear about in software communities. It isn’t much discussed in academic research circles.\nI will explain by way of example: Think about the tidyverse packages in R, or just the dplyr and tidyr packages. These packages provide an interface to useful data manipulations on data frames. These are operations on abstractions of your data—an abstraction in the sense that the functions do not have to know or care what is in those data frames in order to operate on them. The packages provide a uniform interface; they take a data frame as an input and return a data frame as an output, and the semantics are similar across functions. This makes the operations composable, which is jargon for “the operations can be reordered and combined to achieve powerful functionality”. The same basic principles are true for other tools in the tidyverse like stringr, forcats, purrr, and so on. They employ different abstractions for data organized at different levels (strings, factors, and lists respectively), but the emphasis on uniformity and composability is always there.\nSo that’s the “interface” layer. Now, what about the “infrastructure” or the implementation of these functions? Do you know anything about how these functions are actually written? And would it really matter if you did? The infrastructure isn’t what you, the user, care about. What matters is that the tools provide a simple way to perform key tasks with your code without bogging you down in the implementation details.\nCompare this to the way we write research code. There is usually no distinction between interface and infrastructure whatsoever. A lot of the time, we keep all of our nasty and idiosyncratic data-cleaning code right next to our analysis and visualization. On a good day, we may smuggle the data-cleaning code into a different file, but that doesn’t make a huge difference because the flow of the project is still mostly linear from raw data to analysis. The user cannot really avoid spending time in the darkest corners of the code.\nTo be fair, it isn’t necessary that an academic project’s code base should produce an end result as concptually gorgeous as tidyverse tools are. But there are probably some intertwined components in the research project that could be separated into interface and infrastructure layers somehow, and readers who really want to investigate the infrastructure are free to do so.1\nThinking again about a paper that proposes a new way to measure a key variable, or a new method to analyze some data: could those methods not be divided into an interface to access the method and an infrastructure that does the heavy lifting somewhere else? Wouldn’t you be more likely to use and re-use a tool like that? Wouldn’t it be more likely that if the method has a problem, we would have an easier time discovering and fixing it? Wouldn’t that look more like the iterative, communal scientific process that we wish we had?\n\n\nLittle functions, good and bad\nShould you make an interface by writing more functions? Annoyingly, it depends. One way to make an interface more legible to users is package annoying routines into functions that describe what you’re accomplishing. This is nominally easy to implement, but it isn’t always easy to design well. And the design considerations present plenty of trade-offs with no obvious guiding theory.\nTake a halfway concrete example. You have a dataset of administrative data on many individuals, and the dataset has a field for individuals’ names. Your task is to clean these names in some way.\nYou choose to use some combination of regular expressions and stringr to do this. But how do you implement this operation in the code? One way is to create a new variable like…\n\nnew_df &lt;- mutate(\n    df, \n    name = \n        str_replace(...[manipulation 1 here]...) |&gt;\n        str_replace(...[manipulation 2 here]...) |&gt;\n        [...]\n        str_replace(...[final manipulation here]...)\n)\n\n…and this works fine. But if you wanted to change the way this name-cleaning is done, not only do you have to do surgery directly on a data pipeline, but you can’t test your new implementation without re-running this mutate step over and over (which may be only one step of an expensive, multi-function pipe chain, but that is a separate, solvable issue).\nConsider instead the possibility of writing a function called format_names that implements your routine. Now your data pipeline looks like this…\n\nnew_df &lt;- mutate(df, name = format_names(name))\n\nWell, should you do that? The routine is now encapsulated, so if you need to do it more places, you can call the function without rewriting the steps.2 This makes your routine unit-testable: does it clean the patterns you want it to clean (and you are, of course, limited by the patterns you can anticipate). It also makes it a little easier to change your implementation in one location and achieve effects in many places without doing surgery to your data pipeline. Maybe it also makes it easy to move this step in your data pipeline around, which is good.\nAnd what are the drawbacks? Well, you no longer know what the function is doing without hunting down the implementation, and it’s possible that the implementation is idiosyncratic to your project instead of being broadly rules-based. In general, encapsulating code into a function makes it easier to “drive” code, but it doesn’t inherently have any effect on whether your code is operating at a useful level of abstraction. Nor does it have any obvious effect on whether the interface you design for one function is at all related to the interface you create for other functions—uniformity makes your functions easier to use and combine for powerful results. If you aren’t careful, you might write ten different functions that don’t share a uniform abstraction or interface, so now it takes more effort for you to remember how your functions work than it does to write it out using stringr. After all, stringr is already built on familiar abstractions: stringr don’t know what your string is, and it does not care. All it knows is that it has functions with similar semantics for doing operations on strings.\nSo, you have to think about what you want to accomplish if you want to have an effective design.\n\n\nMore modules, fewer pipelines\nSo far I have been a little incredulous toward the idea that your code should be a “pipeline” for your data. This is because pipelines are often in conflict with modularity: the principle of keeping things separate, independent, and interchangeable. A lot of academic projects are lacking in it.\nIt is difficult at first to realize the drawbacks of non-modular pipeline organization because, especially with tidyverse tools, chaining many things together to do big things is the primary benefit. When dplyr and tidyr first start clicking for you, you immediately begin chaining tons of operations together to go from A to B without a ton of code. But as you iterate on the analysis, which requires breaking and changing things in your code, it can suddenly be very cumbersome to find and fix broken parts. This is because you have twisted all of the important steps together! You wrote the code in such a way that steps that do not depend on one another in principal now have to be organized and executed in a strict order. And now the fix for one breakage leads you to break something else because the steps in your pipeline are not abstracted. It is just nice to avoid problems like this.\nTo be clear, this is not the tidyverse’s fault. The tidyverse is an exemplar of modular design. The problem is that you tried to string too much together without thinking about how to keep things smartly separated. We should be asking ourselves questions like,\n\nWhat operation (read: function) do I need to do here, regardless of what my particular data look like? That is, think more about functionality and less about state.\nWhat can be separated? For instance, do I need the analysis of these 3 different data sources to happen in the same file? Or are they unrelated to the point where the analyses don’t have to be aware of one another in any way? Again, functionality without state.\nWhat can be encapsulated? Suppose I convince myself that the analyses of my 3 different data sources don’t have to be aware of one another, but maybe there are some tools I can define that could be useful in each analysis. Perhaps I should encapsulate those tools under a separate file (read: a module!) and then import that module whenever I need it. This lets me both keep things separate without repeating the same steps in many places. We should be skeptical of encapsulation for its own sake—it isn’t always necessarily helpful—but in this case it helps us separate functionality from state.\nCorrespondingly, what can be exported to other files in this project, or to other users who might want to use only that part? This is probably helpful to remember for cleaned data, repeated data manipulation tools, a new model that you build, and more.\n\nIf more pieces of your project are separable, interchangeable, and exportable, it becomes much easier to share little pieces of your project with other researchers.\n\n\nThere’s organization, and then there’s organization\nWe know that our code should be organized, but it is easy to organize code ineffectively. Something we see a lot in research code is an organizational structure that is perfectly discernible but not exactly useful. I have created plenty of impeccably “organized” but unhelpful repositories.\nOne discernible but not-very-useful organizational pattern is to name your files as steps along a data analysis pipeline: 01_setup, 02_clean-data, 03_model, 04_tables-figs, 05_robustness, and so on. Again, guilty. This setup doesn’t help the researcher keep themselves “organized” if every problem they have to solve is tangled together in one bucket ambiguously labeled “data cleaning”. And it doesn’t help anyone consuming the project understand or modify which pieces of the code are responsible for which functionality.\nBut if you are being a good citizen—designing modularity into your code, creating a useful interface, dividing functionality across files, separating analyses that don’t depend on one another, and so on—the pipeline organization will not be the dominant feature. It will naturally be replaced by a layout that reflects the concepts in the code, not the rhetorical contours of a research paper. The code is not a paper, and a paper is not its code.\nTo be sure, you cannot kill every little pipeline; data always need to be read, shaped, and analyzed. But building your code on effective abstractions lets you write smaller pipelines that are quicker from start to finish, conceptually simpler, and untangled from other independent ingredients in your project. The ultimate goal is usable code, not killing pipelines. It just happens that small, minimal pipelines are more usable than gigantic, all-encompassing pipelines.\n\n\nJudicious usage of “literate programming”\nMany researchers agree that statistical output should be algorithmically injected into research writeups to ensure the accuracy of the reporting. This is the real reason to use document prep systems like Rmarkdown or (increasingly) Quarto: not for the math typesetting.\nBut it is also a problem if the statistical work is so intertwined with the manuscript that they cannot be severed in the code. I myself have written plenty of “technically impressive” Rmarkdown documents that are actually are fragile Jenga towers of tangled functionality.\nThis is a pretty easy lesson though. The .Rmd file shouldn’t be the place where important, novel analysis happens. That should happen elsewhere, and your paper should be epiphenomenal to it.\n\n\nWho’s looking at your code?\nYou learn a lot by having other people make suggestions to you about the way your code is structured. I suspect most academic projects, even those with multiple co-authors, don’t feature much code criticism or review of any kind. Outsourcing code tasks to graduate students is great for developing their skills, but if you (the person reading this) are the veteran researcher, they would also benefit from your advice. The student and your project will be better for it.\n\n\nDon’t propose methods. Provide them.\nI have said this already, but I wanted to sloganize it."
  },
  {
    "objectID": "blog/usable_repos/index.html#better-code-vs.-the-academy",
    "href": "blog/usable_repos/index.html#better-code-vs.-the-academy",
    "title": "Replication code should be more usable.",
    "section": "Better code vs. the Academy",
    "text": "Better code vs. the Academy\nThis section could be its own blog post, but I am not an academic anymore and do not want to spend too much energy arguing about stuff that I intentionally left behind. But in case you are thinking about the following things, I just want you to know what I see you, I hear you, but I think these are the questions you have to answer for yourselves.\n\nWriting better code will not help my paper get published. Yeah, that sucks, right? Why is that? If we can agree that more usable code will make it easier to share tools among researchers, test-drive the contributions of other researchers, and bolster the credibility of your field overall, why does your publication model think that it’s a waste of your time as a researcher?\nThis all sounds very hard, and I am not a software engineer. You shouldn’t have to be a software engineer if you do not want to be. What I do wish is for your field to sustain broader collaboration where you have a team of people who are good at different things so your field can put out more reliable products. But many fields care about superstar researchers. I do not care about superstar researchers."
  },
  {
    "objectID": "blog/usable_repos/index.html#what-we-havent-said",
    "href": "blog/usable_repos/index.html#what-we-havent-said",
    "title": "Replication code should be more usable.",
    "section": "What we haven’t said",
    "text": "What we haven’t said\nHere’s a recap of all the things we did not mention about replication / archiving discussions:\n\nPackaging environments and dependencies\nRelatedly, containerization\nSolutions for long-term archiving\nCommenting code / other documentation\n\nI think these problems are important to varying degrees. I mean, I am not as concerned about containerization, but you are free to be. But these issues are more commonly discussed than the design, usability, and shareability of code. So I didn’t talk about them."
  },
  {
    "objectID": "blog/usable_repos/index.html#footnotes",
    "href": "blog/usable_repos/index.html#footnotes",
    "title": "Replication code should be more usable.",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt is also worth noting that merely hiding the messiness behind a function isn’t really the best way to proceed. What you want to achieve by separating the implementation is some kind of abstraction away from the particularities of your data into something simpler and more principled. See this entertaining talk on the difference between “easy” and “simple”.↩︎\nBetter yet, if the patterns you need to clean about the names are the same things you would need to clean about other strings, you could design the function to be abstracted away from “names” altogether.↩︎"
  },
  {
    "objectID": "blog/modular_blog/index.html",
    "href": "blog/modular_blog/index.html",
    "title": "Highly modular blogging with Blogdown",
    "section": "",
    "text": "When I finished graduate school, I tore down my website.\nFor a handful of reasons. I no longer needed a website that cried out, “Help me, I’m finishing my PhD and I need to escape.” I didn’t need to showcase unpublished papers, teaching resources, or old blog posts that I had grown detached from. It was time for a clean reset.\nBut if you work with Blogdown, you know that starting over is laborious. Not that Blogdown isn’t great, because it is. It’s that, when you’re a finicky person like me, setting up a website with the right balance of capable features, pleasant aesthetics, and a principled codebase is legitimately challenging. I was encountering the same familiar challenges over and over.\nFor example, the site’s Hugo theme. I would take a lot of Hugo themes for test-drives. Hugo advertizes themes as if they were completely modular with respect to your /content/ folder. For most themes, this is a lie. Themes usually want too many bespoke variables or file structures in your website content. Some amount of this is okay, but it comes at a cost. If you really want to take a theme for a spin, I would find it easier to create an entirely new blogdown::new_site() than to change my theme in an existing site.\nExcept now you’re dragging the same files around your computer all over again. Ugh, this new website directory needs your blog source files, your website-level .Rprofile that controls Blogdown’s build behaviors, the jpg/png image files that you use to brand yourself online, etc… And maybe these files need to go in different folders or be given different file names from the previous theme. After a while, these files no longer have a single authoritative “home” on your computer, and you may have multiple conflicting(!) versions of these files across your various experimental website folders.\nAnd then there’s reproducibility. Even after lugging around all the same files to the new site, good luck getting your blog posts to render if your package library has changed since they were written, which it probably has. Danielle Navarro wrote about reproducibility in Rmarkdown blogging, and argues convincingly that the best way to protect your .Rmarkdown posts from this rebuilding risk is to create a dedicated package library for each separate blog post using renv. This sounds intense at first, but the underlying principle is simple, which makes it a good solution to a difficult problem.\nThis post will continue that pattern: intense at first, but well-founded, solutions for difficult problems."
  },
  {
    "objectID": "blog/modular_blog/index.html#what-this-post-is-about-modularity",
    "href": "blog/modular_blog/index.html#what-this-post-is-about-modularity",
    "title": "Highly modular blogging with Blogdown",
    "section": "What this post is about: modularity",
    "text": "What this post is about: modularity\nWhat we want is a principled and robust approach for managing the many interlocking components of your website. Specifically, we explore the modularity of the elements in your website. I take the view that your website is a collection of modular components that are better managed independently, with different Git repositories for the different site components. Yes, managing your website with multiple repositories. Stay with me.\nThe modules that compose your website include your theme, your blog posts, your Blogdown build preferences (implemented in your .Rprofile), and maybe more. These modular components come together at the nexus of the website, but as I will argue, these components should not belong to the website. Why not? Because these components can be re-used across different websites or substituted with other similar components. Hugo already flirts with this idea in its core design by separating the /theme/ directory from the /content/ directory, as if to say, “these components can be combined but do not depend on one another.” This post takes an opinionated stance that such modularity is a good idea and should be assertively extended to other components of your Blogdown site. That said, I make no assertion that this stance is objectively correct—only that it has been useful enough for me that I wanted to share some thoughts about the principles and processes at work. (You should do what works for you!)\nModularity as a software philosophy is one thing, but implementing it in code requires technical solutions. This post will discuss how to achieve this using Git submodules, an intermediate-level Git construct that, if you’re like me, is somewhat familiar but somewhat intimidating. In short, a Git submodule is a repository-within-a-repository. It has its own version history that is distinct from the “parent” repository. In this post, I provide a simple tour of submodules and how they can be used to structure your website workflow. We will recast our website as the “primary” repository, and we import other modular site components (like our blog posts) as Git submodules. In case you host your blog on Netlify, I will also discuss how to ensure that Netlify can build your site successfully.\n\nAside: some terminology\nThis discussion will involve plenty of concepts that sound similar to one another but should be understood as distinct things. I want to flag these concepts so that we understand each other better.\nDirectory vs. repository. A directory is a folder on your computer that holds files. A (Git) repository tracks changes to files. For many projects, the project’s root directory is entirely managed by one repository, so the distinction between the two may be blurred. When Git submodules are involved, this is no longer true. Your website directory will be managed by one repository, and sub-directories below your website will be managed by other repositories.\nWebsite vs. module. The website is the entire project that puts your website online. Your website will contain various modules that combine to build the entire project. Your blog posts will be considered a module (or several modules, depending on your implementation). Your theme is another module. Think of modules as building blocks for your website that can be stacked, swapped out, and so on.\nParent repository (for the website) vs. child repository (for the module), a.k.a. “submodule”. The website and the module will be versioned by separate repositories. We can refer to the over-arching project repo (the website) as the “parent” repo and the module repo as the “child” repo. A “Git submodule” is a Git construct that is overlaid onto this relationship between repositories. A repository, in isolation, is simply a repository. But if you import a repository into another project as a dependency, Git designates the dependency as a “submodule” to the parent repository, and this affects our Git workflow as a result. I explain all of that below."
  },
  {
    "objectID": "blog/modular_blog/index.html#websites-are-a-collection-of-modules",
    "href": "blog/modular_blog/index.html#websites-are-a-collection-of-modules",
    "title": "Highly modular blogging with Blogdown",
    "section": "Websites are a collection of modules",
    "text": "Websites are a collection of modules\nModules are like little building blocks, and your website has plenty of them. Setting aside any formal definition of what would mathematically be considered a “module”, let’s crudely define them as structures in your website that are agnostic to the content of other structures. We may be able to replace modules with other modules, or remove modules entirely, without affecting the core function of other modules.\nHere are some examples from my own workflow. I consider my blog posts, Hugo theme, and blogdown build settings (in my site-level .Rprofile) as modular components within the website as a whole, and I version each component with its own separate repository. Here is how I justify this view for each component:\n\nBlog posts: The content of a blog post is completely separable from the website repo. We can take a blog post and locate it in a different website, and the blog post should still be meaningful (and reproducible) unto itself. Many blogdown users remake their websites and carry their old blog posts to the new sites, which shows that the blog content doesn’t functionally depend on the website.\nIt turns out that, for blog posts, modularity and reproducibility are pretty closely related. In her discussion of blog reproducibility, Danielle Navarro touched on the principle that a blog should be “encapsulated” or “isolated” away from the broader website to robustify the blog against other dependencies. By insisting that blog posts also be modular, not only is the blog protected from the website’s computational environment, we can control each post independently of one another, move posts around across contexts, and remove posts entirely without side-effects.\nThis also affects how we treat the blog post’s dependencies. Suppose that your post includes an analysis on a data file that you read from disk. This file should belong to your blog post—and be versioned by that blog post’s Git repository—not your website. This means you should keep all of these files in the blog post directory, and forget about the website’s /static/ folder except for files that rightfully belong to the website.\nHugo theme: Hugo is designed such that the /content/ of a website (specified in markdown files) is more-or-less independent of its /theme/. The same theme can be used for multiple websites, and a single website can (in theory1) swap out one theme for another. Because themes are managed with Git repositories already, you can pull theme updates from their remote repositories without overwriting any bespoke theme customizations specified in your /layouts/ folder.\nBlogdown complicates this somewhat. When you install a theme with blogdown::install_theme(), Blogdown actually deletes the theme’s .git directory. (At least, this was my experience.) This is probably for ease-of-use among users who would not appreciate having to manage the theme as a submodule. But we are enthusiastic seekers of modularity, so we want to keep that upstream remote connection alive. As such, I installed my site’s Hugo theme using Git submodule operations instead of installing it with blogdown::install_theme().\n\n\nThe website .Rprofile file: You may have a global .Rprofile file, but it is an increasingly common Blogdown workflow staple to set up a website-specific .Rprofile to control Blogdown’s build behavior. How is this a module? Your blogdown build preferences are probably not specific to this website repository. Instead, it is likely that your preferences reflect your workflow for blogging in general and could be equally applicable to any other website repo you create or manage. If you change your blogdown workflow in a way that bears on this .Rprofile file, that change may affect all of your blogdown websites equally! Managing these .Rprofiles separately for each website would be inefficient and error-prone, so instead we manage the .Rprofile in one repository that we import to our website as a submodule."
  },
  {
    "objectID": "blog/modular_blog/index.html#how-to-accomplish-this-git-submodules",
    "href": "blog/modular_blog/index.html#how-to-accomplish-this-git-submodules",
    "title": "Highly modular blogging with Blogdown",
    "section": "How to accomplish this: Git submodules",
    "text": "How to accomplish this: Git submodules\nGit submodules are repositories-within-repositories. Suppose you are working on a project repository (like your website), and there are external tools or resources that you want to import from another project. You have a strong project-based workflow, so you want all of the code that creates your website to be contained within the website directory on your computer. At the same time, the external dependency is clearly its own entity, and there is no reason why its code should be owned by the website repository. Git submodules allow you to clone this dependency repo into your website directory so you can use this code without versioning it redundantly.\n\nSubmodule basics\nIf you have never worked with submodules before, here is how they work in broad strokes. (This is not an exhaustive intro.)\nWhen you add a submodule to a parent repository, the parent repository tracks the presence of the submodule, but it does not track the content. Your website repo tracks the presence of submodules to ensure that your project can be reproduced (read: cloned) with all necessary dependencies in place.2 However, your website repo is ignorant of the actual content of the submodule because the submodule code is versioned by its own separate repo. There is no need to duplicate that effort.\nUpstream changes to the submodule repo can be pulled into your website repo. This is standard workflow for Git. If you want to pin your dependency to a particular commit of the submodule, you can git checkout that commit. If you want your dependency to stay dynamically up to date with the submodule’s remote repo, checkout the desired branch and pull changes as they arise on the upstream remote.\nLocal changes to the submodule content can be pushed to remote. If you have write access to the submodule’s remote repository—either you own the repo, or it’s your fork of some other repo—you can make changes to the submodule contents from within the submodule and push those changes back upstream.3 This is just like a Git workflow where multiple users are pushing to the same remote repository, except instead of multiple users, it’s only you, editing the repo and committing/pushing changes from different endpoints. This allows you to keep the submodule content updated on all of its local and remote copies without duplicating any effort.\n\n\nHow to add your website components as submodules\nIn the spirit of modularity, there is actually nothing Blogdown-specific about including submodules within a project repository. All the same, I will discuss a Blogdown-specific example: the .Rprofile module, which I keep in its own repository here. I discuss how I manage blog posts with submodules later on, because that conversation is a little more involved.\nYou can add a submodule to your (already initialized) website repo with git submodule add [my-url] [my-destination-folder]. You will want to be strategic about where you add the repo, since it will effectively behave like a cloned repository. I often create a /submodules/ folder under my project root and clone submodules to that location.\n# from /path/to/site\nmkdir submodules\ncd submodules\ngit submodule add git@github.com:mikedecr/dots_blogdown.git\nAdding the submodule does not clone its contents. It simply registers the submodule with the repository, creating an entry in the website repo’s .gitmodules file. You have to run a separate command to actually clone the submodule repo’s contents:\ngit submodule update --init --recursive\nThe output will look like you did a git clone. At this point, there should exist a folder called /dots_blogdown/ that contains the repo contents.\nFrom there, your next step depends on how you want to use the contents of the submodule. For this particular example, we want this .Rprofile to live at the top of our website root. This ensures that the file’s code is executed when we open R to manage our website. I achieve this by linking the file to the website root (and, bonus, removing write permissions4).\n# exit /submodules/\ncd ..\n# -s = symlink, -f = force\nln -f ./submodules/dots_blogdown/.Rprofile ./.Rprofile\n# bonus: remove write-permissions (make read-only)\nchmod -w ./.Rprofile\nIt is smart to automate any post-Git processes, such as linking files to other destinations, by putting these commands and other pre-build operations in your website’s /R/build.R file. This ensures that these operations are done each time your website is built, ensuring that your website can be safely reproduced if your submodule content should ever change. With that automation in place, if I ever changed my .Rprofile repo, I never have to worry about manually re-linking my updates to the right destination. The build script does it for me.\n\n\nDeveloping within the submodule repo\nThe above instructions describe how to simply employ submodule files in your website. But suppose you wanted to change the content of the submodule files and push those changes back upstream. What would you do?\nBefore making any changes to the submodule files, make sure the submodule isn’t in detached HEAD state. A detached HEAD state is basically what happens when you have checked out a commit in isolation of the branch on which that commit lives. When you are in detached HEAD state, you are basically looking at a copy of the project, but you cannot alter the project tree itself. Any files you change cannot be committed to a persistent branch. To make permanent changes, you have to checkout the branch that you want to track and commit changes to, which is probably main.\nMake your changes. Even though you are editing a file within a submodule repository, Blogdown doesn’t know or care, so it shouldn’t behave any differently. It will knit/render blog posts and serve your website locally like nothing is wrong. That’s because nothing is wrong.\nCommit changes to submdodule files to the submodule repository. From the command line, this means you probably should cd into the submodule repo before adding any files to the index. If you do Git stuff inside of a GUI, you should be able to make the submodule appear as its own repo that you can do add/commit/push actions to. (I don’t use Rstudio, so unfortunately I don’t know if Rstudio makes this easy.) After committing to your local copy of the submodule repo, you should notice that your parent repository detects an updated commit in the submodule! You should commit that change to the parent repository as well. This simply tells the parent repo that it should consult this new submodule to reproduce the project correctly. This is important because anyone else who clones your website repository (ahem, Netlify!) will need to import the submodule at the correct commit.\nBoth submodule and parent repos can be pushed. If this is your first time pushing any submodule-related commits to Netlify, you will want to read the section about Netlify below.\nAs you get more familiar with Git, you won’t need to follow a checklist. You will simply be familiar enough with how Git works to know exactly what to do!"
  },
  {
    "objectID": "blog/modular_blog/index.html#blog",
    "href": "blog/modular_blog/index.html#blog",
    "title": "Highly modular blogging with Blogdown",
    "section": "What to do about your blog?",
    "text": "What to do about your blog?\nShould your blog be one submodule repository, or several? My current setup is to treat every blog post as its own, separate module with its own, separate repository. This keeps each post and all of its dependencies isolated from other posts, which is cleanest for me from a reproducibility and modularity standpoint.\nHowever, you may find many blog post repositories to be overkill, and would instead want a single repository containing all of your blog posts. Would that be fine?\nIn short, the single-blog-module setup may be possible, but it will likely require even more advanced Git magic than just submodules. If you really want to know the nasty technical details, you can read about the problem and one potential solution, with the caveat that I haven’t tested that workflow out. If you trust me that the single-repo workflow is pretty complicated except for people looking to increase their Git dexterity stats, you can skip ahead to read about separate repositories for each post.\n\nOne submodule for all posts: the problem\nTo explain, consider the submodule workflow mentioned earlier. If we wanted to use a “single submodule” approach to blogging, we would\n\nMove our blog posts to another repository and push it to the web.\nAdd this repository as a submodule located in your content/blog or analogous subdirectory.\nThe changes in the content/blog folder are now owned by the submodule repository. The parent repo will no longer see what’s happening in those files—only if you have made new commits.\n\nUnfortunately, this may be a critical problem for your website! This is because many themes ask you to put other important files under your content/blog directory, in addition to the posts. Many popular themes ask for a content/blog/_index.md file to manage the blog’s “listings” page. Many themes also will accept image files in that directory to use for headers and sidebars on the listings page. These files are problems for the single-repo workflow. If we let our blog submodule own the content/blog directory, those files can no longer be tracked by the parent (website) repository. Adding the files to the submodule’s .gitignore does not fix it either. So, what can be done?\n\n\nOne submodule for all posts: there might be a way\nI haven’t tested this, but there might be a way to save the unified-blog-repository workflow: you could make your blog repository a bare repository.\nA bare repository is a repository with no root directory. Now, if you have only used Git on a per-project basis, the idea of a repo with no root directory sounds unthinkable, but it is actually a common way to version your “dotfiles”. Here’s why: your dotfiles usually live at your /home/username/ or ~/ directory. Many folks want to track these files to keep certain preferences synchronized on different machines, but as you can foresee, making a Git repository track your entire ~/ folder would be a horrible and terrifying idea. Instead, people create bare repositories that only track the contents that are explicitly added to the repository, regardless of their relative location to the repo’s .git folder.\nHow might this ameliorate our workflow problem? If we want one submodule repo to track all of the posts in /content/blog, but we don’t want that repo to own the other files in that directory, we might be able to achieve that effect with a bare submodule repo. Such a repo shouldn’t be aware of the other files under content/blog, because the repo doesn’t know that it is the same folder as those files.\nAgain, try it if you want, but you have no assurances from me that it will work.\n\n\nMy choice: every post gets its own repository\nIn lieu of the “advanced solution”, we opt for peak modularity: every blog post gets its own repository.\nThis workflow sounds tedious but is actually easier than you would think, and most of the steps are identical to what I have already covered above. Here’s a quick rundown of what I do:\n\nStart on Github or whichever remote service you prefer. Make a remote-first repository for a new post (give it a meaningful title) and copy its cloning link.\nAdd the new repo as a submodule to a new folder for that post inside of /content/blog. I assume you use a “page bundle” model for organizing your blog code: separate folders for each post that contain respective index.[R]markdown files. It’s advisable to blog with page bundles even if you don’t want to implement hyper-modular blog versioning. Learn more about page bundles from Alison Hill here.\nYour .gitmodules file will automatically update to reflect the new submodule. You will eventually want to commit that change, but it doesn’t have to be now. If necessary, initialize/update the submodule to clone its contents into the new post directory.\nCheckout your desired submodule branch (e.g. main) so you can commit changes to your blog repo.\nEdit your post as you normally would by creating an index.Rmarkdown and typing away. This is where you would use renv to take a snapshot of your R package library in order to reproduce the post. Hugo will trip over the files created by renv, however, so if you want to use it (again, you should), add \"renv\" to the ignoreFiles field in your website’s config.toml (which you only have to do once per site).\nCommit changes to the blog module repository and push to remote.\nYou should notice that your parent repository detects an updated commit in the submodule. Commit that change to the parent repository as well. Pushing this website commit to remote will kick off a new Netlify build if you use continuous integration. Speaking of that…"
  },
  {
    "objectID": "blog/modular_blog/index.html#netlify-setup",
    "href": "blog/modular_blog/index.html#netlify-setup",
    "title": "Highly modular blogging with Blogdown",
    "section": "Getting it working with Netlify",
    "text": "Getting it working with Netlify\nOnce you are done getting your site looking the way you want, and all of your files are committed to the parent and child repositories, you can push your website repo to the remote that Netlify is tracking.\nExcept, whoops, your site may fail to build on Netlify. Why? Netlify works by cloning your website repository to their servers and building it with Hugo on their end. This process fails if Netlify can’t successfully reproduce your website repo with all of the submodules declared in your .gitmodules file. Such failure can happen for two benign and fixable reasons: (1) the submodule is a private repository, or (2) the submodule was added using the repo’s ssh URL instead of the https URL.\nIn either case, all you have to do is add ssh-keys to grant Netlify access to these repositories. It sounds complicated and jargony, but Netlify describes the whole process right here.\nOnce Netlify has access to the repositories, it can build its own copy of your website. This is because your parent Git repository spells out all of the instructions for cloning the required submodules at their requested commits."
  },
  {
    "objectID": "blog/modular_blog/index.html#closing-note",
    "href": "blog/modular_blog/index.html#closing-note",
    "title": "Highly modular blogging with Blogdown",
    "section": "Closing note",
    "text": "Closing note\nThis post presents an opinionated view of a Blogdown website as a collection of modules and a corresponding workflow for managing them. If you find it helpful, awesome! But as always, you should do what works for you. It happened to be the case that I had a particular set of problems and a desire to strengthen some skills could help me solve them."
  },
  {
    "objectID": "blog/modular_blog/index.html#footnotes",
    "href": "blog/modular_blog/index.html#footnotes",
    "title": "Highly modular blogging with Blogdown",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe system isn’t perfect. Some themes define special fields whose values are specified in your content files, but the main idea is there.↩︎\nThis is how Netlify builds your site, in fact. Netlify clones your website’s Git repository and builds it on their servers, so this is actually super important.↩︎\nJust be sure you have checked out a branch (not in detached HEAD state) before you commit changes to the submodule files. More here.↩︎\nBecause I forcefully link the .Rprofile file from the submodule to the website root, any changes I make to the copy at the root would be overwritten if I ever re-linked the file. This is why I make the file read-only: to prevent myself from editing the wrong copy of the file. Just a little trick to guard against bugs :)↩︎"
  },
  {
    "objectID": "blog/mandelbrot_iterative_recursive/index.html",
    "href": "blog/mandelbrot_iterative_recursive/index.html",
    "title": "The Mandelbrot set, in R, with recursion instead of iteration",
    "section": "",
    "text": "One fun lessons in the Structure and Interpretation of Computer Programs1 is the translation of functions between iterative and recursive approaches.\nTo show the difference between iterative and recursive procedures, let’s make a quick example where we raise a number to some (positive integer) power. We could implement an iterative approach in R using a for loop. We call this iterative because each pass through the loop is an “iteration”.\n\n# power must be an integer &gt;= 0\npow_iter = function(x, power) { \n    if (power == 0) { return(1) }\n    if (power == 1) { return(x) }\n    # the iterative part:\n    result = x\n    for (i in 2:power) {\n       result = result * x \n    } \n    return(result)\n}\n\nOr we could do this with a recursive function. We call this function “recursive” because the function calls itself in its own implementation.\n\npow_rec = function(x, power) {\n    if (power == 0) { return(1) }\n    if (power == 1) { return(x) }\n    # here be recursion\n    x * pow_rec(x, power - 1)\n}\n\nThe recursive approach uses the fact that \\(x^n = x \\times x^{n - 1}\\) to decompose your query into an expression that looks like \\(x \\times x \\times x \\ldots \\times x\\) which is evaluated all at once. And, I gotta say, recursion feels way slicker. We have just one powerful line of code that accomplishes as much as the four-line iterative approach, most of which was simply boilerplate for passing data in and out of the loop. The recursive implementation may be a bit of a brain-bender at first, but the more you use recursion, the more comfortable you get with it, and the more opportunities you find to use it productively.\nFor completeness, let’s verify that the approaches agree.\n\n(example = 7^6)\n## [1] 117649\npow_iter(7, 6) == example\n## [1] TRUE\npow_rec(7, 6) == example\n## [1] TRUE\n\nThis blog post will employ this duality between recursion and iteration to create images of the Mandelbrot set using R. Although there are plenty of other blog posts that visualize the Mandelbrot set in R,2 I have only seen them implement an iterative approach with a loop. But the Mandelbrot set is mathematically defined with recursion! So we ought to be able to implement a recursive approach that is, hopefully, more succinct and slick.3\nIn the remainder of this post, we will quickly introduce the math behind the Mandelbrot set, define a recursive function to produce “Mandelbrot data”, and visualize the famous fractal image that the math produces."
  },
  {
    "objectID": "blog/mandelbrot_iterative_recursive/index.html#footnotes",
    "href": "blog/mandelbrot_iterative_recursive/index.html#footnotes",
    "title": "The Mandelbrot set, in R, with recursion instead of iteration",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA classic programming book. You can read it online for free as HTML or as a PDF↩︎\nThere is even an R package called mandelbrot which although it dispatches the implementation to C, also uses iteration.↩︎\nWe will also use newer plotting tools to visualize the results—ggplot instead of graphics::image like the older posts.↩︎\nhttps://youtu.be/SxdOUGdseq4↩︎"
  },
  {
    "objectID": "blog/fp_basics/index.html",
    "href": "blog/fp_basics/index.html",
    "title": "Obnoxious functional programming in R",
    "section": "",
    "text": "When people talk about “functional programming” in R, they usually mean two things.\n[1] is definitely the more popular understanding than [2], which is a shame because functional programming outside of R goes way deeper, and get way weirder, than apply(function, data). This post is an attempt to explain a little of that weirdness and implement it in R with some obnoxious examples.\nReaders with more FP experience will recognize that this post isn’t the most rigorous. They may also realize that R is backed by a lot of functional ideas even if many users may not recognize or articulate those ideas readily. That’s fine if you know that stuff, but this post is meant to be light on the technicals."
  },
  {
    "objectID": "blog/fp_basics/index.html#what-is-functional-programming",
    "href": "blog/fp_basics/index.html#what-is-functional-programming",
    "title": "Obnoxious functional programming in R",
    "section": "What is functional programming?",
    "text": "What is functional programming?\nLet’s do the Wikipedia thing. From the functional programming article:\n\nIn computer science, functional programming is a programming paradigm where programs are constructed by applying and composing functions.\n\nYou may be thinking, “That sounds pretty unremarkable. I already know that I write functions and apply them. Why do we need a name for this?”\nWell… read on:\n\nIt is a declarative programming paradigm in which function definitions are trees of expressions that map values to other values, rather than a sequence of imperative statements which update the running state of the program.\n\nThere are a many information-dense pieces to that sentence that will be difficult to appreciate without a little more theory. But let’s try to break it apart, starting in the middle.\n\n“Functions map values to other values”\nYou have probably seen function definition written like this, \\[\ny = a + bx\n\\] and we say that \\(y\\) is a function of \\(x\\). We can label the transformation of \\(x\\) as \\(f\\) and say \\(y = f(x)\\). Easy!\nBut there is another way to write statements like: \\[\nf : X \\to Y\n\\tag{1}\\] which reads, “\\(f\\) is a map from \\(X\\) to \\(Y\\)”, where capital-\\(X\\) and capital-\\(Y\\) are sets of values from which little-\\(x\\) and little-\\(y\\) are drawn. Which is to say: \\(f\\) is like a lookup table that outputs a value from \\(Y\\) when it sees an input from \\(X\\). In this example, the value of \\(y\\) that it returns is equal to \\(a + (b \\times x)\\).2\n\n\n“Function definitions are trees of expressions …”\nReferring to functions as “trees” won’t make sense until we talk about function composition and associativity.\nFirst, composition. If the notation in Equation 1 is new to you, your mental picture of function composition probably looks like this. We can take two functions \\(f\\) and \\(g\\)…\n\\[\\begin{align}\n    y &= f(x) \\\\\n    z &= g(y)\n\\end{align}\\]\nand put them together… \\[ \\begin{align} z &= g(f(x)) \\end{align}  \\tag{2}\\] and then we can represent that composition with a different symbol \\(h\\), so that that \\(h(x)\\) is equivalent to \\(g(f(x))\\).\nThere’s nothing incorrect about that approach, but it is verbose. We actually don’t have to refer to any function arguments or values (x$, \\(y\\), or \\(z\\)) to introduce \\(h\\). Instead we can define \\(h\\) in a “point-free” style, \\[ h = g \\circ f  \\tag{3}\\] where the symbol \\(\\circ\\) refers to function composition (in a leftward direction). The expression \\(g \\circ f\\) is a new function, which we can say aloud as “\\(g\\) compose \\(f\\)” or “\\(g\\) after \\(f\\)”. Because the composition is itself a function, we can pass it an argument: \\((g \\circ f)(x)\\) would spit out the value \\(z\\), just like in Equation 2.\nOkay, now associativity. Function composition is associative, which means compositions can be “grouped” in whatever way you want as long as you don’t change the ordering of the functions.\nTo demonstrate, let’s say we have three functions, \\(f\\), \\(g\\), and \\(j\\), and we want to compose these functions. There are a few ways to do it:\n\nAs one single chained composition: \\(j \\circ g \\circ f\\), which if applied to \\(x\\) would be equivalent to \\(j(g(f(x)))\\).\nIntroduce \\(h = g \\circ f\\), and rewrite as \\(j \\circ h\\). This is the same as \\(j \\circ (g \\circ f)\\) or \\(j(h(x))\\).\nCompose \\(j\\) and \\(g\\) into some \\(d\\) and say \\(d \\circ f\\), which is the same as \\((j \\circ g) \\circ f\\) or \\(d(f(x))\\).\n\nAll of these expressions are equivalent.\nOkay, recap. As long as the output of one map is the same “type” as the input to the some other map, we can compose those two functions. And we can take a series of compositions and group them in whatever way we want, as long as the input and output types conform.\nBut how is this a “tree”? I will make a little graph with nodes and edges that shows the basic idea. Caveats up front that this will be pretty informal, but let’s say the functions are nodes, and the edges point in the direction of composition. Let’s say function \\(f\\) takes two arguments, \\(a\\) and \\(b\\).\n\n\n\n\n\n\n\nF\n\n  \n\na\n\n a   \n\nf\n\n f   \n\na-&gt;f\n\n    \n\nb\n\n b   \n\nb-&gt;f\n\n   \n\n\n\n\n\nBut maybe the value of \\(a\\) is the result of a function that itself takes two arguments, \\(p\\) and \\(q\\), and \\(b\\) is the result of a function of \\(r\\).\n\n\n\n\n\n\n\nF\n\n  \n\nb\n\n b   \n\nf\n\n f   \n\nb-&gt;f\n\n    \n\na\n\n a   \n\na-&gt;f\n\n    \n\np\n\n p   \n\np-&gt;a\n\n    \n\nq\n\n q   \n\nq-&gt;a\n\n    \n\nr\n\n r   \n\nr-&gt;b\n\n   \n\n\n\n\n\nThe tree lets us express composition and associativity in a different way: \\(f\\) doesn’t really care how you compose or group the operations ahead of it, as long as the values that you pass are \\(a\\)-like and \\(b\\)-like.\n\n\n“…Rather than a sequence of imperative statements that update the running state.”\nThis is the really important stuff.\nMost of the time when we do data analysis, we write code that updates the state of some data. Say I start with some \\(x\\), and then I do functions \\(f\\), \\(g\\), and \\(j\\) to it.\n\ny = f(x)\nz = g(y)\nw = j(z)\n\nThere is some data x that we alter, in steps, by doing things to it, each time saving some data at some intermediate state.\nThe functional approach would be different. Instead of spending most of our time, energy, and keystrokes passing data around, we spend these resources writing functions.\n\nh = compose(g, f)\na = compose(j, h)\n\nAnd only later (say, in some main() routine) do we apply our ultimately-composed functions to data. The data have to be acted on eventually, but we do it only after we compose a map from the data to some endpoint.\nStated a different way, when we do more imperative programming, the present state of x is always our concern. Our path from beginning to end requires us to leave x in some intermediate state at many different steps, and if the state of x is exposed along the way, there can be problems that mutate x into something you don’t want it to be. When we do functional programming, however, we write the recipe for what we will do to x if we ever encountered such an x. The roadmap is set ahead of time, so the intermediate state of x is obscured behind the functions that we compose. We can’t really get to the intermediate state because we aren’t supposed to be able to.\nThis might be hard to envision because we haven’t talked tangibly about functional programming interfaces in R yet, so let’s do that."
  },
  {
    "objectID": "blog/fp_basics/index.html#functional-fundamentals-with-r",
    "href": "blog/fp_basics/index.html#functional-fundamentals-with-r",
    "title": "Obnoxious functional programming in R",
    "section": "Functional fundamentals, with R",
    "text": "Functional fundamentals, with R\nWe saw a notational approach to function composition above that looked like this: \\[\\begin{align}\na = (j \\circ g \\circ u)\n\\end{align}\\] If we could do that in a computer, it might look like this:\n\na = compose(j, g, f)\n\nThere are two important things to remember about this.\n\nFunction composition creates a new function. It does not immediately evaluate the function on any data. In the above example, a is the composition, and it is entirely ignorant of any data you may pass to it.\nFunction compositions are themselves composable, so we want a framework to compose a lot of functions all at once with whatever associative groupings we want. We saw above (with math) that we could compose an arbitrary sequence of functions and the result should be well-behaved (as long as their input and output types conform—more on that in a bit). We would like to achieve that behavior in the code as well.\n\nWe start with a primitive operation to compose just two functions. We call it compose_once—it implements only one composition of a left-hand and a right-hand function.\n\n# returns the new function: f . g\ncompose_once = function(f, g) {\n    function(...) f(g(...))\n}\n\nRead this closely. compose_once takes two arguments, and those arguments are functions. We do not know or care what those functions are. We also return a new function, rather than some data value. That new function defines a recipe for evaluating the functions \\(f\\) and \\(g\\) in sequence on some unknown inputs ..., whever it happens to come across those inputs—we didn’t pass any ... yet. This is also intentional: composition does not care what the data are. It only knows how to evaluate functions in order.\nThis might feel weird at first, but it will give us legible behavior right from the start. For example, we often want to know how many unique elements are in some data object. In base R, we might ask length(unique(x)). The dplyr package provides a combined function n_unique(x), but we haven’t invented dplyr yet, so we have to make n_unique ourselves:\n\n# we save the function returned by compose_once\n# no computation on data is done yet.\nn_unique = compose_once(length, unique)\n\n# examine the object to convince yourself of this\nn_unique\n## function(...) f(g(...))\n## &lt;environment: 0x12a20e5c0&gt;\n\n# apply the function on some data\nx = c(0, 0, 0, 1, 2)\nn_unique(x)\n## [1] 3\n\ncompose_once lets us compose two functions, but we want to compose an arbitary sequence of functions into one composition. So we extend this interface by performing a reduction across an array of functions.\n\n# pass vector fns = c(f1, f2, ..., fn)\ncompose = function(fns) {\n    Reduce(compose_once, fns, init=identity)\n}\n\nIf you aren’t familiar with reductions, they are an efficient trick to collapse an array (loosely speaking) of input values into one output value, accumulating at each step the results of some binary operation. If you want to learn more about reduction, follow this footnote.3 We also supply an argument to init, which isn’t strictly necessary in this R example but is interesting (IMO) in the mathematical context of function composition, which I explain in this other footnote.4\nLet’s see it in action. Let’s do an additional step to convert n_unique into an English word.\n\n# same as n_unique but convert to a word\nenglish_n_unique = compose(c(english::english, length, unique))\n\n# apply to our vector x\nenglish_n_unique(x)\n## [1] three\n\nAnd just to underscore your confidence in the associativity of composition, we can exhaustively regroup these compositions without affecting the results.\n\ncompose(c(english::english, compose(c(length, unique))))(x)\n## [1] three\ncompose(c(compose(c(english::english, length)), unique))(x)\n## [1] three\n\n\nTypes are our guide.\nYou may have seen online discussions about “strong typing”. Types are like representations of data in the computer. We have basic data types like short integers, long integers, characters, and strings. Types encode abstract features of some data that without caring about the data values themselves. Languages implement other data structures like lists, dictionaries, arrays, tuples, etc. that we can also call types for our purposes.\nYou may have seen some people discuss the “type flexibility” of R or Python as advantages. Well…sorry! Functional programming like strong types because strong types provide structure to function composition at scale. More specifically, two functions can be composed if the output type of one function matches the input type of the next function. If we can trust this fact, we can build really big, abstract creations out of function composition, maybe even creations that are so big and complicated that we struggle to keep track of it all in our brains. But these creations are virtually guaranteed to work as long as they are composed of functions with conforming input and output types.\nNot coincidentally, the associativity of function composition is additionally helpful for API design. If we have to build a big, abstract structure, we always have the option to group some of the operations together if they perform a coherent and useful operation. Intermediate functions are more useful than intermediate state because at least an intermediate function is legible and potentially reusable. Chances are your intermediate data are not that legible or useful.\nLet’s look at the “type roadmap” for the english_n_unique example. We started with a vector type and mapped to an english type. How did we know it would work? We can diagram the types. \\[\n\\begin{align}\n   \\mathtt{unique} &: \\mathtt{many} \\to \\mathtt{vector} \\\\\n   \\mathtt{length} &: \\mathtt{many} \\to \\mathtt{integer} \\\\\n   \\mathtt{english::english} &: \\mathtt{many} \\to \\mathtt{vector}\n\\end{align}\n\\] We know that unique takes objects of various types (which I represent as many to say “this function has many methods for different data types”) and returns a vector of unique elements. We know that length takes many and returns its integer length, and that should work for a vector input. And english::english turns many objects into english type, which ought to work for an integer input. But we know that these operations should compose because we can know that we can pass a vector to unique, a vector to length, and an integer to english::english. Stated differently, if I know the type behavior of each function, I know the type behavior of the compositions. That is an extremely useful and powerful foundation for building big things."
  },
  {
    "objectID": "blog/fp_basics/index.html#something-bigger-means-within-groups",
    "href": "blog/fp_basics/index.html#something-bigger-means-within-groups",
    "title": "Obnoxious functional programming in R",
    "section": "Something bigger: means within groups",
    "text": "Something bigger: means within groups\nThis is where we really start to see the difference between typical “imperative” programming and functional programming “trees”. Let’s take something we commonly do in R, calculate means within groups of data, and do it with our functional tools. We will write this in a functional setup that, I admit, is sort of an ugly interface. But the point isn’t to write a perfect interface, it’s to demonstrate what it’s like to be functional. So let’s roll with some ugly code for a while.\nLet’s take the mtcars data frame…\n\nhead(mtcars, 5)\n##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n…and calculate the mean of each variable as a new data frame with one row per group. We want to be able to specify the group on the fly.\nWe want to chart a course through this problem that composes small, re-usable steps on well-defined types. Here is my plan:\n\ndefine a function that maps a data frame of raw data to a data frame of column means.\ndefine a function that applies any other function to grouped data.\ncompose a function that implements a split-apply-combine which is the composition of the above steps.\n\n\n1. Means for a data frame.\nWe create a function by composing the following functions that map the following types:\n\ncolMeans: data frame \\(\\to\\) vector\nas.list: vector \\(\\to\\) list\ntibble::as_tibble: list \\(\\to\\) data_frame\n\nYou can see how these functions take us incrementally from data frame, to vector, to list, back to data frame. So we know that this composition should work before we ever test it on data.\n\nmeans = compose(c(tibble::as_tibble, as.list, colMeans))\n\nTesting it on the full data:\n\nmeans(mtcars)\n## # A tibble: 1 × 11\n##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1  20.1  6.19  231.  147.  3.60  3.22  17.8 0.438 0.406  3.69  2.81\n\n\n\n2. Apply a function over groups\nRemember, functional programming hasn’t been invented yet, so we can’t simply do his with lapply. We can, however, reimplement lapply knowing what we know about functional programming.\nLet’s create an function that takes any function f and an iterable object l, and returns a list.\n\napply_on_elements = function(l, f) {\n    # initialize an empty list\n    v = vector(mode = \"list\", length = length(l))\n    # assign f(x) for each x in l\n    for (x in seq_along(l)) {\n        v[[x]] = f(l[[x]])\n    }\n    return(v)\n}\n\nNotice, this function takes two objects as arguments and returns one object. In order to nicely compose it with other functions that take and return only one object, I want a way to reduce the arguments required when I call it. This will look a little weird, but I am going to create a common object in functional programming called a partial function or curried function. A partial function is a function that has some of its arguments fixed ahead of time. Let’s define it before I explain further:\n\npartial_apply = function(f) {\n    function(x) apply_on_elements(x, f)\n}\n\nSo partial_apply takes a function f and returns a new function. That new function applies f to some iterable object x to return a list. But the function isn’t evaluated; it is only created, because I never provide an x in the outer function. The user has to pass x to evaluate the partial at a later time.\nHere we use this tool to create a partial length, which we apply to some x.\n\nlens = partial_apply(length)\n\n# examine, it's a function\nlens\n## function(x) apply_on_elements(x, f)\n## &lt;environment: 0x12a354c18&gt;\n\nlens(x)\n## [[1]]\n## [1] 1\n## \n## [[2]]\n## [1] 1\n## \n## [[3]]\n## [1] 1\n## \n## [[4]]\n## [1] 1\n## \n## [[5]]\n## [1] 1\n\nWe see more of that delayed evaluation behavior. This lets us create a function that applies our earlier means to groups of data.\n\npartial_apply(means)\n## function(x) apply_on_elements(x, f)\n## &lt;bytecode: 0x1388538c0&gt;\n## &lt;environment: 0x138a84eb0&gt;\n\n\n\n3. Apply our partial function to data.\nRather, create a function that would do that, if it were evaluated.\nThis function should turn a data frame into an iterable collection of groups (like a list), apply means to each element of that collection, and return a final data frame that re-merges the collection. Here’s how we map these steps from type to type.\n\nsplit: pair of (data frame, vector) \\(\\to\\) list\npartial_apply(means): list \\(\\to\\) list\ndplyr::bind_rows: list \\(\\to\\) data frame\n\nThe composition of all these steps creates a function that eats a data frame and returns a data frame.\n\nmeans_by = compose(c(dplyr::bind_rows, partial_apply(group_means), split))\n\n\n\nPut it all together.\nThe code below retraces our steps. Step (1) creates our means function. Step (2) creates some functional infrastructure to apply functions over iterable objects, which isn’t the kind of thing we would ordinarily have to mess with as end-users. And step (3) composes our means function with the iteration tools to make our eventual result.\n\n# 1. small function to be applied\nmeans = compose(c(tibble::as_tibble, as.list, colMeans))\n\n# 2. infrastructure layer:\n# you can see why these would be generally useful for many problems\n# 2a. reimplement *-apply()\napply_on_elements = function(l, f) {\n    v = vector(mode = \"list\", length = length(l))\n    for (x in seq_along(l)) {\n        v[[x]] = f(l[[x]])\n    }\n    return(v)\n}\n# 2b. partial apply\npartial_apply = function(f) {\n    function(x) apply_on_elements(x, f)\n}\n\n\n# 3. interface layer\nmeans_by = compose(c(dplyr::bind_rows, partial_apply(means), split))\n\nAgain, given that step (2) is rebuilding the wheel, it’s pretty impressive how little code goes into steps (1) and (3) to achieve the end results. The interface currently isn’t as succinct as dplyr grouping and summarizing, but remember, functional programming hasn’t been invented yet. But moreover, you can start to imagine how tools like partials can be stacked to create tools that are essentially as powerful as dplyr with nice interfaces too, even if those interfaces are different from the imperative steps you are used to.\nLet’s apply the ultimate function, means_by, to various groups in mtcars.\n\nmeans_by(mtcars, mtcars$cyl)\n## # A tibble: 3 × 11\n##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1  26.7     4  105.  82.6  4.07  2.29  19.1 0.909 0.727  4.09  1.55\n## 2  19.7     6  183. 122.   3.59  3.12  18.0 0.571 0.429  3.86  3.43\n## 3  15.1     8  353. 209.   3.23  4.00  16.8 0     0.143  3.29  3.5\nmeans_by(mtcars, mtcars$vs)\n## # A tibble: 2 × 11\n##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1  16.6  7.44  307. 190.   3.39  3.69  16.7     0 0.333  3.56  3.61\n## 2  24.6  4.57  132.  91.4  3.86  2.61  19.3     1 0.5    3.86  1.79\nmeans_by(mtcars, mtcars$am)\n## # A tibble: 2 × 11\n##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1  17.1  6.95  290.  160.  3.29  3.77  18.2 0.368     0  3.21  2.74\n## 2  24.4  5.08  144.  127.  4.05  2.41  17.4 0.538     1  4.38  2.92\nmeans_by(mtcars, mtcars$gear)\n## # A tibble: 3 × 11\n##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1  16.1  7.47  326. 176.   3.13  3.89  17.7 0.2   0         3  2.67\n## 2  24.5  4.67  123.  89.5  4.04  2.62  19.0 0.833 0.667     4  2.33\n## 3  21.4  6     202. 196.   3.92  2.63  15.6 0.2   1         5  4.4\nmeans_by(mtcars, mtcars$carb)\n## # A tibble: 6 × 11\n##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1  25.3  4.57  134.   86   3.68  2.49  19.5   1   0.571  3.57     1\n## 2  22.4  5.6   208.  117.  3.70  2.86  18.2   0.5 0.4    3.8      2\n## 3  16.3  8     276.  180   3.07  3.86  17.7   0   0      3        3\n## 4  15.8  7.2   309.  187   3.60  3.90  17.0   0.2 0.3    3.6      4\n## 5  19.7  6     145   175   3.62  2.77  15.5   0   1      5        6\n## 6  15    8     301   335   3.54  3.57  14.6   0   1      5        8\n\n\n\nEnding notes on the tidyverse\nNow that we have invented functional programming, we can better appreciate how tidyverse tools leverage functional infrastructure to make nice APIs, even if those APIs feel way less hardcore-functional than the example we just created.\nThe pipe operator. You may have thought to yourself, composing “left” sure is harder to read than composing “right” like the pipe operator. First, functional programming hadn’t been invented yet, so you can’t blame me for not knowing about the pipe operator. Second, creating a composition function that reads more “linearly” is easy with one extra step: reversing the direction of the Reduce.\n\npipe = function(fns) {\n    # reverse the order of fns before composing\n    compose(rev(fns))\n}\n\nNow we can write our means_by function with more linear recipe that reminds us more of tidyverse code.\n\nmeans_by = pipe(c(split, partial_apply(means), dplyr::bind_rows))\nmeans_by(mtcars, mtcars$cyl)\n## # A tibble: 3 × 11\n##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1  26.7     4  105.  82.6  4.07  2.29  19.1 0.909 0.727  4.09  1.55\n## 2  19.7     6  183. 122.   3.59  3.12  18.0 0.571 0.429  3.86  3.43\n## 3  15.1     8  353. 209.   3.23  4.00  16.8 0     0.143  3.29  3.5\n\nPartial functions. The partial_apply function might have been the weirdest-feeling step in the earlier example. But if you squint at it, you realize that this is sort of what the tidyverse does with tidy evaluation. Whenever you pipe a data frame to mutate or filter and so on, and you write expressions on unquoted variables, those arguments are (in a way) creating new partial functions. There is also delayed evaluation of that function: the unquoted expressions are not evaluated on the spot, but instead are translated to create the partial function you actually want. It is that translated/partial function that actually is evaluated when you pass it your data frame."
  },
  {
    "objectID": "blog/fp_basics/index.html#footnotes",
    "href": "blog/fp_basics/index.html#footnotes",
    "title": "Obnoxious functional programming in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMost of the time, loops in R are slow because people are doing unwise things with them.↩︎\nYou may have assumed that \\(x\\) and \\(y\\) are numbers and that the operations \\(+\\) and \\(\\times\\) refer to the addition and multiplication of numbers. It’s fine if you made those assumptions, but it wasn’t necessary. \\(X\\) and \\(Y\\) could be other sets containing god-knows-what, and there are many kinds of operations that follow “algebraic” patterns akin to addition and subtraction. But we leave algebraic data types for another day.↩︎\nYou may have seen a reduction in an operation like max()—for an array of numbers, recursively check to see if left is greater than right, passing the winner to the next iteration as left. In this case, however, the array we are collapsing is full of functions rather than, say, numbers. And the operation we apply as we accumulate is compose_once(left, right) rather than left &gt; right. The result of the reduction is a function that encodes the composition of all functions in fns from left to right.↩︎\nThe role of an initialization in a reduction is to handle an identity condition: how do we handle “empty” values along the reduction without affecting the output. Stated differently, what value can we pass to ensure a “no-op” when the reduction is applied, which can also be used as an initial value to left. For example, if we reduce an array of numbers by addition, the initialization would be the value 0, because applying + 0 to any number gives you the original number. Same with multiplication and the value 1. When we are reducing functions by composing them, we use the identity function. Not surprisingly, the identity function simply returns the function arguments unaffected: identity(x) gives you x. For function composition, compose(f, identity) is equal to compose(identity, f) is equal to f. Now you’re programmin with abstractions!↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "michael decrescenzo",
    "section": "",
    "text": "— work —\nI work in low-latency trading as a quantitative researcher. I use mathematical models, statistics, and novel computational tools to build trading strategies that go very fast.\nMy work deals heavily with “market microstructure”, which in plain English means, modern financial markets are rule-based systems built on software and hardware in physical computers, the design and capabilities of those systems have real consequences for fast trading.\n\n\n\n\n\n\n\n\n\n— background —\nI have a Ph.D. in political science from the University of Wisconsin–Madison, where I was a graduate affiliate of the Elections Research Center. My graduate work focused on Bayesian and causal methods for studying U.S. elections and public opinion. My dissertation built new Bayesian models to measure latent political ideology in congressional districts.\n\n\n— this site —\nThe source code for the site is on Github. The blog posts are all separate repositories for your cloning and forking needs; find them here. The site is built with Quarto, a multilingual publishing system backed by Pandoc, and hosted on Netlify."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Posts are timestamped with an approximate “first-published” date, but I tend to revise them after the fact.\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nMar 11, 2023\n\n\nThe Mandelbrot set, in R, with recursion instead of iteration\n\n\n\n\nAug 31, 2022\n\n\nTreating \\(\\mathrm{\\LaTeX}\\) like a programming language: the case of color-coded equations\n\n\n\n\nAug 17, 2022\n\n\nObnoxious functional programming in R\n\n\n\n\nMay 15, 2022\n\n\nReplication code should be more usable.\n\n\n\n\nNov 6, 2021\n\n\nHighly modular blogging with Blogdown\n\n\n\n\nJun 30, 2020\n\n\nNon-flat implications of flat priors\n\n\n\n\n\n\nNo matching items"
  }
]