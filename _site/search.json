[
  {
    "objectID": "detritus/mod/index.html",
    "href": "detritus/mod/index.html",
    "title": "Highly modular blogging with Blogdown",
    "section": "",
    "text": "When I finished graduate school, I tore down my website.\nFor a handful of reasons. I no longer needed a website that cried out, “Help me, I’m finishing my PhD and I need to escape.” I didn’t need to showcase unpublished papers, teaching resources, or old blog posts that I had grown detached from. It was time for a clean reset.\nBut if you work with Blogdown, you know that starting over is laborious. Not that Blogdown isn’t great, because it is. It’s that, when you’re a finicky person like me, setting up a website with the right balance of capable features, pleasant aesthetics, and a principled codebase is legitimately challenging. I was encountering the same familiar challenges over and over.\nFor example, the site’s Hugo theme. I would take a lot of Hugo themes for test-drives. Hugo advertizes themes as if they were completely modular with respect to your /content/ folder. For most themes, this is a lie. Themes usually want too many bespoke variables or file structures in your website content. Some amount of this is okay, but it comes at a cost. If you really want to take a theme for a spin, I would find it easier to create an entirely new blogdown::new_site() than to change my theme in an existing site.\nExcept now you’re dragging the same files around your computer all over again. Ugh, this new website directory needs your blog source files, your website-level .Rprofile that controls Blogdown’s build behaviors, the jpg/png image files that you use to brand yourself online, etc… And maybe these files need to go in different folders or be given different file names from the previous theme. After a while, these files no longer have a single authoritative “home” on your computer, and you may have multiple conflicting(!) versions of these files across your various experimental website folders.\nAnd then there’s reproducibility. Even after lugging around all the same files to the new site, good luck getting your blog posts to render if your package library has changed since they were written, which it probably has. Danielle Navarro wrote about reproducibility in Rmarkdown blogging, and argues convincingly that the best way to protect your .Rmarkdown posts from this rebuilding risk is to create a dedicated package library for each separate blog post using renv. This sounds intense at first, but the underlying principle is simple, which makes it a good solution to a difficult problem.\nThis post will continue that pattern: intense at first, but well-founded, solutions for difficult problems."
  },
  {
    "objectID": "detritus/mod/index.html#what-this-post-is-about-modularity",
    "href": "detritus/mod/index.html#what-this-post-is-about-modularity",
    "title": "Highly modular blogging with Blogdown",
    "section": "What this post is about: modularity",
    "text": "What this post is about: modularity\nWhat we want is a principled and robust approach for managing the many interlocking components of your website. Specifically, we explore the modularity of the elements in your website. I take the view that your website is a collection of modular components that are better managed independently, with different Git repositories for the different site components. Yes, managing your website with multiple repositories. Stay with me.\nThe modules that compose your website include your theme, your blog posts, your Blogdown build preferences (implemented in your .Rprofile), and maybe more. These modular components come together at the nexus of the website, but as I will argue, these components should not belong to the website. Why not? Because these components can be re-used across different websites or substituted with other similar components. Hugo already flirts with this idea in its core design by separating the /theme/ directory from the /content/ directory, as if to say, “these components can be combined but do not depend on one another.” This post takes an opinionated stance that such modularity is a good idea and should be assertively extended to other components of your Blogdown site. That said, I make no assertion that this stance is objectively correct—only that it has been useful enough for me that I wanted to share some thoughts about the principles and processes at work. (You should do what works for you!)\nModularity as a software philosophy is one thing, but implementing it in code requires technical solutions. This post will discuss how to achieve this using Git submodules, an intermediate-level Git construct that, if you’re like me, is somewhat familiar but somewhat intimidating. In short, a Git submodule is a repository-within-a-repository. It has its own version history that is distinct from the “parent” repository. In this post, I provide a simple tour of submodules and how they can be used to structure your website workflow. We will recast our website as the “primary” repository, and we import other modular site components (like our blog posts) as Git submodules. In case you host your blog on Netlify, I will also discuss how to ensure that Netlify can build your site successfully.\n\nAside: some terminology\nThis discussion will involve plenty of concepts that sound similar to one another but should be understood as distinct things. I want to flag these concepts so that we understand each other better.\nDirectory vs. repository. A directory is a folder on your computer that holds files. A (Git) repository tracks changes to files. For many projects, the project’s root directory is entirely managed by one repository, so the distinction between the two may be blurred. When Git submodules are involved, this is no longer true. Your website directory will be managed by one repository, and sub-directories below your website will be managed by other repositories.\nWebsite vs. module. The website is the entire project that puts your website online. Your website will contain various modules that combine to build the entire project. Your blog posts will be considered a module (or several modules, depending on your implementation). Your theme is another module. Think of modules as building blocks for your website that can be stacked, swapped out, and so on.\nParent repository (for the website) vs. child repository (for the module), a.k.a. “submodule”. The website and the module will be versioned by separate repositories. We can refer to the over-arching project repo (the website) as the “parent” repo and the module repo as the “child” repo. A “Git submodule” is a Git construct that is overlaid onto this relationship between repositories. A repository, in isolation, is simply a repository. But if you import a repository into another project as a dependency, Git designates the dependency as a “submodule” to the parent repository, and this affects our Git workflow as a result. I explain all of that below."
  },
  {
    "objectID": "detritus/mod/index.html#websites-are-a-collection-of-modules",
    "href": "detritus/mod/index.html#websites-are-a-collection-of-modules",
    "title": "Highly modular blogging with Blogdown",
    "section": "Websites are a collection of modules",
    "text": "Websites are a collection of modules\nModules are like little building blocks, and your website has plenty of them. Setting aside any formal definition of what would mathematically be considered a “module”, let’s crudely define them as structures in your website that are agnostic to the content of other structures. We may be able to replace modules with other modules, or remove modules entirely, without affecting the core function of other modules.\nHere are some examples from my own workflow. I consider my blog posts, Hugo theme, and blogdown build settings (in my site-level .Rprofile) as modular components within the website as a whole, and I version each component with its own separate repository. Here is how I justify this view for each component:\n\nBlog posts: The content of a blog post is completely separable from the website repo. We can take a blog post and locate it in a different website, and the blog post should still be meaningful (and reproducible) unto itself. Many blogdown users remake their websites and carry their old blog posts to the new sites, which shows that the blog content doesn’t functionally depend on the website.\nIt turns out that, for blog posts, modularity and reproducibility are pretty closely related. In her discussion of blog reproducibility, Danielle Navarro touched on the principle that a blog should be “encapsulated” or “isolated” away from the broader website to robustify the blog against other dependencies. By insisting that blog posts also be modular, not only is the blog protected from the website’s computational environment, we can control each post independently of one another, move posts around across contexts, and remove posts entirely without side-effects.\nThis also affects how we treat the blog post’s dependencies. Suppose that your post includes an analysis on a data file that you read from disk. This file should belong to your blog post—and be versioned by that blog post’s Git repository—not your website. This means you should keep all of these files in the blog post directory, and forget about the website’s /static/ folder except for files that rightfully belong to the website.\nHugo theme: Hugo is designed such that the /content/ of a website (specified in markdown files) is more-or-less independent of its /theme/. The same theme can be used for multiple websites, and a single website can (in theory1) swap out one theme for another. Because themes are managed with Git repositories already, you can pull theme updates from their remote repositories without overwriting any bespoke theme customizations specified in your /layouts/ folder.\nBlogdown complicates this somewhat. When you install a theme with blogdown::install_theme(), Blogdown actually deletes the theme’s .git directory. (At least, this was my experience.) This is probably for ease-of-use among users who would not appreciate having to manage the theme as a submodule. But we are enthusiastic seekers of modularity, so we want to keep that upstream remote connection alive. As such, I installed my site’s Hugo theme using Git submodule operations instead of installing it with blogdown::install_theme().\n\n\nThe website .Rprofile file: You may have a global .Rprofile file, but it is an increasingly common Blogdown workflow staple to set up a website-specific .Rprofile to control Blogdown’s build behavior. How is this a module? Your blogdown build preferences are probably not specific to this website repository. Instead, it is likely that your preferences reflect your workflow for blogging in general and could be equally applicable to any other website repo you create or manage. If you change your blogdown workflow in a way that bears on this .Rprofile file, that change may affect all of your blogdown websites equally! Managing these .Rprofiles separately for each website would be inefficient and error-prone, so instead we manage the .Rprofile in one repository that we import to our website as a submodule."
  },
  {
    "objectID": "detritus/mod/index.html#how-to-accomplish-this-git-submodules",
    "href": "detritus/mod/index.html#how-to-accomplish-this-git-submodules",
    "title": "Highly modular blogging with Blogdown",
    "section": "How to accomplish this: Git submodules",
    "text": "How to accomplish this: Git submodules\nGit submodules are repositories-within-repositories. Suppose you are working on a project repository (like your website), and there are external tools or resources that you want to import from another project. You have a strong project-based workflow, so you want all of the code that creates your website to be contained within the website directory on your computer. At the same time, the external dependency is clearly its own entity, and there is no reason why its code should be owned by the website repository. Git submodules allow you to clone this dependency repo into your website directory so you can use this code without versioning it redundantly.\n\nSubmodule basics\nIf you have never worked with submodules before, here is how they work in broad strokes. (This is not an exhaustive intro.)\nWhen you add a submodule to a parent repository, the parent repository tracks the presence of the submodule, but it does not track the content. Your website repo tracks the presence of submodules to ensure that your project can be reproduced (read: cloned) with all necessary dependencies in place.2 However, your website repo is ignorant of the actual content of the submodule because the submodule code is versioned by its own separate repo. There is no need to duplicate that effort.\nUpstream changes to the submodule repo can be pulled into your website repo. This is standard workflow for Git. If you want to pin your dependency to a particular commit of the submodule, you can git checkout that commit. If you want your dependency to stay dynamically up to date with the submodule’s remote repo, checkout the desired branch and pull changes as they arise on the upstream remote.\nLocal changes to the submodule content can be pushed to remote. If you have write access to the submodule’s remote repository—either you own the repo, or it’s your fork of some other repo—you can make changes to the submodule contents from within the submodule and push those changes back upstream.3 This is just like a Git workflow where multiple users are pushing to the same remote repository, except instead of multiple users, it’s only you, editing the repo and committing/pushing changes from different endpoints. This allows you to keep the submodule content updated on all of its local and remote copies without duplicating any effort.\n\n\nHow to add your website components as submodules\nIn the spirit of modularity, there is actually nothing Blogdown-specific about including submodules within a project repository. All the same, I will discuss a Blogdown-specific example: the .Rprofile module, which I keep in its own repository here. I discuss how I manage blog posts with submodules later on, because that conversation is a little more involved.\nYou can add a submodule to your (already initialized) website repo with git submodule add [my-url] [my-destination-folder]. You will want to be strategic about where you add the repo, since it will effectively behave like a cloned repository. I often create a /submodules/ folder under my project root and clone submodules to that location.\n# from /path/to/site\nmkdir submodules\ncd submodules\ngit submodule add git@github.com:mikedecr/dots_blogdown.git\nAdding the submodule does not clone its contents. It simply registers the submodule with the repository, creating an entry in the website repo’s .gitmodules file. You have to run a separate command to actually clone the submodule repo’s contents:\ngit submodule update --init --recursive\nThe output will look like you did a git clone. At this point, there should exist a folder called /dots_blogdown/ that contains the repo contents.\nFrom there, your next step depends on how you want to use the contents of the submodule. For this particular example, we want this .Rprofile to live at the top of our website root. This ensures that the file’s code is executed when we open R to manage our website. I achieve this by linking the file to the website root (and, bonus, removing write permissions4).\n# exit /submodules/\ncd ..\n# -s = symlink, -f = force\nln -f ./submodules/dots_blogdown/.Rprofile ./.Rprofile\n# bonus: remove write-permissions (make read-only)\nchmod -w ./.Rprofile\nIt is smart to automate any post-Git processes, such as linking files to other destinations, by putting these commands and other pre-build operations in your website’s /R/build.R file. This ensures that these operations are done each time your website is built, ensuring that your website can be safely reproduced if your submodule content should ever change. With that automation in place, if I ever changed my .Rprofile repo, I never have to worry about manually re-linking my updates to the right destination. The build script does it for me.\n\n\nDeveloping within the submodule repo\nThe above instructions describe how to simply employ submodule files in your website. But suppose you wanted to change the content of the submodule files and push those changes back upstream. What would you do?\nBefore making any changes to the submodule files, make sure the submodule isn’t in detached HEAD state. A detached HEAD state is basically what happens when you have checked out a commit in isolation of the branch on which that commit lives. When you are in detached HEAD state, you are basically looking at a copy of the project, but you cannot alter the project tree itself. Any files you change cannot be committed to a persistent branch. To make permanent changes, you have to checkout the branch that you want to track and commit changes to, which is probably main.\nMake your changes. Even though you are editing a file within a submodule repository, Blogdown doesn’t know or care, so it shouldn’t behave any differently. It will knit/render blog posts and serve your website locally like nothing is wrong. That’s because nothing is wrong.\nCommit changes to submdodule files to the submodule repository. From the command line, this means you probably should cd into the submodule repo before adding any files to the index. If you do Git stuff inside of a GUI, you should be able to make the submodule appear as its own repo that you can do add/commit/push actions to. (I don’t use Rstudio, so unfortunately I don’t know if Rstudio makes this easy.) After committing to your local copy of the submodule repo, you should notice that your parent repository detects an updated commit in the submodule! You should commit that change to the parent repository as well. This simply tells the parent repo that it should consult this new submodule to reproduce the project correctly. This is important because anyone else who clones your website repository (ahem, Netlify!) will need to import the submodule at the correct commit.\nBoth submodule and parent repos can be pushed. If this is your first time pushing any submodule-related commits to Netlify, you will want to read the section about Netlify below.\nAs you get more familiar with Git, you won’t need to follow a checklist. You will simply be familiar enough with how Git works to know exactly what to do!"
  },
  {
    "objectID": "detritus/mod/index.html#blog",
    "href": "detritus/mod/index.html#blog",
    "title": "Highly modular blogging with Blogdown",
    "section": "What to do about your blog?",
    "text": "What to do about your blog?\nShould your blog be one submodule repository, or several? My current setup is to treat every blog post as its own, separate module with its own, separate repository. This keeps each post and all of its dependencies isolated from other posts, which is cleanest for me from a reproducibility and modularity standpoint.\nHowever, you may find many blog post repositories to be overkill, and would instead want a single repository containing all of your blog posts. Would that be fine?\nIn short, the single-blog-module setup may be possible, but it will likely require even more advanced Git magic than just submodules. If you really want to know the nasty technical details, you can read about the problem and one potential solution, with the caveat that I haven’t tested that workflow out. If you trust me that the single-repo workflow is pretty complicated except for people looking to increase their Git dexterity stats, you can skip ahead to read about separate repositories for each post.\n\nOne submodule for all posts: the problem\nTo explain, consider the submodule workflow mentioned earlier. If we wanted to use a “single submodule” approach to blogging, we would\n\nMove our blog posts to another repository and push it to the web.\nAdd this repository as a submodule located in your content/blog or analogous subdirectory.\nThe changes in the content/blog folder are now owned by the submodule repository. The parent repo will no longer see what’s happening in those files—only if you have made new commits.\n\nUnfortunately, this may be a critical problem for your website! This is because many themes ask you to put other important files under your content/blog directory, in addition to the posts. Many popular themes ask for a content/blog/_index.md file to manage the blog’s “listings” page. Many themes also will accept image files in that directory to use for headers and sidebars on the listings page. These files are problems for the single-repo workflow. If we let our blog submodule own the content/blog directory, those files can no longer be tracked by the parent (website) repository. Adding the files to the submodule’s .gitignore does not fix it either. So, what can be done?\n\n\nOne submodule for all posts: there might be a way\nI haven’t tested this, but there might be a way to save the unified-blog-repository workflow: you could make your blog repository a bare repository.\nA bare repository is a repository with no root directory. Now, if you have only used Git on a per-project basis, the idea of a repo with no root directory sounds unthinkable, but it is actually a common way to version your “dotfiles”. Here’s why: your dotfiles usually live at your /home/username/ or ~/ directory. Many folks want to track these files to keep certain preferences synchronized on different machines, but as you can foresee, making a Git repository track your entire ~/ folder would be a horrible and terrifying idea. Instead, people create bare repositories that only track the contents that are explicitly added to the repository, regardless of their relative location to the repo’s .git folder.\nHow might this ameliorate our workflow problem? If we want one submodule repo to track all of the posts in /content/blog, but we don’t want that repo to own the other files in that directory, we might be able to achieve that effect with a bare submodule repo. Such a repo shouldn’t be aware of the other files under content/blog, because the repo doesn’t know that it is the same folder as those files.\nAgain, try it if you want, but you have no assurances from me that it will work.\n\n\nMy choice: every post gets its own repository\nIn lieu of the “advanced solution”, we opt for peak modularity: every blog post gets its own repository.\nThis workflow sounds tedious but is actually easier than you would think, and most of the steps are identical to what I have already covered above. Here’s a quick rundown of what I do:\n\nStart on Github or whichever remote service you prefer. Make a remote-first repository for a new post (give it a meaningful title) and copy its cloning link.\nAdd the new repo as a submodule to a new folder for that post inside of /content/blog. I assume you use a “page bundle” model for organizing your blog code: separate folders for each post that contain respective index.[R]markdown files. It’s advisable to blog with page bundles even if you don’t want to implement hyper-modular blog versioning. Learn more about page bundles from Alison Hill here.\nYour .gitmodules file will automatically update to reflect the new submodule. You will eventually want to commit that change, but it doesn’t have to be now. If necessary, initialize/update the submodule to clone its contents into the new post directory.\nCheckout your desired submodule branch (e.g. main) so you can commit changes to your blog repo.\nEdit your post as you normally would by creating an index.Rmarkdown and typing away. This is where you would use renv to take a snapshot of your R package library in order to reproduce the post. Hugo will trip over the files created by renv, however, so if you want to use it (again, you should), add \"renv\" to the ignoreFiles field in your website’s config.toml (which you only have to do once per site).\nCommit changes to the blog module repository and push to remote.\nYou should notice that your parent repository detects an updated commit in the submodule. Commit that change to the parent repository as well. Pushing this website commit to remote will kick off a new Netlify build if you use continuous integration. Speaking of that…"
  },
  {
    "objectID": "detritus/mod/index.html#netlify-setup",
    "href": "detritus/mod/index.html#netlify-setup",
    "title": "Highly modular blogging with Blogdown",
    "section": "Getting it working with Netlify",
    "text": "Getting it working with Netlify\nOnce you are done getting your site looking the way you want, and all of your files are committed to the parent and child repositories, you can push your website repo to the remote that Netlify is tracking.\nExcept, whoops, your site may fail to build on Netlify. Why? Netlify works by cloning your website repository to their servers and building it with Hugo on their end. This process fails if Netlify can’t successfully reproduce your website repo with all of the submodules declared in your .gitmodules file. Such failure can happen for two benign and fixable reasons: (1) the submodule is a private repository, or (2) the submodule was added using the repo’s ssh URL instead of the https URL.\nIn either case, all you have to do is add ssh-keys to grant Netlify access to these repositories. It sounds complicated and jargony, but Netlify describes the whole process right here.\nOnce Netlify has access to the repositories, it can build its own copy of your website. This is because your parent Git repository spells out all of the instructions for cloning the required submodules at their requested commits."
  },
  {
    "objectID": "detritus/mod/index.html#closing-note",
    "href": "detritus/mod/index.html#closing-note",
    "title": "Highly modular blogging with Blogdown",
    "section": "Closing note",
    "text": "Closing note\nThis post presents an opinionated view of a Blogdown website as a collection of modules and a corresponding workflow for managing them. If you find it helpful, awesome! But as always, you should do what works for you. It happened to be the case that I had a particular set of problems and a desire to strengthen some skills could help me solve them."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "welcome",
    "section": "",
    "text": "I used to be a political scientist. Now I do statistics and software development in high-frequency / low-latency trading."
  },
  {
    "objectID": "blog/usable_repos/index.html",
    "href": "blog/usable_repos/index.html",
    "title": "Replication code should be more usable.",
    "section": "",
    "text": "My mind has been on replication archives lately. I often go digging in other peoples’ projects for data to practice some new statistical skill or another. And I have been digging in my own projects lately to refactor some code for a dormant academic project. In both of these situations I am interfacing with some else’s code (me today ≠ me in the past), and in both situations I am having a bad time.\nThe academic community has been increasingly interested in replication archives since they realized that a lot of public research is, err, systematically untrustworthy. Formal requirements (from journals) and informal pressures (from other researchers) in academia are increasingly requiring authors to prepare these repositories for new projects, which seems good on the whole. But if you ever go digging in these republication archives, you quickly realize that just because authors provide data and code doesn’t mean that the experience is all that helpful. But why? You can see the data, maybe the code runs without errors…What’s the problem?\nMy main argument is that the code was not really designed to be understood or manipulated by other people. If it were, the code would not look the way it does.\nNow, I’m not an academic anymore, so don’t have much stake in this. But I do write quantitative research code in a collaborative environment at work all the time, with the intention that my code will be consumed and repurposed by others. As it turns out, writing code under these conditions changes how you write that code, improves the reliability of your work, and has positive effects on the community in which your code is deliberated and consumed. This blog post will attempt to distill some of the things I have learned into discrete, attainable lessons for writing academic research code.\nBefore I get too concrete though, I make a normative argument for caring about this at all. Changing the way code is written takes effort, and I want to argue why that effort would be justified."
  },
  {
    "objectID": "blog/usable_repos/index.html#getting-oriented",
    "href": "blog/usable_repos/index.html#getting-oriented",
    "title": "Replication code should be more usable.",
    "section": "Getting oriented",
    "text": "Getting oriented\n\nThis isn’t about “good” code vs. “bad” code.\nLike you, I don’t have a defensible theory about what makes code Good for any enduring understanding of Good. “Good” code seems like a contested and highly contextual idea. Code may have “good properties”, but certain features of code design also present important trade-offs, so goodness is understood relative to our goals. So maybe we should start by talking about what we want academic code to do for us.\n\n\nWhat is the point of the code repository for academic projects?\nI will discuss two broad models for code repositories, an archive model and a product model. I am making these up as I go, but you will get the idea.\nThe archive model. This is what most academic repositories are doing right now, I would guess. We call these code repositories “replication archives” because that’s what we think we want them to do: replicate and archive. The model is focused on the “validity” of the code—does it correctly reproduce the results in the writing, tables, and figures (replication) in a robust and enduring way (archiving).\nThese are important goals, but focusing exclusively on them has some predictable side-effects. The repositories end up serving as an audit trail for the paper, so the code is designed around the rhetoric of the paper instead of around the operations that the code performs. We see files named things like 01_setup, 02_clean-data, 03_model, 04_tables-figs, 05_robustness and so on. Even supposing that all of the code runs just fine, all we can really do with the code is rebuild the paper exactly as it is. If you could draw a web of all the interlocking pieces of this project, it would be a long, single strand from start to finish, highly dependent from one script to the next (perhaps intended to be run in the same R/Python session), with no feasible way to swap out any components for other components or modify them in isolation. And crucially, if we wanted to use the data or some of the code from this project for some other purpose, we would have to mangle the project in order to extract the components we wanted.\nThe product model. The product model organizes its code as if to offer it as a product for someone else to use. The code is a key part of the research package, and researchers care that it is a good experience, just as they care about the quality of their writing. An important assertion here is that there is value in the code even if the paper did not exist. If a project proposes a method to measure or estimate a key variable, those methods are valuable outside of the context of the written paper, so part of the product of the project is to make these tools available and usable for other researchers to benefit from. Projects that propose a new model make this model available for others to use with a practical interface. Indeed, there may be a notion of interface as distinct from infrastructure; the code that drives the project at a high level is uniform and understandable, and the ugly code that does heavy lifting might be encapsulated away from the interface. And better yet, the important infrastructure pieces are modularly designed: they may combine at the locus of the project, but the components can exist without reference to one another and could be exchanged for other components that perform similar functions.\n\n\nIncreasing the amount of shared / re-used code would be good for research credibility and knowledge accumulation\nPosting code online is not the same as making it usable. I suspect many social scientists put code online without really wanting others to dig into it. But there are real services that academics could provide for each other more regularly if they were more generous with the way their code is designed. Political scientists in particular use a lot of the same data sources and do a lot of similar transformations on it, but why all the wasted effort? I quite admire Steven V. Miller’s peacesciencer package for R, a set of tools whose rationale seems to be that there’s no sense in having everybody duplicate each other’s work to shape the data into common formats for analysis in political science. I gotta say, I agree.\nBut it doesn’t stop at broad-based datasets or data-shaping tools. Think about every paper you have ever read that proposes a new way to measure something. Did those authors provide a code module to do the appropriate calculations on new data of appropriate shape? Did those authors provide an interface for validating these measures against alternatives? I suspect that in most cases the answer is no. The authors may put their code online, but it isn’t online so that you can use it. Not really.\nI don’t want to point fingers, so I will use some examples from my own academic work to show that I, too, wasn’t thinking enough about this. In my dissertation, I built a group-level measurement model to estimate latent political ideology in partisan groups at subnational units of aggregation, and I wrote a series of Stan files to iterate on a few approaches to it. Other projects I have seen before and after I finished my thesis have built similar models. Did I package my models into a tool that they could use? No, I did not. I also implemented a reweighting method for aggregated survey data that I first saw in a paper from nearly a decade ago. This is probably a pretty broadly applicable correction for similar data, but did I provide a little module for others to apply the same calculations on data that wasn’t exactly my own? Nah. I designed a Bayesian implementation of the Acharya, Blackwell, and Sen formulation of sequential-g estimator that addresses some of the things that Bayesians would care about in situations like that, and I would even say I was pretty proud of it. But I wasn’t proud enough to share a generic version that others could use and adapt for their purposes.\nYou get the idea.\nIt just makes me worry that when we advertise our work as tools that others could use, we do not really mean it. I worry that phrases like “We propose a method…”, or “we provide an approach…” are only things we were trained to say to make it sound like we are contributing tools for the community. But we are not doing the work that would make those tools available for others. The code that goes into the paper repository is for ourselves, because the goal is getting our paper over the finish line. The code is just another part of the game.\nThere recently was a post on the Gelman blog that stuck out to me about cumulative science and re-using each other’s work. Here is an excerpt that gives you the idea:\n\nHow could a famous study sit there for 20 years with nobody trying to replicate it? […] Pamela said this doesn’t happen as often in biology. Why? Because in biology, when one research team publishes something useful, then other labs want to use it too. Important work in biology gets replicated all the time—not because people want to prove it’s right, not because people want to shoot it down, not as part of a “replication study,” but just because they want to use the method. So if there’s something that everybody’s talking about, and it doesn’t replicate, word will get out.\nThe way she put it is that biology is a cumulative science.\n\nThinking about how this applies to our code, it is clear that there is more than a vague moral value to posting usable code for others. There is scientific value. And it is interesting to me that after all the commotion about replication and running code without errors, there is comparatively little discussion about the scientific value of the code outside of the narrow, narrow context of the paper it is written for."
  },
  {
    "objectID": "blog/usable_repos/index.html#what-can-be-done",
    "href": "blog/usable_repos/index.html#what-can-be-done",
    "title": "Replication code should be more usable.",
    "section": "What can be done?",
    "text": "What can be done?\nNow that I am done complaining, we can talk about recommendations. I will focus mainly on two concepts, interface and modularity, from a couple different angles. Interface refers to the way other people use your tools. Modularity describes how those tools are combined for easier development (for your own benefit) and exporting (for others’ benefit).\n\nInterfaces for living projects, not memorials to dead projects\nWhenever I want to download a project’s data and code, I only want to get it from a version control platform like Github. I want to fork the project, see the history, get the intended file structure, and maybe even contribute to the project by opening issues or pull requests. I never want to go to Harvard Dataverse. I’m sure Dataverse was a great idea when it first hit the scene, but by today’s standards, it feels like it is chasing yesterday’s problems, like the problem of “pure replication”. But I think the credibility problems in social science warrant more than old-world-style replication. We should be looking for platforms that accommodate and encourage sharing, re-use, mutual contribution, and stress-testing by others.\n\n\nInterface vs. Infrastructure\nThis is a pretty common distinction you hear about in software communities. It isn’t much discussed in academic research circles.\nI will explain by way of example: Think about the tidyverse packages in R, or just the dplyr and tidyr packages. These packages provide an interface to useful data manipulations on data frames. These are operations on abstractions of your data—an abstraction in the sense that the functions do not have to know or care what is in those data frames in order to operate on them. The packages provide a uniform interface; they take a data frame as an input and return a data frame as an output, and the semantics are similar across functions. This makes the operations composable, which is jargon for “the operations can be reordered and combined to achieve powerful functionality”. The same basic principles are true for other tools in the tidyverse like stringr, forcats, purrr, and so on. They employ different abstractions for data organized at different levels (strings, factors, and lists respectively), but the emphasis on uniformity and composability is always there.\nSo that’s the “interface” layer. Now, what about the “infrastructure” or the implementation of these functions? Do you know anything about how these functions are actually written? And would it really matter if you did? The infrastructure isn’t what you, the user, care about. What matters is that the tools provide a simple way to perform key tasks with your code without bogging you down in the implementation details.\nCompare this to the way we write research code. There is usually no distinction between interface and infrastructure whatsoever. A lot of the time, we keep all of our nasty and idiosyncratic data-cleaning code right next to our analysis and visualization. On a good day, we may smuggle the data-cleaning code into a different file, but that doesn’t make a huge difference because the flow of the project is still mostly linear from raw data to analysis. The user cannot really avoid spending time in the darkest corners of the code.\nTo be fair, it isn’t necessary that an academic project’s code base should produce an end result as concptually gorgeous as tidyverse tools are. But there are probably some intertwined components in the research project that could be separated into interface and infrastructure layers somehow, and readers who really want to investigate the infrastructure are free to do so.1\nThinking again about a paper that proposes a new way to measure a key variable, or a new method to analyze some data: could those methods not be divided into an interface to access the method and an infrastructure that does the heavy lifting somewhere else? Wouldn’t you be more likely to use and re-use a tool like that? Wouldn’t it be more likely that if the method has a problem, we would have an easier time discovering and fixing it? Wouldn’t that look more like the iterative, communal scientific process that we wish we had?\n\n\nLittle functions, good and bad\nShould you make an interface by writing more functions? Annoyingly, it depends. One way to make an interface more legible to users is package annoying routines into functions that describe what you’re accomplishing. This is nominally easy to implement, but it isn’t always easy to design well. And the design considerations present plenty of trade-offs with no obvious guiding theory.\nTake a halfway concrete example. You have a dataset of administrative data on many individuals, and the dataset has a field for individuals’ names. Your task is to clean these names in some way.\nYou choose to use some combination of regular expressions and stringr to do this. But how do you implement this operation in the code? One way is to create a new variable like…\n\nnew_df <- mutate(\n    df, \n    name = \n        str_replace(...[manipulation 1 here]...) |>\n        str_replace(...[manipulation 2 here]...) |>\n        [...]\n        str_replace(...[final manipulation here]...)\n)\n\n…and this works fine. But if you wanted to change the way this name-cleaning is done, not only do you have to do surgery directly on a data pipeline, but you can’t test your new implementation without re-running this mutate step over and over (which may be only one step of an expensive, multi-function pipe chain, but that is a separate, solvable issue).\nConsider instead the possibility of writing a function called format_names that implements your routine. Now your data pipeline looks like this…\n\nnew_df <- mutate(df, name = format_names(name))\n\nWell, should you do that? The routine is now encapsulated, so if you need to do it more places, you can call the function without rewriting the steps.2 This makes your routine unit-testable: does it clean the patterns you want it to clean (and you are, of course, limited by the patterns you can anticipate). It also makes it a little easier to change your implementation in one location and achieve effects in many places without doing surgery to your data pipeline. Maybe it also makes it easy to move this step in your data pipeline around, which is good.\nAnd what are the drawbacks? Well, you no longer know what the function is doing without hunting down the implementation, and it’s possible that the implementation is idiosyncratic to your project instead of being broadly rules-based. In general, encapsulating code into a function makes it easier to “drive” code, but it doesn’t inherently have any effect on whether your code is operating at a useful level of abstraction. Nor does it have any obvious effect on whether the interface you design for one function is at all related to the interface you create for other functions—uniformity makes your functions easier to use and combine for powerful results. If you aren’t careful, you might write ten different functions that don’t share a uniform abstraction or interface, so now it takes more effort for you to remember how your functions work than it does to write it out using stringr. After all, stringr is already built on familiar abstractions: stringr don’t know what your string is, and it does not care. All it knows is that it has functions with similar semantics for doing operations on strings.\nSo, you have to think about what you want to accomplish if you want to have an effective design.\n\n\nMore modules, fewer pipelines\nSo far I have been a little incredulous toward the idea that your code should be a “pipeline” for your data. This is because pipelines are often in conflict with modularity: the principle of keeping things separate, independent, and interchangeable. A lot of academic projects are lacking in it.\nIt is difficult at first to realize the drawbacks of non-modular pipeline organization because, especially with tidyverse tools, chaining many things together to do big things is the primary benefit. When dplyr and tidyr first start clicking for you, you immediately begin chaining tons of operations together to go from A to B without a ton of code. But as you iterate on the analysis, which requires breaking and changing things in your code, it can suddenly be very cumbersome to find and fix broken parts. This is because you have twisted all of the important steps together! You wrote the code in such a way that steps that do not depend on one another in principal now have to be organized and executed in a strict order. And now the fix for one breakage leads you to break something else because the steps in your pipeline are not abstracted. It is just nice to avoid problems like this.\nTo be clear, this is not the tidyverse’s fault. The tidyverse is an exemplar of modular design. The problem is that you tried to string too much together without thinking about how to keep things smartly separated. We should be asking ourselves questions like,\n\nWhat operation (read: function) do I need to do here, regardless of what my particular data look like? That is, think more about functionality and less about state.\nWhat can be separated? For instance, do I need the analysis of these 3 different data sources to happen in the same file? Or are they unrelated to the point where the analyses don’t have to be aware of one another in any way? Again, functionality without state.\nWhat can be encapsulated? Suppose I convince myself that the analyses of my 3 different data sources don’t have to be aware of one another, but maybe there are some tools I can define that could be useful in each analysis. Perhaps I should encapsulate those tools under a separate file (read: a module!) and then import that module whenever I need it. This lets me both keep things separate without repeating the same steps in many places. We should be skeptical of encapsulation for its own sake—it isn’t always necessarily helpful—but in this case it helps us separate functionality from state.\nCorrespondingly, what can be exported to other files in this project, or to other users who might want to use only that part? This is probably helpful to remember for cleaned data, repeated data manipulation tools, a new model that you build, and more.\n\nIf more pieces of your project are separable, interchangeable, and exportable, it becomes much easier to share little pieces of your project with other researchers.\n\n\nThere’s organization, and then there’s organization\nWe know that our code should be organized, but it is easy to organize code ineffectively. Something we see a lot in research code is an organizational structure that is perfectly discernible but not exactly useful. I have created plenty of impeccably “organized” but unhelpful repositories.\nOne discernible but not-very-useful organizational pattern is to name your files as steps along a data analysis pipeline: 01_setup, 02_clean-data, 03_model, 04_tables-figs, 05_robustness, and so on. Again, guilty. This setup doesn’t help the researcher keep themselves “organized” if every problem they have to solve is tangled together in one bucket ambiguously labeled “data cleaning”. And it doesn’t help anyone consuming the project understand or modify which pieces of the code are responsible for which functionality.\nBut if you are being a good citizen—designing modularity into your code, creating a useful interface, dividing functionality across files, separating analyses that don’t depend on one another, and so on—the pipeline organization will not be the dominant feature. It will naturally be replaced by a layout that reflects the concepts in the code, not the rhetorical contours of a research paper. The code is not a paper, and a paper is not its code.\nTo be sure, you cannot kill every little pipeline; data always need to be read, shaped, and analyzed. But building your code on effective abstractions lets you write smaller pipelines that are quicker from start to finish, conceptually simpler, and untangled from other independent ingredients in your project. The ultimate goal is usable code, not killing pipelines. It just happens that small, minimal pipelines are more usable than gigantic, all-encompassing pipelines.\n\n\nJudicious usage of “literate programming”\nMany researchers agree that statistical output should be algorithmically injected into research writeups to ensure the accuracy of the reporting. This is the real reason to use document prep systems like Rmarkdown or (increasingly) Quarto: not for the math typesetting.\nBut it is also a problem if the statistical work is so intertwined with the manuscript that they cannot be severed in the code. I myself have written plenty of “technically impressive” Rmarkdown documents that are actually are fragile Jenga towers of tangled functionality.\nThis is a pretty easy lesson though. The .Rmd file shouldn’t be the place where important, novel analysis happens. That should happen elsewhere, and your paper should be epiphenomenal to it.\n\n\nWho’s looking at your code?\nYou learn a lot by having other people make suggestions to you about the way your code is structured. I suspect most academic projects, even those with multiple co-authors, don’t feature much code criticism or review of any kind. Outsourcing code tasks to graduate students is great for developing their skills, but if you (the person reading this) are the veteran researcher, they would also benefit from your advice. The student and your project will be better for it.\n\n\nDon’t propose methods. Provide them.\nI have said this already, but I wanted to sloganize it."
  },
  {
    "objectID": "blog/usable_repos/index.html#better-code-vs.-the-academy",
    "href": "blog/usable_repos/index.html#better-code-vs.-the-academy",
    "title": "Replication code should be more usable.",
    "section": "Better code vs. the Academy",
    "text": "Better code vs. the Academy\nThis section could be its own blog post, but I am not an academic anymore and do not want to spend too much energy arguing about stuff that I intentionally left behind. But in case you are thinking about the following things, I just want you to know what I see you, I hear you, but I think these are the questions you have to answer for yourselves.\n\nWriting better code will not help my paper get published. Yeah, that sucks, right? Why is that? If we can agree that more usable code will make it easier to share tools among researchers, test-drive the contributions of other researchers, and bolster the credibility of your field overall, why does your publication model think that it’s a waste of your time as a researcher?\nThis all sounds very hard, and I am not a software engineer. You shouldn’t have to be a software engineer if you do not want to be. What I do wish is for your field to sustain broader collaboration where you have a team of people who are good at different things so your field can put out more reliable products. But many fields care about superstar researchers. I do not care about superstar researchers."
  },
  {
    "objectID": "blog/usable_repos/index.html#what-we-havent-said",
    "href": "blog/usable_repos/index.html#what-we-havent-said",
    "title": "Replication code should be more usable.",
    "section": "What we haven’t said",
    "text": "What we haven’t said\nHere’s a recap of all the things we did not mention about replication / archiving discussions:\n\nPackaging environments and dependencies\nRelatedly, containerization\nSolutions for long-term archiving\nCommenting code / other documentation\n\nI think these problems are important to varying degrees. I mean, I am not as concerned about containerization, but you are free to be. But these issues are more commonly discussed than the design, usability, and shareability of code. So I didn’t talk about them."
  },
  {
    "objectID": "blog/modular_blog/index.html",
    "href": "blog/modular_blog/index.html",
    "title": "Highly modular blogging with Blogdown",
    "section": "",
    "text": "When I finished graduate school, I tore down my website.\nFor a handful of reasons. I no longer needed a website that cried out, “Help me, I’m finishing my PhD and I need to escape.” I didn’t need to showcase unpublished papers, teaching resources, or old blog posts that I had grown detached from. It was time for a clean reset.\nBut if you work with Blogdown, you know that starting over is laborious. Not that Blogdown isn’t great, because it is. It’s that, when you’re a finicky person like me, setting up a website with the right balance of capable features, pleasant aesthetics, and a principled codebase is legitimately challenging. I was encountering the same familiar challenges over and over.\nFor example, the site’s Hugo theme. I would take a lot of Hugo themes for test-drives. Hugo advertizes themes as if they were completely modular with respect to your /content/ folder. For most themes, this is a lie. Themes usually want too many bespoke variables or file structures in your website content. Some amount of this is okay, but it comes at a cost. If you really want to take a theme for a spin, I would find it easier to create an entirely new blogdown::new_site() than to change my theme in an existing site.\nExcept now you’re dragging the same files around your computer all over again. Ugh, this new website directory needs your blog source files, your website-level .Rprofile that controls Blogdown’s build behaviors, the jpg/png image files that you use to brand yourself online, etc… And maybe these files need to go in different folders or be given different file names from the previous theme. After a while, these files no longer have a single authoritative “home” on your computer, and you may have multiple conflicting(!) versions of these files across your various experimental website folders.\nAnd then there’s reproducibility. Even after lugging around all the same files to the new site, good luck getting your blog posts to render if your package library has changed since they were written, which it probably has. Danielle Navarro wrote about reproducibility in Rmarkdown blogging, and argues convincingly that the best way to protect your .Rmarkdown posts from this rebuilding risk is to create a dedicated package library for each separate blog post using renv. This sounds intense at first, but the underlying principle is simple, which makes it a good solution to a difficult problem.\nThis post will continue that pattern: intense at first, but well-founded, solutions for difficult problems."
  },
  {
    "objectID": "blog/modular_blog/index.html#what-this-post-is-about-modularity",
    "href": "blog/modular_blog/index.html#what-this-post-is-about-modularity",
    "title": "Highly modular blogging with Blogdown",
    "section": "What this post is about: modularity",
    "text": "What this post is about: modularity\nWhat we want is a principled and robust approach for managing the many interlocking components of your website. Specifically, we explore the modularity of the elements in your website. I take the view that your website is a collection of modular components that are better managed independently, with different Git repositories for the different site components. Yes, managing your website with multiple repositories. Stay with me.\nThe modules that compose your website include your theme, your blog posts, your Blogdown build preferences (implemented in your .Rprofile), and maybe more. These modular components come together at the nexus of the website, but as I will argue, these components should not belong to the website. Why not? Because these components can be re-used across different websites or substituted with other similar components. Hugo already flirts with this idea in its core design by separating the /theme/ directory from the /content/ directory, as if to say, “these components can be combined but do not depend on one another.” This post takes an opinionated stance that such modularity is a good idea and should be assertively extended to other components of your Blogdown site. That said, I make no assertion that this stance is objectively correct—only that it has been useful enough for me that I wanted to share some thoughts about the principles and processes at work. (You should do what works for you!)\nModularity as a software philosophy is one thing, but implementing it in code requires technical solutions. This post will discuss how to achieve this using Git submodules, an intermediate-level Git construct that, if you’re like me, is somewhat familiar but somewhat intimidating. In short, a Git submodule is a repository-within-a-repository. It has its own version history that is distinct from the “parent” repository. In this post, I provide a simple tour of submodules and how they can be used to structure your website workflow. We will recast our website as the “primary” repository, and we import other modular site components (like our blog posts) as Git submodules. In case you host your blog on Netlify, I will also discuss how to ensure that Netlify can build your site successfully.\n\nAside: some terminology\nThis discussion will involve plenty of concepts that sound similar to one another but should be understood as distinct things. I want to flag these concepts so that we understand each other better.\nDirectory vs. repository. A directory is a folder on your computer that holds files. A (Git) repository tracks changes to files. For many projects, the project’s root directory is entirely managed by one repository, so the distinction between the two may be blurred. When Git submodules are involved, this is no longer true. Your website directory will be managed by one repository, and sub-directories below your website will be managed by other repositories.\nWebsite vs. module. The website is the entire project that puts your website online. Your website will contain various modules that combine to build the entire project. Your blog posts will be considered a module (or several modules, depending on your implementation). Your theme is another module. Think of modules as building blocks for your website that can be stacked, swapped out, and so on.\nParent repository (for the website) vs. child repository (for the module), a.k.a. “submodule”. The website and the module will be versioned by separate repositories. We can refer to the over-arching project repo (the website) as the “parent” repo and the module repo as the “child” repo. A “Git submodule” is a Git construct that is overlaid onto this relationship between repositories. A repository, in isolation, is simply a repository. But if you import a repository into another project as a dependency, Git designates the dependency as a “submodule” to the parent repository, and this affects our Git workflow as a result. I explain all of that below."
  },
  {
    "objectID": "blog/modular_blog/index.html#websites-are-a-collection-of-modules",
    "href": "blog/modular_blog/index.html#websites-are-a-collection-of-modules",
    "title": "Highly modular blogging with Blogdown",
    "section": "Websites are a collection of modules",
    "text": "Websites are a collection of modules\nModules are like little building blocks, and your website has plenty of them. Setting aside any formal definition of what would mathematically be considered a “module”, let’s crudely define them as structures in your website that are agnostic to the content of other structures. We may be able to replace modules with other modules, or remove modules entirely, without affecting the core function of other modules.\nHere are some examples from my own workflow. I consider my blog posts, Hugo theme, and blogdown build settings (in my site-level .Rprofile) as modular components within the website as a whole, and I version each component with its own separate repository. Here is how I justify this view for each component:\n\nBlog posts: The content of a blog post is completely separable from the website repo. We can take a blog post and locate it in a different website, and the blog post should still be meaningful (and reproducible) unto itself. Many blogdown users remake their websites and carry their old blog posts to the new sites, which shows that the blog content doesn’t functionally depend on the website.\nIt turns out that, for blog posts, modularity and reproducibility are pretty closely related. In her discussion of blog reproducibility, Danielle Navarro touched on the principle that a blog should be “encapsulated” or “isolated” away from the broader website to robustify the blog against other dependencies. By insisting that blog posts also be modular, not only is the blog protected from the website’s computational environment, we can control each post independently of one another, move posts around across contexts, and remove posts entirely without side-effects.\nThis also affects how we treat the blog post’s dependencies. Suppose that your post includes an analysis on a data file that you read from disk. This file should belong to your blog post—and be versioned by that blog post’s Git repository—not your website. This means you should keep all of these files in the blog post directory, and forget about the website’s /static/ folder except for files that rightfully belong to the website.\nHugo theme: Hugo is designed such that the /content/ of a website (specified in markdown files) is more-or-less independent of its /theme/. The same theme can be used for multiple websites, and a single website can (in theory1) swap out one theme for another. Because themes are managed with Git repositories already, you can pull theme updates from their remote repositories without overwriting any bespoke theme customizations specified in your /layouts/ folder.\nBlogdown complicates this somewhat. When you install a theme with blogdown::install_theme(), Blogdown actually deletes the theme’s .git directory. (At least, this was my experience.) This is probably for ease-of-use among users who would not appreciate having to manage the theme as a submodule. But we are enthusiastic seekers of modularity, so we want to keep that upstream remote connection alive. As such, I installed my site’s Hugo theme using Git submodule operations instead of installing it with blogdown::install_theme().\n\n\nThe website .Rprofile file: You may have a global .Rprofile file, but it is an increasingly common Blogdown workflow staple to set up a website-specific .Rprofile to control Blogdown’s build behavior. How is this a module? Your blogdown build preferences are probably not specific to this website repository. Instead, it is likely that your preferences reflect your workflow for blogging in general and could be equally applicable to any other website repo you create or manage. If you change your blogdown workflow in a way that bears on this .Rprofile file, that change may affect all of your blogdown websites equally! Managing these .Rprofiles separately for each website would be inefficient and error-prone, so instead we manage the .Rprofile in one repository that we import to our website as a submodule."
  },
  {
    "objectID": "blog/modular_blog/index.html#how-to-accomplish-this-git-submodules",
    "href": "blog/modular_blog/index.html#how-to-accomplish-this-git-submodules",
    "title": "Highly modular blogging with Blogdown",
    "section": "How to accomplish this: Git submodules",
    "text": "How to accomplish this: Git submodules\nGit submodules are repositories-within-repositories. Suppose you are working on a project repository (like your website), and there are external tools or resources that you want to import from another project. You have a strong project-based workflow, so you want all of the code that creates your website to be contained within the website directory on your computer. At the same time, the external dependency is clearly its own entity, and there is no reason why its code should be owned by the website repository. Git submodules allow you to clone this dependency repo into your website directory so you can use this code without versioning it redundantly.\n\nSubmodule basics\nIf you have never worked with submodules before, here is how they work in broad strokes. (This is not an exhaustive intro.)\nWhen you add a submodule to a parent repository, the parent repository tracks the presence of the submodule, but it does not track the content. Your website repo tracks the presence of submodules to ensure that your project can be reproduced (read: cloned) with all necessary dependencies in place.2 However, your website repo is ignorant of the actual content of the submodule because the submodule code is versioned by its own separate repo. There is no need to duplicate that effort.\nUpstream changes to the submodule repo can be pulled into your website repo. This is standard workflow for Git. If you want to pin your dependency to a particular commit of the submodule, you can git checkout that commit. If you want your dependency to stay dynamically up to date with the submodule’s remote repo, checkout the desired branch and pull changes as they arise on the upstream remote.\nLocal changes to the submodule content can be pushed to remote. If you have write access to the submodule’s remote repository—either you own the repo, or it’s your fork of some other repo—you can make changes to the submodule contents from within the submodule and push those changes back upstream.3 This is just like a Git workflow where multiple users are pushing to the same remote repository, except instead of multiple users, it’s only you, editing the repo and committing/pushing changes from different endpoints. This allows you to keep the submodule content updated on all of its local and remote copies without duplicating any effort.\n\n\nHow to add your website components as submodules\nIn the spirit of modularity, there is actually nothing Blogdown-specific about including submodules within a project repository. All the same, I will discuss a Blogdown-specific example: the .Rprofile module, which I keep in its own repository here. I discuss how I manage blog posts with submodules later on, because that conversation is a little more involved.\nYou can add a submodule to your (already initialized) website repo with git submodule add [my-url] [my-destination-folder]. You will want to be strategic about where you add the repo, since it will effectively behave like a cloned repository. I often create a /submodules/ folder under my project root and clone submodules to that location.\n# from /path/to/site\nmkdir submodules\ncd submodules\ngit submodule add git@github.com:mikedecr/dots_blogdown.git\nAdding the submodule does not clone its contents. It simply registers the submodule with the repository, creating an entry in the website repo’s .gitmodules file. You have to run a separate command to actually clone the submodule repo’s contents:\ngit submodule update --init --recursive\nThe output will look like you did a git clone. At this point, there should exist a folder called /dots_blogdown/ that contains the repo contents.\nFrom there, your next step depends on how you want to use the contents of the submodule. For this particular example, we want this .Rprofile to live at the top of our website root. This ensures that the file’s code is executed when we open R to manage our website. I achieve this by linking the file to the website root (and, bonus, removing write permissions4).\n# exit /submodules/\ncd ..\n# -s = symlink, -f = force\nln -f ./submodules/dots_blogdown/.Rprofile ./.Rprofile\n# bonus: remove write-permissions (make read-only)\nchmod -w ./.Rprofile\nIt is smart to automate any post-Git processes, such as linking files to other destinations, by putting these commands and other pre-build operations in your website’s /R/build.R file. This ensures that these operations are done each time your website is built, ensuring that your website can be safely reproduced if your submodule content should ever change. With that automation in place, if I ever changed my .Rprofile repo, I never have to worry about manually re-linking my updates to the right destination. The build script does it for me.\n\n\nDeveloping within the submodule repo\nThe above instructions describe how to simply employ submodule files in your website. But suppose you wanted to change the content of the submodule files and push those changes back upstream. What would you do?\nBefore making any changes to the submodule files, make sure the submodule isn’t in detached HEAD state. A detached HEAD state is basically what happens when you have checked out a commit in isolation of the branch on which that commit lives. When you are in detached HEAD state, you are basically looking at a copy of the project, but you cannot alter the project tree itself. Any files you change cannot be committed to a persistent branch. To make permanent changes, you have to checkout the branch that you want to track and commit changes to, which is probably main.\nMake your changes. Even though you are editing a file within a submodule repository, Blogdown doesn’t know or care, so it shouldn’t behave any differently. It will knit/render blog posts and serve your website locally like nothing is wrong. That’s because nothing is wrong.\nCommit changes to submdodule files to the submodule repository. From the command line, this means you probably should cd into the submodule repo before adding any files to the index. If you do Git stuff inside of a GUI, you should be able to make the submodule appear as its own repo that you can do add/commit/push actions to. (I don’t use Rstudio, so unfortunately I don’t know if Rstudio makes this easy.) After committing to your local copy of the submodule repo, you should notice that your parent repository detects an updated commit in the submodule! You should commit that change to the parent repository as well. This simply tells the parent repo that it should consult this new submodule to reproduce the project correctly. This is important because anyone else who clones your website repository (ahem, Netlify!) will need to import the submodule at the correct commit.\nBoth submodule and parent repos can be pushed. If this is your first time pushing any submodule-related commits to Netlify, you will want to read the section about Netlify below.\nAs you get more familiar with Git, you won’t need to follow a checklist. You will simply be familiar enough with how Git works to know exactly what to do!"
  },
  {
    "objectID": "blog/modular_blog/index.html#blog",
    "href": "blog/modular_blog/index.html#blog",
    "title": "Highly modular blogging with Blogdown",
    "section": "What to do about your blog?",
    "text": "What to do about your blog?\nShould your blog be one submodule repository, or several? My current setup is to treat every blog post as its own, separate module with its own, separate repository. This keeps each post and all of its dependencies isolated from other posts, which is cleanest for me from a reproducibility and modularity standpoint.\nHowever, you may find many blog post repositories to be overkill, and would instead want a single repository containing all of your blog posts. Would that be fine?\nIn short, the single-blog-module setup may be possible, but it will likely require even more advanced Git magic than just submodules. If you really want to know the nasty technical details, you can read about the problem and one potential solution, with the caveat that I haven’t tested that workflow out. If you trust me that the single-repo workflow is pretty complicated except for people looking to increase their Git dexterity stats, you can skip ahead to read about separate repositories for each post.\n\nOne submodule for all posts: the problem\nTo explain, consider the submodule workflow mentioned earlier. If we wanted to use a “single submodule” approach to blogging, we would\n\nMove our blog posts to another repository and push it to the web.\nAdd this repository as a submodule located in your content/blog or analogous subdirectory.\nThe changes in the content/blog folder are now owned by the submodule repository. The parent repo will no longer see what’s happening in those files—only if you have made new commits.\n\nUnfortunately, this may be a critical problem for your website! This is because many themes ask you to put other important files under your content/blog directory, in addition to the posts. Many popular themes ask for a content/blog/_index.md file to manage the blog’s “listings” page. Many themes also will accept image files in that directory to use for headers and sidebars on the listings page. These files are problems for the single-repo workflow. If we let our blog submodule own the content/blog directory, those files can no longer be tracked by the parent (website) repository. Adding the files to the submodule’s .gitignore does not fix it either. So, what can be done?\n\n\nOne submodule for all posts: there might be a way\nI haven’t tested this, but there might be a way to save the unified-blog-repository workflow: you could make your blog repository a bare repository.\nA bare repository is a repository with no root directory. Now, if you have only used Git on a per-project basis, the idea of a repo with no root directory sounds unthinkable, but it is actually a common way to version your “dotfiles”. Here’s why: your dotfiles usually live at your /home/username/ or ~/ directory. Many folks want to track these files to keep certain preferences synchronized on different machines, but as you can foresee, making a Git repository track your entire ~/ folder would be a horrible and terrifying idea. Instead, people create bare repositories that only track the contents that are explicitly added to the repository, regardless of their relative location to the repo’s .git folder.\nHow might this ameliorate our workflow problem? If we want one submodule repo to track all of the posts in /content/blog, but we don’t want that repo to own the other files in that directory, we might be able to achieve that effect with a bare submodule repo. Such a repo shouldn’t be aware of the other files under content/blog, because the repo doesn’t know that it is the same folder as those files.\nAgain, try it if you want, but you have no assurances from me that it will work.\n\n\nMy choice: every post gets its own repository\nIn lieu of the “advanced solution”, we opt for peak modularity: every blog post gets its own repository.\nThis workflow sounds tedious but is actually easier than you would think, and most of the steps are identical to what I have already covered above. Here’s a quick rundown of what I do:\n\nStart on Github or whichever remote service you prefer. Make a remote-first repository for a new post (give it a meaningful title) and copy its cloning link.\nAdd the new repo as a submodule to a new folder for that post inside of /content/blog. I assume you use a “page bundle” model for organizing your blog code: separate folders for each post that contain respective index.[R]markdown files. It’s advisable to blog with page bundles even if you don’t want to implement hyper-modular blog versioning. Learn more about page bundles from Alison Hill here.\nYour .gitmodules file will automatically update to reflect the new submodule. You will eventually want to commit that change, but it doesn’t have to be now. If necessary, initialize/update the submodule to clone its contents into the new post directory.\nCheckout your desired submodule branch (e.g. main) so you can commit changes to your blog repo.\nEdit your post as you normally would by creating an index.Rmarkdown and typing away. This is where you would use renv to take a snapshot of your R package library in order to reproduce the post. Hugo will trip over the files created by renv, however, so if you want to use it (again, you should), add \"renv\" to the ignoreFiles field in your website’s config.toml (which you only have to do once per site).\nCommit changes to the blog module repository and push to remote.\nYou should notice that your parent repository detects an updated commit in the submodule. Commit that change to the parent repository as well. Pushing this website commit to remote will kick off a new Netlify build if you use continuous integration. Speaking of that…"
  },
  {
    "objectID": "blog/modular_blog/index.html#netlify-setup",
    "href": "blog/modular_blog/index.html#netlify-setup",
    "title": "Highly modular blogging with Blogdown",
    "section": "Getting it working with Netlify",
    "text": "Getting it working with Netlify\nOnce you are done getting your site looking the way you want, and all of your files are committed to the parent and child repositories, you can push your website repo to the remote that Netlify is tracking.\nExcept, whoops, your site may fail to build on Netlify. Why? Netlify works by cloning your website repository to their servers and building it with Hugo on their end. This process fails if Netlify can’t successfully reproduce your website repo with all of the submodules declared in your .gitmodules file. Such failure can happen for two benign and fixable reasons: (1) the submodule is a private repository, or (2) the submodule was added using the repo’s ssh URL instead of the https URL.\nIn either case, all you have to do is add ssh-keys to grant Netlify access to these repositories. It sounds complicated and jargony, but Netlify describes the whole process right here.\nOnce Netlify has access to the repositories, it can build its own copy of your website. This is because your parent Git repository spells out all of the instructions for cloning the required submodules at their requested commits."
  },
  {
    "objectID": "blog/modular_blog/index.html#closing-note",
    "href": "blog/modular_blog/index.html#closing-note",
    "title": "Highly modular blogging with Blogdown",
    "section": "Closing note",
    "text": "Closing note\nThis post presents an opinionated view of a Blogdown website as a collection of modules and a corresponding workflow for managing them. If you find it helpful, awesome! But as always, you should do what works for you. It happened to be the case that I had a particular set of problems and a desire to strengthen some skills could help me solve them."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "more about me",
    "section": "",
    "text": "—background—\nI have a Ph.D. in political science from the University of Wisconsin–Madison, where I was a graduate affiliate of the Elections Research Center. My graduate work developed Bayesian + causal models to study U.S. elections and public opinion.\n\n\n—lately—\nSome things I have been interested in lately:\n\nSoftware design, functional programming especially\nLua-flavored Neovim\nFrequentist statistics"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog Posts",
    "section": "",
    "text": "code\n\n\n\nMy mind has been on replication archives lately. I often go digging in other peoples’ projects for data to practice some new statistical skill or another. And I have…\n\n\n\nMichael DeCrescenzo\n\n\nMay 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\n\n\ngit\n\n\n\nWhen I finished graduate school, I tore down my website.\n\n\n\nMichael DeCrescenzo\n\n\nNov 6, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  }
]