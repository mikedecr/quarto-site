[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "welcome",
    "section": "",
    "text": "I used to be a political scientist. Now I do statistics and software development in high-frequency / low-latency trading."
  },
  {
    "objectID": "blog/latex_colors/index.html",
    "href": "blog/latex_colors/index.html",
    "title": "Treating \\(\\mathrm{\\LaTeX}\\) like a programming language: the case of color-coded equations",
    "section": "",
    "text": "\\(\\newcommand{latex}{\\mathrm{\\LaTeX}}\\)\n\\(\\latex\\) can be a headache. The syntax is clunky. Normal usage leads us to engage with a lot of frustrating patterns. I see academics and other researchers complain about it, and I usually agree with where these complaints are coming from.\nBut I also feel like many researchers don’t get the most out of \\(\\latex\\). When I used to write more \\(\\latex\\) in graduate school, I knew I didn’t. I mean, I was good at algorithmically injecting my statistical results into documents, managing citations and cross-references, and so on. But the shoeleather work of \\(\\latex\\)—writing the boilerplate code to itemize and enumerate, control environments, typeset math—wasn’t something I was equipped to criticize very confidently. I would end up repeating myself a lot in my code, getting trapped inside of irritating design patterns, and feeling the age of the language constantly.\nBut you get older, you (hopefully) get better at programming, and you realize what you were being silly about.\nThis blog post is about how \\(\\mathit{\\LaTeX}\\) is a programming language. You can do ordinary programming language things with it, like save variables and write functions. And then you can turn those variables and functions into interfaces that let you work with greater efficiency. And you can design those interfaces to make your work more joyful.\nThinking about programming challenges in \\(\\latex\\) has been on my mind for a little while, but I worked up a quick example when Jordan Nafa was talking about color-coding different parts of some equations. So that will be our example for this post: color-coding an equation. This will not require deep \\(\\latex\\) expertise, knowledge of external packages, or other complicated tricks. We only have to a little ordinary programming thinking to turn \\(\\latex\\)’s built-in color tools into something more practical and friendly."
  },
  {
    "objectID": "blog/latex_colors/index.html#color-fundamentals-in-latex",
    "href": "blog/latex_colors/index.html#color-fundamentals-in-latex",
    "title": "Treating \\(\\mathrm{\\LaTeX}\\) like a programming language: the case of color-coded equations",
    "section": "Color fundamentals in \\(\\latex\\)",
    "text": "Color fundamentals in \\(\\latex\\)\nLike many things in \\(\\latex\\), the built-in experience of color-control is a little clunky. You change your text color with \\color{}, and naturally you pass some color argument. The language provides a few builtin keywords like red and blue, which are harsh, but others like violet, teal, and maroon are fine. But the color options aren’t the problem. The problem is the interface, which works like this.\nAny time you change a color, you change it indefinitely, at least up until there is a scope change that bounds the application of the new color. Here is an example:\n\nWe start with some normal text.\n\\color{red} And now the text is red indefinitely.\n\nWe can render this in the browser:1\n\\[\n\\text{\n    We start with some normal text.\n    \\color{red} And now the text is red indefinitely.\n}\n\\]\nYou may have encountered similar behavior with altering text size.\n\nWe start with some normal text.\n\\huge And now the text is huge indefinitely.\n\n\\[\n\\text{\n    We start with some normal text.\n    \\huge And now the text is huge indefinitely.\n}\n\\]\nWe can extert some control over the “indefinite” application of these settings by introducing some scope, for instance with curly braces.\n\nWe start with some normal text. \n{ \\color{red} But the } redness is { \\color{red} contained }.\n\n\\[\n\\text{\n    We start with some normal text.\n    {\\color{red} But the} redness is {\\color{red} contained}.\n}\n\\]\nThis isn’t the worst but it doesn’t feel too comfortable. It would feel better to control colors more like this:\n\\red{But the} redness is \\red{contained}\nwhich not only saves keystrokes but makes the whole experience feel more function-oriented and declarative. We will work toward something that feels similar (not identical) to this."
  },
  {
    "objectID": "blog/latex_colors/index.html#doing-better-with-an-equation-example",
    "href": "blog/latex_colors/index.html#doing-better-with-an-equation-example",
    "title": "Treating \\(\\mathrm{\\LaTeX}\\) like a programming language: the case of color-coded equations",
    "section": "Doing better, with an equation example",
    "text": "Doing better, with an equation example\nLet’s say we have an equation that has certain terms that we want to apply colors to. More specifically, we want to map certain semantic features of the equation’s meaning to specific colors in a consistent way. We would like the interface to be minimally burdensome. I don’t want to have to type (or think) too much to get good effects.\nLet’s meet our equation and its “semantics”. We collect data on some individuals subscripted \\(i\\), who are located within groups \\(g\\), and measured across time units \\(t\\). We model some outcome \\(y_{it}\\) as varying across individuals within groups and over time, \\[\\begin{align}\n    y_{it} &= \\alpha + \\mu_{i} + \\gamma_{g[i]} + \\tau_{t} + \\varepsilon_{it}\n\\end{align}\\] where \\(\\alpha\\) is a constant term, \\(\\mu_{i}\\) is a term that is fixed for an individual \\(i\\) across repeated observations, \\(\\gamma_{g[i]}\\) is a group-specific that is fixed across time for all \\(i\\) in group \\(g\\), \\(\\tau_{t}\\) is a time-unit effect that is fixed for all individuals, and \\(\\varepsilon_{it}\\) is random error. These units of measurement—units, groups, time periods—are the “semantics” that we want to map to colors.\nWe have already identified one problem: we don’t want colors to apply indefinitely. This means that in order to turn “off” a color, I either have to explicitly call \\color{black} again, or I have to scope the color e.g. with curly braces.\n\n% back to black\ny_{it} &= \\alpha + \\color{violet} \\mu_{i} \\color{black} \n          + \\gamma_{g[i]} + \\tau_{t} + \\varepsilon_{it} \\\\\n\n% use scope\ny_{it} &= \\alpha + {\\color{violet} \\mu_{i}} \n          + \\gamma_{g[i]} + \\tau_{t} + \\varepsilon_{it}\n\n\\[\\begin{align}\n    y_{it} &= \\alpha + \\color{violet} \\mu_{i} \\color{black} + \\gamma_{g[i]} + \\tau_{t} + \\varepsilon_{it} \\\\\n    y_{it} &= \\alpha + {\\color{violet} \\mu_{i}} + \\gamma_{g[i]} + \\tau_{t} + \\varepsilon_{it}\n\\end{align}\\]\nUsing the curly braces is definitely safer than \\color{black}; I don’t want to assume that we always want to return to black. But managing the curly braces yourself can be cumbersome if you aren’t used to writing \\(\\latex\\) that way already. I don’t write with that style, so I don’t want to burden myself with unusual patterns.2\nSo to improve things, we will introduce a function that, at first, will not feel like much of an improvement. But we discuss it to highlight both how we can modify interfaces with pretty simple tools and why we may want to do that. So, consider a function called setcolor, which takes two arguments: a color code and the text you want apply the color to locally.\n\n% notice the extra {} braces in the definition\n\\newcommand{\\setcolor}[2]{ {\\color{#1} {#2}} } \n\n% apply function to the equation\ny_{it} &= \\alpha + \\setcolor{violet}{\\mu_{i}} \n          + \\tau_{t} + \\varepsilon_{it}\n\n\\[\\begin{align}\n    \\newcommand{\\setcolor}[2]{ {\\color{#1} {#2}} }\n    y_{it} &= \\alpha + \\setcolor{violet}{\\mu_{i}} + \\gamma_{g[i]} + \\tau_{t} + \\varepsilon_{it}\n\\end{align}\\]\nWhy do I say that this function may not feel like much of an improvement? It has some drawbacks: it really isn’t any “faster” to type \\setcolor{violet}{\\mu_{i}} than it is to type {\\color{violet} \\mu_{i}}. It actually has more characters and just as many curly braces. But it is better in at least two important respects that we should care about when we write code. One, we made the problem of locally scoping the color inherent to the function instead of being procedurally managed ad hoc outside of the function. This is good because it makes the whole thing more bug-proof. It also makes the interface feel more naturally function-like: we achieve a coherent result by calling a function with a predictable interface, then our work is done. No managing other special characters in the language as a side concern. It is easier to remember one thing (use a function) than it is to remember two things (use a function oh and also manage the weird scope). So we get a safer function with a more recognizable interface. Not so bad!\nBut we aren’t done there. We complete the interface by using this function to map semantics to colors directly. We create a function called \\unitfx{} which applies the same color to any term in the equation that semantically refers to unit-level effects. Same for a functions called groupfx{}, \\timefx{}, and so on. We also throw in a generic \\param{} function for other terms.\n\n\\newcommand{unitfx}[1]{\\setcolor{violet}{#1}}\n\\newcommand{groupfx}[1]{\\setcolor{green}{#1}}\n\\newcommand{timefx}[1]{\\setcolor{lightblue}{#1}}\n\\newcommand{param}[1]{\\setcolor{maroon}{#1}}\n\nSo as long as we define these color commands in one place, all the hard work is done. All downstream calls to these functions are simple. Just wrap a term in the equation inside of the function corresponding to its semantic.\n\ny_{it} &= \\param{\\alpha} + \\unitfx{\\mu_{i}}\n          + \\groupfx{\\gamma_{g[i]}} + \\timefx{\\tau_{t}}\n          + \\param{\\varepsilon_{it}}\n\n\\[\\begin{align}\n    \\newcommand{unitfx}[1]{\\setcolor{violet}{#1}}\n    \\newcommand{groupfx}[1]{\\setcolor{green}{#1}}\n    \\newcommand{timefx}[1]{\\setcolor{orange}{#1}}\n    \\newcommand{param}[1]{\\setcolor{maroon}{#1}}\n    y_{it} &= \\param{\\alpha} + \\unitfx{\\mu_{i}} + \\groupfx{\\gamma_{g[i]}} + \\timefx{\\tau_{t}} + \\param{\\varepsilon_{it}}\n\\end{align}\\]"
  },
  {
    "objectID": "blog/latex_colors/index.html#conclusion",
    "href": "blog/latex_colors/index.html#conclusion",
    "title": "Treating \\(\\mathrm{\\LaTeX}\\) like a programming language: the case of color-coded equations",
    "section": "Conclusion",
    "text": "Conclusion\nSo that’s it. Our solution meaningfully improves the color experience in \\(\\latex\\) using just five lines of code: one to create a helper function and four more to create some color mappings. The helper function let us change the interface to color control in the general case. And the color mappings let us apply the new interface to create simple key-value pairs that map a semantic to a color. And bonus: the interface is also safer because managing the scope of a color modification requires no more work for the user.\nWe also do this without introducing any dependencies. We could still load outside packages to access more color values, but that choice doesn’t bear on the general interface or vice-versa. That modularity is a good design feature.\nAnd all because we thought about a \\(\\latex\\) problem like a programming problem."
  },
  {
    "objectID": "blog/usable_repos/index.html",
    "href": "blog/usable_repos/index.html",
    "title": "Replication code should be more usable.",
    "section": "",
    "text": "My mind has been on replication archives lately. I often go digging in other peoples’ projects for data to practice some new statistical skill or another. And I have been digging in my own projects lately to refactor some code for a dormant academic project. In both of these situations I am interfacing with some else’s code (me today ≠ me in the past), and in both situations I am having a bad time.\nThe academic community has been increasingly interested in replication archives since they realized that a lot of public research is, err, systematically untrustworthy. Formal requirements (from journals) and informal pressures (from other researchers) in academia are increasingly requiring authors to prepare these repositories for new projects, which seems good on the whole. But if you ever go digging in these republication archives, you quickly realize that just because authors provide data and code doesn’t mean that the experience is all that helpful. But why? You can see the data, maybe the code runs without errors…What’s the problem?\nMy main argument is that the code was not really designed to be understood or manipulated by other people. If it were, the code would not look the way it does.\nNow, I’m not an academic anymore, so don’t have much stake in this. But I do write quantitative research code in a collaborative environment at work all the time, with the intention that my code will be consumed and repurposed by others. As it turns out, writing code under these conditions changes how you write that code, improves the reliability of your work, and has positive effects on the community in which your code is deliberated and consumed. This blog post will attempt to distill some of the things I have learned into discrete, attainable lessons for writing academic research code.\nBefore I get too concrete though, I make a normative argument for caring about this at all. Changing the way code is written takes effort, and I want to argue why that effort would be justified."
  },
  {
    "objectID": "blog/usable_repos/index.html#getting-oriented",
    "href": "blog/usable_repos/index.html#getting-oriented",
    "title": "Replication code should be more usable.",
    "section": "Getting oriented",
    "text": "Getting oriented\n\nThis isn’t about “good” code vs. “bad” code.\nLike you, I don’t have a defensible theory about what makes code Good for any enduring understanding of Good. “Good” code seems like a contested and highly contextual idea. Code may have “good properties”, but certain features of code design also present important trade-offs, so goodness is understood relative to our goals. So maybe we should start by talking about what we want academic code to do for us.\n\n\nWhat is the point of the code repository for academic projects?\nI will discuss two broad models for code repositories, an archive model and a product model. I am making these up as I go, but you will get the idea.\nThe archive model. This is what most academic repositories are doing right now, I would guess. We call these code repositories “replication archives” because that’s what we think we want them to do: replicate and archive. The model is focused on the “validity” of the code—does it correctly reproduce the results in the writing, tables, and figures (replication) in a robust and enduring way (archiving).\nThese are important goals, but focusing exclusively on them has some predictable side-effects. The repositories end up serving as an audit trail for the paper, so the code is designed around the rhetoric of the paper instead of around the operations that the code performs. We see files named things like 01_setup, 02_clean-data, 03_model, 04_tables-figs, 05_robustness and so on. Even supposing that all of the code runs just fine, all we can really do with the code is rebuild the paper exactly as it is. If you could draw a web of all the interlocking pieces of this project, it would be a long, single strand from start to finish, highly dependent from one script to the next (perhaps intended to be run in the same R/Python session), with no feasible way to swap out any components for other components or modify them in isolation. And crucially, if we wanted to use the data or some of the code from this project for some other purpose, we would have to mangle the project in order to extract the components we wanted.\nThe product model. The product model organizes its code as if to offer it as a product for someone else to use. The code is a key part of the research package, and researchers care that it is a good experience, just as they care about the quality of their writing. An important assertion here is that there is value in the code even if the paper did not exist. If a project proposes a method to measure or estimate a key variable, those methods are valuable outside of the context of the written paper, so part of the product of the project is to make these tools available and usable for other researchers to benefit from. Projects that propose a new model make this model available for others to use with a practical interface. Indeed, there may be a notion of interface as distinct from infrastructure; the code that drives the project at a high level is uniform and understandable, and the ugly code that does heavy lifting might be encapsulated away from the interface. And better yet, the important infrastructure pieces are modularly designed: they may combine at the locus of the project, but the components can exist without reference to one another and could be exchanged for other components that perform similar functions.\n\n\nIncreasing the amount of shared / re-used code would be good for research credibility and knowledge accumulation\nPosting code online is not the same as making it usable. I suspect many social scientists put code online without really wanting others to dig into it. But there are real services that academics could provide for each other more regularly if they were more generous with the way their code is designed. Political scientists in particular use a lot of the same data sources and do a lot of similar transformations on it, but why all the wasted effort? I quite admire Steven V. Miller’s peacesciencer package for R, a set of tools whose rationale seems to be that there’s no sense in having everybody duplicate each other’s work to shape the data into common formats for analysis in political science. I gotta say, I agree.\nBut it doesn’t stop at broad-based datasets or data-shaping tools. Think about every paper you have ever read that proposes a new way to measure something. Did those authors provide a code module to do the appropriate calculations on new data of appropriate shape? Did those authors provide an interface for validating these measures against alternatives? I suspect that in most cases the answer is no. The authors may put their code online, but it isn’t online so that you can use it. Not really.\nI don’t want to point fingers, so I will use some examples from my own academic work to show that I, too, wasn’t thinking enough about this. In my dissertation, I built a group-level measurement model to estimate latent political ideology in partisan groups at subnational units of aggregation, and I wrote a series of Stan files to iterate on a few approaches to it. Other projects I have seen before and after I finished my thesis have built similar models. Did I package my models into a tool that they could use? No, I did not. I also implemented a reweighting method for aggregated survey data that I first saw in a paper from nearly a decade ago. This is probably a pretty broadly applicable correction for similar data, but did I provide a little module for others to apply the same calculations on data that wasn’t exactly my own? Nah. I designed a Bayesian implementation of the Acharya, Blackwell, and Sen formulation of sequential-g estimator that addresses some of the things that Bayesians would care about in situations like that, and I would even say I was pretty proud of it. But I wasn’t proud enough to share a generic version that others could use and adapt for their purposes.\nYou get the idea.\nIt just makes me worry that when we advertise our work as tools that others could use, we do not really mean it. I worry that phrases like “We propose a method…”, or “we provide an approach…” are only things we were trained to say to make it sound like we are contributing tools for the community. But we are not doing the work that would make those tools available for others. The code that goes into the paper repository is for ourselves, because the goal is getting our paper over the finish line. The code is just another part of the game.\nThere recently was a post on the Gelman blog that stuck out to me about cumulative science and re-using each other’s work. Here is an excerpt that gives you the idea:\n\nHow could a famous study sit there for 20 years with nobody trying to replicate it? […] Pamela said this doesn’t happen as often in biology. Why? Because in biology, when one research team publishes something useful, then other labs want to use it too. Important work in biology gets replicated all the time—not because people want to prove it’s right, not because people want to shoot it down, not as part of a “replication study,” but just because they want to use the method. So if there’s something that everybody’s talking about, and it doesn’t replicate, word will get out.\nThe way she put it is that biology is a cumulative science.\n\nThinking about how this applies to our code, it is clear that there is more than a vague moral value to posting usable code for others. There is scientific value. And it is interesting to me that after all the commotion about replication and running code without errors, there is comparatively little discussion about the scientific value of the code outside of the narrow, narrow context of the paper it is written for."
  },
  {
    "objectID": "blog/usable_repos/index.html#what-can-be-done",
    "href": "blog/usable_repos/index.html#what-can-be-done",
    "title": "Replication code should be more usable.",
    "section": "What can be done?",
    "text": "What can be done?\nNow that I am done complaining, we can talk about recommendations. I will focus mainly on two concepts, interface and modularity, from a couple different angles. Interface refers to the way other people use your tools. Modularity describes how those tools are combined for easier development (for your own benefit) and exporting (for others’ benefit).\n\nInterfaces for living projects, not memorials to dead projects\nWhenever I want to download a project’s data and code, I only want to get it from a version control platform like Github. I want to fork the project, see the history, get the intended file structure, and maybe even contribute to the project by opening issues or pull requests. I never want to go to Harvard Dataverse. I’m sure Dataverse was a great idea when it first hit the scene, but by today’s standards, it feels like it is chasing yesterday’s problems, like the problem of “pure replication”. But I think the credibility problems in social science warrant more than old-world-style replication. We should be looking for platforms that accommodate and encourage sharing, re-use, mutual contribution, and stress-testing by others.\n\n\nInterface vs. Infrastructure\nThis is a pretty common distinction you hear about in software communities. It isn’t much discussed in academic research circles.\nI will explain by way of example: Think about the tidyverse packages in R, or just the dplyr and tidyr packages. These packages provide an interface to useful data manipulations on data frames. These are operations on abstractions of your data—an abstraction in the sense that the functions do not have to know or care what is in those data frames in order to operate on them. The packages provide a uniform interface; they take a data frame as an input and return a data frame as an output, and the semantics are similar across functions. This makes the operations composable, which is jargon for “the operations can be reordered and combined to achieve powerful functionality”. The same basic principles are true for other tools in the tidyverse like stringr, forcats, purrr, and so on. They employ different abstractions for data organized at different levels (strings, factors, and lists respectively), but the emphasis on uniformity and composability is always there.\nSo that’s the “interface” layer. Now, what about the “infrastructure” or the implementation of these functions? Do you know anything about how these functions are actually written? And would it really matter if you did? The infrastructure isn’t what you, the user, care about. What matters is that the tools provide a simple way to perform key tasks with your code without bogging you down in the implementation details.\nCompare this to the way we write research code. There is usually no distinction between interface and infrastructure whatsoever. A lot of the time, we keep all of our nasty and idiosyncratic data-cleaning code right next to our analysis and visualization. On a good day, we may smuggle the data-cleaning code into a different file, but that doesn’t make a huge difference because the flow of the project is still mostly linear from raw data to analysis. The user cannot really avoid spending time in the darkest corners of the code.\nTo be fair, it isn’t necessary that an academic project’s code base should produce an end result as concptually gorgeous as tidyverse tools are. But there are probably some intertwined components in the research project that could be separated into interface and infrastructure layers somehow, and readers who really want to investigate the infrastructure are free to do so.1\nThinking again about a paper that proposes a new way to measure a key variable, or a new method to analyze some data: could those methods not be divided into an interface to access the method and an infrastructure that does the heavy lifting somewhere else? Wouldn’t you be more likely to use and re-use a tool like that? Wouldn’t it be more likely that if the method has a problem, we would have an easier time discovering and fixing it? Wouldn’t that look more like the iterative, communal scientific process that we wish we had?\n\n\nLittle functions, good and bad\nShould you make an interface by writing more functions? Annoyingly, it depends. One way to make an interface more legible to users is package annoying routines into functions that describe what you’re accomplishing. This is nominally easy to implement, but it isn’t always easy to design well. And the design considerations present plenty of trade-offs with no obvious guiding theory.\nTake a halfway concrete example. You have a dataset of administrative data on many individuals, and the dataset has a field for individuals’ names. Your task is to clean these names in some way.\nYou choose to use some combination of regular expressions and stringr to do this. But how do you implement this operation in the code? One way is to create a new variable like…\n\nnew_df <- mutate(\n    df, \n    name = \n        str_replace(...[manipulation 1 here]...) |>\n        str_replace(...[manipulation 2 here]...) |>\n        [...]\n        str_replace(...[final manipulation here]...)\n)\n\n…and this works fine. But if you wanted to change the way this name-cleaning is done, not only do you have to do surgery directly on a data pipeline, but you can’t test your new implementation without re-running this mutate step over and over (which may be only one step of an expensive, multi-function pipe chain, but that is a separate, solvable issue).\nConsider instead the possibility of writing a function called format_names that implements your routine. Now your data pipeline looks like this…\n\nnew_df <- mutate(df, name = format_names(name))\n\nWell, should you do that? The routine is now encapsulated, so if you need to do it more places, you can call the function without rewriting the steps.2 This makes your routine unit-testable: does it clean the patterns you want it to clean (and you are, of course, limited by the patterns you can anticipate). It also makes it a little easier to change your implementation in one location and achieve effects in many places without doing surgery to your data pipeline. Maybe it also makes it easy to move this step in your data pipeline around, which is good.\nAnd what are the drawbacks? Well, you no longer know what the function is doing without hunting down the implementation, and it’s possible that the implementation is idiosyncratic to your project instead of being broadly rules-based. In general, encapsulating code into a function makes it easier to “drive” code, but it doesn’t inherently have any effect on whether your code is operating at a useful level of abstraction. Nor does it have any obvious effect on whether the interface you design for one function is at all related to the interface you create for other functions—uniformity makes your functions easier to use and combine for powerful results. If you aren’t careful, you might write ten different functions that don’t share a uniform abstraction or interface, so now it takes more effort for you to remember how your functions work than it does to write it out using stringr. After all, stringr is already built on familiar abstractions: stringr don’t know what your string is, and it does not care. All it knows is that it has functions with similar semantics for doing operations on strings.\nSo, you have to think about what you want to accomplish if you want to have an effective design.\n\n\nMore modules, fewer pipelines\nSo far I have been a little incredulous toward the idea that your code should be a “pipeline” for your data. This is because pipelines are often in conflict with modularity: the principle of keeping things separate, independent, and interchangeable. A lot of academic projects are lacking in it.\nIt is difficult at first to realize the drawbacks of non-modular pipeline organization because, especially with tidyverse tools, chaining many things together to do big things is the primary benefit. When dplyr and tidyr first start clicking for you, you immediately begin chaining tons of operations together to go from A to B without a ton of code. But as you iterate on the analysis, which requires breaking and changing things in your code, it can suddenly be very cumbersome to find and fix broken parts. This is because you have twisted all of the important steps together! You wrote the code in such a way that steps that do not depend on one another in principal now have to be organized and executed in a strict order. And now the fix for one breakage leads you to break something else because the steps in your pipeline are not abstracted. It is just nice to avoid problems like this.\nTo be clear, this is not the tidyverse’s fault. The tidyverse is an exemplar of modular design. The problem is that you tried to string too much together without thinking about how to keep things smartly separated. We should be asking ourselves questions like,\n\nWhat operation (read: function) do I need to do here, regardless of what my particular data look like? That is, think more about functionality and less about state.\nWhat can be separated? For instance, do I need the analysis of these 3 different data sources to happen in the same file? Or are they unrelated to the point where the analyses don’t have to be aware of one another in any way? Again, functionality without state.\nWhat can be encapsulated? Suppose I convince myself that the analyses of my 3 different data sources don’t have to be aware of one another, but maybe there are some tools I can define that could be useful in each analysis. Perhaps I should encapsulate those tools under a separate file (read: a module!) and then import that module whenever I need it. This lets me both keep things separate without repeating the same steps in many places. We should be skeptical of encapsulation for its own sake—it isn’t always necessarily helpful—but in this case it helps us separate functionality from state.\nCorrespondingly, what can be exported to other files in this project, or to other users who might want to use only that part? This is probably helpful to remember for cleaned data, repeated data manipulation tools, a new model that you build, and more.\n\nIf more pieces of your project are separable, interchangeable, and exportable, it becomes much easier to share little pieces of your project with other researchers.\n\n\nThere’s organization, and then there’s organization\nWe know that our code should be organized, but it is easy to organize code ineffectively. Something we see a lot in research code is an organizational structure that is perfectly discernible but not exactly useful. I have created plenty of impeccably “organized” but unhelpful repositories.\nOne discernible but not-very-useful organizational pattern is to name your files as steps along a data analysis pipeline: 01_setup, 02_clean-data, 03_model, 04_tables-figs, 05_robustness, and so on. Again, guilty. This setup doesn’t help the researcher keep themselves “organized” if every problem they have to solve is tangled together in one bucket ambiguously labeled “data cleaning”. And it doesn’t help anyone consuming the project understand or modify which pieces of the code are responsible for which functionality.\nBut if you are being a good citizen—designing modularity into your code, creating a useful interface, dividing functionality across files, separating analyses that don’t depend on one another, and so on—the pipeline organization will not be the dominant feature. It will naturally be replaced by a layout that reflects the concepts in the code, not the rhetorical contours of a research paper. The code is not a paper, and a paper is not its code.\nTo be sure, you cannot kill every little pipeline; data always need to be read, shaped, and analyzed. But building your code on effective abstractions lets you write smaller pipelines that are quicker from start to finish, conceptually simpler, and untangled from other independent ingredients in your project. The ultimate goal is usable code, not killing pipelines. It just happens that small, minimal pipelines are more usable than gigantic, all-encompassing pipelines.\n\n\nJudicious usage of “literate programming”\nMany researchers agree that statistical output should be algorithmically injected into research writeups to ensure the accuracy of the reporting. This is the real reason to use document prep systems like Rmarkdown or (increasingly) Quarto: not for the math typesetting.\nBut it is also a problem if the statistical work is so intertwined with the manuscript that they cannot be severed in the code. I myself have written plenty of “technically impressive” Rmarkdown documents that are actually are fragile Jenga towers of tangled functionality.\nThis is a pretty easy lesson though. The .Rmd file shouldn’t be the place where important, novel analysis happens. That should happen elsewhere, and your paper should be epiphenomenal to it.\n\n\nWho’s looking at your code?\nYou learn a lot by having other people make suggestions to you about the way your code is structured. I suspect most academic projects, even those with multiple co-authors, don’t feature much code criticism or review of any kind. Outsourcing code tasks to graduate students is great for developing their skills, but if you (the person reading this) are the veteran researcher, they would also benefit from your advice. The student and your project will be better for it.\n\n\nDon’t propose methods. Provide them.\nI have said this already, but I wanted to sloganize it."
  },
  {
    "objectID": "blog/usable_repos/index.html#better-code-vs.-the-academy",
    "href": "blog/usable_repos/index.html#better-code-vs.-the-academy",
    "title": "Replication code should be more usable.",
    "section": "Better code vs. the Academy",
    "text": "Better code vs. the Academy\nThis section could be its own blog post, but I am not an academic anymore and do not want to spend too much energy arguing about stuff that I intentionally left behind. But in case you are thinking about the following things, I just want you to know what I see you, I hear you, but I think these are the questions you have to answer for yourselves.\n\nWriting better code will not help my paper get published. Yeah, that sucks, right? Why is that? If we can agree that more usable code will make it easier to share tools among researchers, test-drive the contributions of other researchers, and bolster the credibility of your field overall, why does your publication model think that it’s a waste of your time as a researcher?\nThis all sounds very hard, and I am not a software engineer. You shouldn’t have to be a software engineer if you do not want to be. What I do wish is for your field to sustain broader collaboration where you have a team of people who are good at different things so your field can put out more reliable products. But many fields care about superstar researchers. I do not care about superstar researchers."
  },
  {
    "objectID": "blog/usable_repos/index.html#what-we-havent-said",
    "href": "blog/usable_repos/index.html#what-we-havent-said",
    "title": "Replication code should be more usable.",
    "section": "What we haven’t said",
    "text": "What we haven’t said\nHere’s a recap of all the things we did not mention about replication / archiving discussions:\n\nPackaging environments and dependencies\nRelatedly, containerization\nSolutions for long-term archiving\nCommenting code / other documentation\n\nI think these problems are important to varying degrees. I mean, I am not as concerned about containerization, but you are free to be. But these issues are more commonly discussed than the design, usability, and shareability of code. So I didn’t talk about them."
  },
  {
    "objectID": "blog/modular_blog/index.html",
    "href": "blog/modular_blog/index.html",
    "title": "Highly modular blogging with Blogdown",
    "section": "",
    "text": "When I finished graduate school, I tore down my website.\nFor a handful of reasons. I no longer needed a website that cried out, “Help me, I’m finishing my PhD and I need to escape.” I didn’t need to showcase unpublished papers, teaching resources, or old blog posts that I had grown detached from. It was time for a clean reset.\nBut if you work with Blogdown, you know that starting over is laborious. Not that Blogdown isn’t great, because it is. It’s that, when you’re a finicky person like me, setting up a website with the right balance of capable features, pleasant aesthetics, and a principled codebase is legitimately challenging. I was encountering the same familiar challenges over and over.\nFor example, the site’s Hugo theme. I would take a lot of Hugo themes for test-drives. Hugo advertizes themes as if they were completely modular with respect to your /content/ folder. For most themes, this is a lie. Themes usually want too many bespoke variables or file structures in your website content. Some amount of this is okay, but it comes at a cost. If you really want to take a theme for a spin, I would find it easier to create an entirely new blogdown::new_site() than to change my theme in an existing site.\nExcept now you’re dragging the same files around your computer all over again. Ugh, this new website directory needs your blog source files, your website-level .Rprofile that controls Blogdown’s build behaviors, the jpg/png image files that you use to brand yourself online, etc… And maybe these files need to go in different folders or be given different file names from the previous theme. After a while, these files no longer have a single authoritative “home” on your computer, and you may have multiple conflicting(!) versions of these files across your various experimental website folders.\nAnd then there’s reproducibility. Even after lugging around all the same files to the new site, good luck getting your blog posts to render if your package library has changed since they were written, which it probably has. Danielle Navarro wrote about reproducibility in Rmarkdown blogging, and argues convincingly that the best way to protect your .Rmarkdown posts from this rebuilding risk is to create a dedicated package library for each separate blog post using renv. This sounds intense at first, but the underlying principle is simple, which makes it a good solution to a difficult problem.\nThis post will continue that pattern: intense at first, but well-founded, solutions for difficult problems."
  },
  {
    "objectID": "blog/modular_blog/index.html#what-this-post-is-about-modularity",
    "href": "blog/modular_blog/index.html#what-this-post-is-about-modularity",
    "title": "Highly modular blogging with Blogdown",
    "section": "What this post is about: modularity",
    "text": "What this post is about: modularity\nWhat we want is a principled and robust approach for managing the many interlocking components of your website. Specifically, we explore the modularity of the elements in your website. I take the view that your website is a collection of modular components that are better managed independently, with different Git repositories for the different site components. Yes, managing your website with multiple repositories. Stay with me.\nThe modules that compose your website include your theme, your blog posts, your Blogdown build preferences (implemented in your .Rprofile), and maybe more. These modular components come together at the nexus of the website, but as I will argue, these components should not belong to the website. Why not? Because these components can be re-used across different websites or substituted with other similar components. Hugo already flirts with this idea in its core design by separating the /theme/ directory from the /content/ directory, as if to say, “these components can be combined but do not depend on one another.” This post takes an opinionated stance that such modularity is a good idea and should be assertively extended to other components of your Blogdown site. That said, I make no assertion that this stance is objectively correct—only that it has been useful enough for me that I wanted to share some thoughts about the principles and processes at work. (You should do what works for you!)\nModularity as a software philosophy is one thing, but implementing it in code requires technical solutions. This post will discuss how to achieve this using Git submodules, an intermediate-level Git construct that, if you’re like me, is somewhat familiar but somewhat intimidating. In short, a Git submodule is a repository-within-a-repository. It has its own version history that is distinct from the “parent” repository. In this post, I provide a simple tour of submodules and how they can be used to structure your website workflow. We will recast our website as the “primary” repository, and we import other modular site components (like our blog posts) as Git submodules. In case you host your blog on Netlify, I will also discuss how to ensure that Netlify can build your site successfully.\n\nAside: some terminology\nThis discussion will involve plenty of concepts that sound similar to one another but should be understood as distinct things. I want to flag these concepts so that we understand each other better.\nDirectory vs. repository. A directory is a folder on your computer that holds files. A (Git) repository tracks changes to files. For many projects, the project’s root directory is entirely managed by one repository, so the distinction between the two may be blurred. When Git submodules are involved, this is no longer true. Your website directory will be managed by one repository, and sub-directories below your website will be managed by other repositories.\nWebsite vs. module. The website is the entire project that puts your website online. Your website will contain various modules that combine to build the entire project. Your blog posts will be considered a module (or several modules, depending on your implementation). Your theme is another module. Think of modules as building blocks for your website that can be stacked, swapped out, and so on.\nParent repository (for the website) vs. child repository (for the module), a.k.a. “submodule”. The website and the module will be versioned by separate repositories. We can refer to the over-arching project repo (the website) as the “parent” repo and the module repo as the “child” repo. A “Git submodule” is a Git construct that is overlaid onto this relationship between repositories. A repository, in isolation, is simply a repository. But if you import a repository into another project as a dependency, Git designates the dependency as a “submodule” to the parent repository, and this affects our Git workflow as a result. I explain all of that below."
  },
  {
    "objectID": "blog/modular_blog/index.html#websites-are-a-collection-of-modules",
    "href": "blog/modular_blog/index.html#websites-are-a-collection-of-modules",
    "title": "Highly modular blogging with Blogdown",
    "section": "Websites are a collection of modules",
    "text": "Websites are a collection of modules\nModules are like little building blocks, and your website has plenty of them. Setting aside any formal definition of what would mathematically be considered a “module”, let’s crudely define them as structures in your website that are agnostic to the content of other structures. We may be able to replace modules with other modules, or remove modules entirely, without affecting the core function of other modules.\nHere are some examples from my own workflow. I consider my blog posts, Hugo theme, and blogdown build settings (in my site-level .Rprofile) as modular components within the website as a whole, and I version each component with its own separate repository. Here is how I justify this view for each component:\n\nBlog posts: The content of a blog post is completely separable from the website repo. We can take a blog post and locate it in a different website, and the blog post should still be meaningful (and reproducible) unto itself. Many blogdown users remake their websites and carry their old blog posts to the new sites, which shows that the blog content doesn’t functionally depend on the website.\nIt turns out that, for blog posts, modularity and reproducibility are pretty closely related. In her discussion of blog reproducibility, Danielle Navarro touched on the principle that a blog should be “encapsulated” or “isolated” away from the broader website to robustify the blog against other dependencies. By insisting that blog posts also be modular, not only is the blog protected from the website’s computational environment, we can control each post independently of one another, move posts around across contexts, and remove posts entirely without side-effects.\nThis also affects how we treat the blog post’s dependencies. Suppose that your post includes an analysis on a data file that you read from disk. This file should belong to your blog post—and be versioned by that blog post’s Git repository—not your website. This means you should keep all of these files in the blog post directory, and forget about the website’s /static/ folder except for files that rightfully belong to the website.\nHugo theme: Hugo is designed such that the /content/ of a website (specified in markdown files) is more-or-less independent of its /theme/. The same theme can be used for multiple websites, and a single website can (in theory1) swap out one theme for another. Because themes are managed with Git repositories already, you can pull theme updates from their remote repositories without overwriting any bespoke theme customizations specified in your /layouts/ folder.\nBlogdown complicates this somewhat. When you install a theme with blogdown::install_theme(), Blogdown actually deletes the theme’s .git directory. (At least, this was my experience.) This is probably for ease-of-use among users who would not appreciate having to manage the theme as a submodule. But we are enthusiastic seekers of modularity, so we want to keep that upstream remote connection alive. As such, I installed my site’s Hugo theme using Git submodule operations instead of installing it with blogdown::install_theme().\n\n\nThe website .Rprofile file: You may have a global .Rprofile file, but it is an increasingly common Blogdown workflow staple to set up a website-specific .Rprofile to control Blogdown’s build behavior. How is this a module? Your blogdown build preferences are probably not specific to this website repository. Instead, it is likely that your preferences reflect your workflow for blogging in general and could be equally applicable to any other website repo you create or manage. If you change your blogdown workflow in a way that bears on this .Rprofile file, that change may affect all of your blogdown websites equally! Managing these .Rprofiles separately for each website would be inefficient and error-prone, so instead we manage the .Rprofile in one repository that we import to our website as a submodule."
  },
  {
    "objectID": "blog/modular_blog/index.html#how-to-accomplish-this-git-submodules",
    "href": "blog/modular_blog/index.html#how-to-accomplish-this-git-submodules",
    "title": "Highly modular blogging with Blogdown",
    "section": "How to accomplish this: Git submodules",
    "text": "How to accomplish this: Git submodules\nGit submodules are repositories-within-repositories. Suppose you are working on a project repository (like your website), and there are external tools or resources that you want to import from another project. You have a strong project-based workflow, so you want all of the code that creates your website to be contained within the website directory on your computer. At the same time, the external dependency is clearly its own entity, and there is no reason why its code should be owned by the website repository. Git submodules allow you to clone this dependency repo into your website directory so you can use this code without versioning it redundantly.\n\nSubmodule basics\nIf you have never worked with submodules before, here is how they work in broad strokes. (This is not an exhaustive intro.)\nWhen you add a submodule to a parent repository, the parent repository tracks the presence of the submodule, but it does not track the content. Your website repo tracks the presence of submodules to ensure that your project can be reproduced (read: cloned) with all necessary dependencies in place.2 However, your website repo is ignorant of the actual content of the submodule because the submodule code is versioned by its own separate repo. There is no need to duplicate that effort.\nUpstream changes to the submodule repo can be pulled into your website repo. This is standard workflow for Git. If you want to pin your dependency to a particular commit of the submodule, you can git checkout that commit. If you want your dependency to stay dynamically up to date with the submodule’s remote repo, checkout the desired branch and pull changes as they arise on the upstream remote.\nLocal changes to the submodule content can be pushed to remote. If you have write access to the submodule’s remote repository—either you own the repo, or it’s your fork of some other repo—you can make changes to the submodule contents from within the submodule and push those changes back upstream.3 This is just like a Git workflow where multiple users are pushing to the same remote repository, except instead of multiple users, it’s only you, editing the repo and committing/pushing changes from different endpoints. This allows you to keep the submodule content updated on all of its local and remote copies without duplicating any effort.\n\n\nHow to add your website components as submodules\nIn the spirit of modularity, there is actually nothing Blogdown-specific about including submodules within a project repository. All the same, I will discuss a Blogdown-specific example: the .Rprofile module, which I keep in its own repository here. I discuss how I manage blog posts with submodules later on, because that conversation is a little more involved.\nYou can add a submodule to your (already initialized) website repo with git submodule add [my-url] [my-destination-folder]. You will want to be strategic about where you add the repo, since it will effectively behave like a cloned repository. I often create a /submodules/ folder under my project root and clone submodules to that location.\n# from /path/to/site\nmkdir submodules\ncd submodules\ngit submodule add git@github.com:mikedecr/dots_blogdown.git\nAdding the submodule does not clone its contents. It simply registers the submodule with the repository, creating an entry in the website repo’s .gitmodules file. You have to run a separate command to actually clone the submodule repo’s contents:\ngit submodule update --init --recursive\nThe output will look like you did a git clone. At this point, there should exist a folder called /dots_blogdown/ that contains the repo contents.\nFrom there, your next step depends on how you want to use the contents of the submodule. For this particular example, we want this .Rprofile to live at the top of our website root. This ensures that the file’s code is executed when we open R to manage our website. I achieve this by linking the file to the website root (and, bonus, removing write permissions4).\n# exit /submodules/\ncd ..\n# -s = symlink, -f = force\nln -f ./submodules/dots_blogdown/.Rprofile ./.Rprofile\n# bonus: remove write-permissions (make read-only)\nchmod -w ./.Rprofile\nIt is smart to automate any post-Git processes, such as linking files to other destinations, by putting these commands and other pre-build operations in your website’s /R/build.R file. This ensures that these operations are done each time your website is built, ensuring that your website can be safely reproduced if your submodule content should ever change. With that automation in place, if I ever changed my .Rprofile repo, I never have to worry about manually re-linking my updates to the right destination. The build script does it for me.\n\n\nDeveloping within the submodule repo\nThe above instructions describe how to simply employ submodule files in your website. But suppose you wanted to change the content of the submodule files and push those changes back upstream. What would you do?\nBefore making any changes to the submodule files, make sure the submodule isn’t in detached HEAD state. A detached HEAD state is basically what happens when you have checked out a commit in isolation of the branch on which that commit lives. When you are in detached HEAD state, you are basically looking at a copy of the project, but you cannot alter the project tree itself. Any files you change cannot be committed to a persistent branch. To make permanent changes, you have to checkout the branch that you want to track and commit changes to, which is probably main.\nMake your changes. Even though you are editing a file within a submodule repository, Blogdown doesn’t know or care, so it shouldn’t behave any differently. It will knit/render blog posts and serve your website locally like nothing is wrong. That’s because nothing is wrong.\nCommit changes to submdodule files to the submodule repository. From the command line, this means you probably should cd into the submodule repo before adding any files to the index. If you do Git stuff inside of a GUI, you should be able to make the submodule appear as its own repo that you can do add/commit/push actions to. (I don’t use Rstudio, so unfortunately I don’t know if Rstudio makes this easy.) After committing to your local copy of the submodule repo, you should notice that your parent repository detects an updated commit in the submodule! You should commit that change to the parent repository as well. This simply tells the parent repo that it should consult this new submodule to reproduce the project correctly. This is important because anyone else who clones your website repository (ahem, Netlify!) will need to import the submodule at the correct commit.\nBoth submodule and parent repos can be pushed. If this is your first time pushing any submodule-related commits to Netlify, you will want to read the section about Netlify below.\nAs you get more familiar with Git, you won’t need to follow a checklist. You will simply be familiar enough with how Git works to know exactly what to do!"
  },
  {
    "objectID": "blog/modular_blog/index.html#blog",
    "href": "blog/modular_blog/index.html#blog",
    "title": "Highly modular blogging with Blogdown",
    "section": "What to do about your blog?",
    "text": "What to do about your blog?\nShould your blog be one submodule repository, or several? My current setup is to treat every blog post as its own, separate module with its own, separate repository. This keeps each post and all of its dependencies isolated from other posts, which is cleanest for me from a reproducibility and modularity standpoint.\nHowever, you may find many blog post repositories to be overkill, and would instead want a single repository containing all of your blog posts. Would that be fine?\nIn short, the single-blog-module setup may be possible, but it will likely require even more advanced Git magic than just submodules. If you really want to know the nasty technical details, you can read about the problem and one potential solution, with the caveat that I haven’t tested that workflow out. If you trust me that the single-repo workflow is pretty complicated except for people looking to increase their Git dexterity stats, you can skip ahead to read about separate repositories for each post.\n\nOne submodule for all posts: the problem\nTo explain, consider the submodule workflow mentioned earlier. If we wanted to use a “single submodule” approach to blogging, we would\n\nMove our blog posts to another repository and push it to the web.\nAdd this repository as a submodule located in your content/blog or analogous subdirectory.\nThe changes in the content/blog folder are now owned by the submodule repository. The parent repo will no longer see what’s happening in those files—only if you have made new commits.\n\nUnfortunately, this may be a critical problem for your website! This is because many themes ask you to put other important files under your content/blog directory, in addition to the posts. Many popular themes ask for a content/blog/_index.md file to manage the blog’s “listings” page. Many themes also will accept image files in that directory to use for headers and sidebars on the listings page. These files are problems for the single-repo workflow. If we let our blog submodule own the content/blog directory, those files can no longer be tracked by the parent (website) repository. Adding the files to the submodule’s .gitignore does not fix it either. So, what can be done?\n\n\nOne submodule for all posts: there might be a way\nI haven’t tested this, but there might be a way to save the unified-blog-repository workflow: you could make your blog repository a bare repository.\nA bare repository is a repository with no root directory. Now, if you have only used Git on a per-project basis, the idea of a repo with no root directory sounds unthinkable, but it is actually a common way to version your “dotfiles”. Here’s why: your dotfiles usually live at your /home/username/ or ~/ directory. Many folks want to track these files to keep certain preferences synchronized on different machines, but as you can foresee, making a Git repository track your entire ~/ folder would be a horrible and terrifying idea. Instead, people create bare repositories that only track the contents that are explicitly added to the repository, regardless of their relative location to the repo’s .git folder.\nHow might this ameliorate our workflow problem? If we want one submodule repo to track all of the posts in /content/blog, but we don’t want that repo to own the other files in that directory, we might be able to achieve that effect with a bare submodule repo. Such a repo shouldn’t be aware of the other files under content/blog, because the repo doesn’t know that it is the same folder as those files.\nAgain, try it if you want, but you have no assurances from me that it will work.\n\n\nMy choice: every post gets its own repository\nIn lieu of the “advanced solution”, we opt for peak modularity: every blog post gets its own repository.\nThis workflow sounds tedious but is actually easier than you would think, and most of the steps are identical to what I have already covered above. Here’s a quick rundown of what I do:\n\nStart on Github or whichever remote service you prefer. Make a remote-first repository for a new post (give it a meaningful title) and copy its cloning link.\nAdd the new repo as a submodule to a new folder for that post inside of /content/blog. I assume you use a “page bundle” model for organizing your blog code: separate folders for each post that contain respective index.[R]markdown files. It’s advisable to blog with page bundles even if you don’t want to implement hyper-modular blog versioning. Learn more about page bundles from Alison Hill here.\nYour .gitmodules file will automatically update to reflect the new submodule. You will eventually want to commit that change, but it doesn’t have to be now. If necessary, initialize/update the submodule to clone its contents into the new post directory.\nCheckout your desired submodule branch (e.g. main) so you can commit changes to your blog repo.\nEdit your post as you normally would by creating an index.Rmarkdown and typing away. This is where you would use renv to take a snapshot of your R package library in order to reproduce the post. Hugo will trip over the files created by renv, however, so if you want to use it (again, you should), add \"renv\" to the ignoreFiles field in your website’s config.toml (which you only have to do once per site).\nCommit changes to the blog module repository and push to remote.\nYou should notice that your parent repository detects an updated commit in the submodule. Commit that change to the parent repository as well. Pushing this website commit to remote will kick off a new Netlify build if you use continuous integration. Speaking of that…"
  },
  {
    "objectID": "blog/modular_blog/index.html#netlify-setup",
    "href": "blog/modular_blog/index.html#netlify-setup",
    "title": "Highly modular blogging with Blogdown",
    "section": "Getting it working with Netlify",
    "text": "Getting it working with Netlify\nOnce you are done getting your site looking the way you want, and all of your files are committed to the parent and child repositories, you can push your website repo to the remote that Netlify is tracking.\nExcept, whoops, your site may fail to build on Netlify. Why? Netlify works by cloning your website repository to their servers and building it with Hugo on their end. This process fails if Netlify can’t successfully reproduce your website repo with all of the submodules declared in your .gitmodules file. Such failure can happen for two benign and fixable reasons: (1) the submodule is a private repository, or (2) the submodule was added using the repo’s ssh URL instead of the https URL.\nIn either case, all you have to do is add ssh-keys to grant Netlify access to these repositories. It sounds complicated and jargony, but Netlify describes the whole process right here.\nOnce Netlify has access to the repositories, it can build its own copy of your website. This is because your parent Git repository spells out all of the instructions for cloning the required submodules at their requested commits."
  },
  {
    "objectID": "blog/modular_blog/index.html#closing-note",
    "href": "blog/modular_blog/index.html#closing-note",
    "title": "Highly modular blogging with Blogdown",
    "section": "Closing note",
    "text": "Closing note\nThis post presents an opinionated view of a Blogdown website as a collection of modules and a corresponding workflow for managing them. If you find it helpful, awesome! But as always, you should do what works for you. It happened to be the case that I had a particular set of problems and a desire to strengthen some skills could help me solve them."
  },
  {
    "objectID": "blog/fp_basics/index.html",
    "href": "blog/fp_basics/index.html",
    "title": "More than lapply: functional programming adventures in R",
    "section": "",
    "text": "When people talk about “functional programming” in R, they usually mean two things.\nBy the numbers, most R users are probably more familiar with the meaning of [1] than [2], which is a shame because functional paradigms outside of R go way deeper, and get way weirder, than apply(function, data). This post will be an attempt to explain a little of this weirdness and implement it in R.\nReaders who are experienced with functional programming will recognize that this post doesn’t get so detailed. They may also realize that underneath R’s interfaces are a lot of functional ideas, especially the tidyverse’s “tidy evaluation”. But this post keeps the material fairly light."
  },
  {
    "objectID": "blog/fp_basics/index.html#what-is-functional-programming",
    "href": "blog/fp_basics/index.html#what-is-functional-programming",
    "title": "More than lapply: functional programming adventures in R",
    "section": "What is functional programming?",
    "text": "What is functional programming?\nLet’s do the Wikipedia thing. From the functional programming article:\n\nIn computer science, functional programming is a programming paradigm where programs are constructed by applying and composing functions.\n\nYou may be thinking, “That sounds pretty unremarkable. I already know that I write functions and apply them. Why do we need a name for this?”\nWell… read on:\n\nIt is a declarative programming paradigm in which function definitions are trees of expressions that map values to other values, rather than a sequence of imperative statements which update the running state of the program.\n\nThere are a lot of information-dense parts of that sentence that may be difficult to appreciate without knowing more of the theory. But let’s try to break it apart. Starting in the middle.\n\n“Functions map values to other values”\nYou have probably seen function definition written like this,\n\\[\ny = a + bx\n\\]\nand we say that \\(y\\) is a function of \\(x\\). We can label the transformation of \\(x\\) as \\(f\\) and say \\(y = f(x)\\). Easy!\nBut there is another way to write statements like this that you may have seen in papers that have high opinions of their own notation:\n\\[\nf : X \\mapsto Y\n\\tag{1}\\]\nwhich reads, “\\(f\\) is a map from \\(X\\) to \\(Y\\)”, where capital-\\(X\\) and capital-\\(Y\\) are sets of values from which the single values \\(x\\) and \\(y\\) are drawn. Which is to say: \\(f\\) is simply a thing that eats a value from \\(X\\) and spits out a value from \\(Y\\). In this example, the value of \\(y\\) that it returns is \\(a + (b \\times x)\\).2\n\n\n“Function definitions are trees of expressions …”\nReferring to functions as “trees” won’t make sense until we talk about function composition and associativity.\nFirst, composition. If the notation in Equation 1 is new to you, your mental picture of function composition probably looks like this. We can take two functions \\(f\\) and \\(g\\)… \\[\\begin{align}\n    y &= f(x) \\\\\n    z &= g(y)\n\\end{align}\\] and put them together…\n\\[ \\begin{align} z &= g(f(x)) \\end{align}  \\tag{2}\\] and then we can represent that composition with a different symbol \\(h\\), so that that \\(h(x)\\) is equivalent to \\(g(f(x))\\).\nThere’s nothing incorrect about that approach, but it is verbose. We actually don’t have to refer to any function arguments or values (x$, \\(y\\), or \\(z\\)) to introduce \\(h\\). Instead we can define \\(h\\) “point-free”: \\[ h = g \\circ f  \\tag{3}\\]\nwhere the symbol \\(\\circ\\) refers to function composition (in a leftward direction). The expression \\(g \\circ f\\) is a new function, which we can speak aloud as “\\(g\\) compose \\(f\\)” or “\\(g\\) after \\(f\\)”. Because the composition is itself a function, we can pass it an argument: \\((g \\circ f)(x)\\) would spit out the value \\(z\\), just like in Equation 2.\nOkay, next point. Associativity. Function composition is associative, which means compositions can be “grouped” in whatever way you want as long as you don’t rearrange the ordering of the base functions.\nTo demonstrate, let’s write some functions out in “map” notation, and for the sake of simplicity let’s say that each is a map from a real number to a real number. \\[\\begin{align}\n    \\newcommand{\\R}{\\mathbb{R}}\n    f &: \\R \\mapsto \\R \\\\\n    g &: \\R \\mapsto \\R \\\\\n    j &: \\R \\mapsto \\R\n\\end{align}\\] Now we want to compose these functions. Because composition is associative, we could write the composition many ways:\n\nAs one single chained composition: \\(j \\circ g \\circ f\\), which if applied to \\(x\\) would be equivalent to \\(j(g(f(x)))\\).\nIntroduce \\(h = g \\circ f\\) as above, and rewrite as \\(j \\circ h\\). This is the same as \\(j \\circ (g \\circ f)\\) or \\(j(h(x))\\).\nCompose \\(j\\) and \\(g\\) into some \\(d\\) and say \\(d \\circ f\\), which is the same as \\((j \\circ g) \\circ f\\) or \\(d(f(x))\\).\n\nAll of these expressions are equivalent.\nOkay, recap. As long as the output of one map is the same “type” as the input to the some other map, we can compose those two functions. And we can take a series of compositions and group them in whatever way we want, as long as the input and output types conform.\nBut how is this a “tree”? I will make a little graph with nodes and edges that shows the basic idea. Caveats up front that this will be pretty informal, but let’s say the functions are nodes, and the edges point in the direction of composition. Here is an example showing \\(a = j \\circ g \\circ f\\) as a tree.\n\n\n\n\n\n\n\nF\n\n  \n\nj\n\n j   \n\na\n\n a   \n\nj->a\n\n    \n\ng\n\n g   \n\ng->a\n\n    \n\nf\n\n f   \n\nf->a\n\n   \n\n\n\n\n\nThe tree lets us express associativity in a different way: we can pre-compose any two functions by creating another node from them.\n\n\n\n\n\n\n\nF\n\n  \n\nj\n\n j   \n\na\n\n a   \n\nj->a\n\n    \n\nh\n\n h   \n\nh->a\n\n    \n\ng\n\n g   \n\ng->h\n\n    \n\nf\n\n f   \n\nf->h\n\n   \n\n\n\n\n\nWe can see that the composition is associative because the composition of \\(f\\) and \\(g\\) into \\(h\\) doesn’t change the order in which functions are applied to form \\(a\\). We can “pre-group” functions in the tree however we want, but we can’t change their ordering except in special circumstances. Composition is associative, but not all functions are commutative.\n\n\n“…Rather than a sequence of imperative statements that update the running state.”\nThis is the really important stuff.\nMost of the time when we do data analysis, we write code that updates the state of some data. Say I start with some \\(x\\), and then I do functions \\(f\\), \\(g\\), and \\(j\\) to it.\n\ny = f(x)\nz = g(y)\nw = j(z)\n\nThere is some data x that we alter, in steps, by doing things to it, each time saving some data at some intermediate state.\nThe functional approach would be different. Instead of spending most of our time, energy, and keystrokes passing data around, we spend these resources writing functions.\n\nh = compose(g, f)\na = compose(j, h)\n\nAnd only later (say, in some main() routine) do we apply our ultimately-composed functions to data. The data have to be acted on eventually, but we do it only after we compose a map from the data to some endpoint.\nStated a different way, when we do more imperative programming, the present state of x is always our concern. Our path from beginning to end requires us to leave x in some intermediate state at many different steps, and the present state of x can causes a lot of problems for us as we try to plot our way to our destination. When we do functional programming, however, we simply write the recipe for what we will do to x before we touch x at all. We prefer not to let the intermediate state become visible. The intermediate state is, after all, not that useful.\nThis might be hard to envision because we haven’t invented functional programming yet, so let’s invent it."
  },
  {
    "objectID": "blog/fp_basics/index.html#functional-fundamentals-with-r",
    "href": "blog/fp_basics/index.html#functional-fundamentals-with-r",
    "title": "More than lapply: functional programming adventures in R",
    "section": "Functional fundamentals, with R",
    "text": "Functional fundamentals, with R\nWe saw a notational approach to function composition above that looked like this: \\[\\begin{align}\na = (j \\circ g \\circ u)\n\\end{align}\\] If we could do that in a computer, it might look like this:\n\na = compose(j, g, f)\n\nThere are two important things to remember about this.\n\nFunction composition creates a new function. It does not immediately create data. In the example, a is the composition, which is only evaluated on data when it is necessary. It may not be evaluated on data at all directly—we will likely compose a with another function before we even think about data. If you have heard of delayed evaluation, this is probably why. You were doing something that created new functionality instead of new data.\nFunction compositions are themselves composable, so we want a framework to compose a lot of functions all at once with whatever associative groupings we want. We saw above (with math) that we could compose an arbitrary sequence of functions and the result should be well-behaved (as long as their input and output types conform—more on that in a bit). We would like to achieve that behavior in the code as well.\n\nWe start with a primitive operation to compose just two functions. We call it compose_once—it implements only one composition of a left-hand and a right-hand function.\n\n# returns the new function: f . g\ncompose_once = function(f, g) {\n    function(...) f(g(...))\n}\n\nRead this closely. compose_once takes two arguments, and those arguments are functions. We do not know or care what those functions are. We also return a new function, rather than some value. That new function defines a recipe for evaluating the functions \\(f\\) and \\(g\\) in sequence on some unknown inputs ..., whever it happens to come across those inputs—we didn’t pass any ... yet. This is also on purpose. Composition does not care what the data are. It only knows how to evaluate functions in order.\nThis might feel weird at first, but it will give us legible behavior right from the start. For example, we often want to know how many unique elements are in some data object. In base R, we might ask length(unique(x)). The dplyr package provides a combined function n_unique(x), but we haven’t invented dplyr yet, so we have to make n_unique ourselves:\n\n# we save the function returned by compose_once\n# no computation on data is done yet.\nn_unique = compose_once(length, unique)\n\n# examine the object to convince yourself of this\nn_unique\n## function(...) f(g(...))\n## <environment: 0x131ad8390>\n\n# apply the function on some data\nx = c(0, 0, 0, 1, 2)\nn_unique(x)\n## [1] 3\n\ncompose_once lets us compose two functions, but we already know that function composition lets us compose however many functions we want (as long as the output type from one function conforms with the input type of next function). So we extend this to an arbitrary series of functions by performing a reduce across an array of functions.\n\n# pass vector fns = c(f1, f2, ..., fn)\ncompose = function(fns) {\n    Reduce(compose_once, fns, init=identity)\n}\n\nIf you aren’t familiar with reductions, they are an efficient trick to collapse an array (loosely speaking) of input values into one output value, accumulating at each step the results of some binary operation. If you want to learn more about reduction, follow this footnote.3 We also supply an argument to init, which isn’t strictly necessary in this R example but is interesting (IMO) in the mathematical context of function composition, which I explain in this other footnote.4\nLet’s see it in action. Let’s do an additional step to convert n_unique into an English word.\n\n# same as n_unique but convert to a word\nenglish_n_unique = compose(c(english::english, length, unique))\n\n# apply to our vector x\nenglish_n_unique(x)\n## [1] three\n\nAnd just to underscore your confidence in the associativity of composition, we can exhaustively group these compositions without affecting the results.\n\ncompose(c(english::english, compose(c(length, unique))))(x)\n## [1] three\ncompose(c(compose(c(english::english, length)), unique))(x)\n## [1] three\n\n\nTypes are our guide.\nYou may have seen online discussions about “strong typing”. Types are representations of data in the computer. We have basic data types like short integers, long integers, characters, and strings. Languages implement other data structures like lists, dictionaries, arrays, tuples, etc. that we can also call types for our purposes.\nYou may have seen some people discuss the “type flexibility” of R or Python as advantages. Well…sorry! Functional programming like strong types because strong types provide structure to function composition at scale. Functions can be composed if the output type of one function is the input type of the next function. If we can trust this fact, we can build really big, abstract creations out of function composition, maybe even creations that are so big and complicated that we struggle to keep track of it all in our brains. But these creations are virtually guaranteed to work as long as they are composed of functions with conforming input and output types.\nNot coincidentally, the associativity of function composition is additionally helpful for API design. If we have to build a big, abstract structure, we always have the option to group some of the operations together if they perform a coherent and useful operation. Intermediate functions are more useful than intermediate state because at least an intermediate function is legible and potentially reusable. Chances are your intermediate data are not that legible or useful.\nLet’s look at the “type roadmap” for the english_n_unique example. We started with a vector type and mapped to an english type. How did we know it would work? We can diagram the types.\n\\[\n\\begin{align}\n   \\mathtt{unique} &: \\mathtt{many} \\mapsto \\mathtt{vector} \\\\\n   \\mathtt{length} &: \\mathtt{many} \\mapsto \\mathtt{integer} \\\\\n   \\mathtt{english::english} &: \\mathtt{many} \\mapsto \\mathtt{vector}\n\\end{align}\n\\]\nWe know that unique takes objects of various types (which I represent as many to say “this function has many methods for different data types”) and returns a vector of unique elements. We know that length takes many and returns its integer length, and that should work for a vector input. And engligh::english turns many objects into english type, which ought to work for an integer input. But we know that these operations should compose because we can know that we can pass a vector to unique, a vector to length, and an integer to english::english. Stated differently, if I know the type behavior of each function, I know the type behavior of the compositions. That is an extremely useful and powerful foundation for building big things."
  },
  {
    "objectID": "blog/fp_basics/index.html#something-bigger-means-within-groups",
    "href": "blog/fp_basics/index.html#something-bigger-means-within-groups",
    "title": "More than lapply: functional programming adventures in R",
    "section": "Something bigger: means within groups",
    "text": "Something bigger: means within groups\nThis is where we really start to see the difference between typical “imperative” programming and functional programming “trees”. Let’s take something we commonly do in R, calculate means within groups of data, and do it with our functional tools. Let’s take the mtcars data frame…\n\nhead(mtcars, 5)\n##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n\n…and calculate the mean of each variable as a new data frame with one row per group. We want to be able to specify the group on the fly.\nWe want to chart a course through this problem that composes small, re-usable steps on well-defined types. Here is my plan:\n\ndefine a function that maps a data frame of raw data to a data frame of column means.\ndefine a function to any function to groups of data\ncompose a function that implements a split-apply-combine which is the composition of the above steps.\n\n\n1. Means for a data frame.\nWe create a function by composing the following functions that map the following types:\n\ncolMeans: data frame \\(\\mapsto\\) vector\nas.list: vector \\(\\mapsto\\) list\ntibble::as_tibble: list \\(\\mapsto\\) data_frame\n\nYou can see how these functions take us incrementally from data frame, to vector, to list, back to data frame. So we these operations should compose nicely.\n\nmeans = compose(c(tibble::as_tibble, as.list, colMeans))\n\nTesting it on the full data:\n\nmeans(mtcars)\n## # A tibble: 1 × 11\n##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n##   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1  20.1  6.19  231.  147.  3.60  3.22  17.8 0.438 0.406  3.69  2.81\n\n\n\n2. Apply a function over groups\nRemember, functional programming hasn’t been invented yet, so we can’t simply do his with lapply. We can, however, reimplement lapply knowing what we know about functional programming.\nLet’s create an function that takes any function f and an iterable object l, and returns a list.\n\napply_on_elements = function(l, f) {\n    # initialize an empty list\n    v = vector(mode = \"list\", length = length(l))\n    # assign f(x) for each x in l\n    for (x in seq_along(l)) {\n        v[[x]] = f(l[[x]])\n    }\n    return(v)\n}\n\nNotice, this function takes two objects as arguments and returns one object. In order to nicely compose it with other functions that take and return only one object, I want a way to reduce the arguments required when I call it. This is a little weird, but I am going to create something like a Python tool called a partial, which takes a function f and returns a new function f*, which is just like f but with some of the arguments pre-filled. Let’s define it before I explain further:\n\npartial_apply = function(f) {\n    function(x) apply_on_elements(x, f)\n}\n\nSo partial_apply takes a function f and returns a new function. That new function applies f to some iterable object x to return a list. But the function isn’t evaluated; it is only created, because I never provide an x in the outer function. The user has to pass x to evaluate the partial at a later time.\nHere we use this tool to create a partial length, which we apply to some x.\n\nlens = partial_apply(length)\n\n# examine, it's a function\nlens\n## function(x) apply_on_elements(x, f)\n## <environment: 0x1313aa000>\n\nlens(x)\n## [[1]]\n## [1] 1\n## \n## [[2]]\n## [1] 1\n## \n## [[3]]\n## [1] 1\n## \n## [[4]]\n## [1] 1\n## \n## [[5]]\n## [1] 1\n\nWe see more of that delayed evaluation behavior. This lets us create a function that applies our earlier means to groups of data.\n\npartial_apply(means)\n## function(x) apply_on_elements(x, f)\n## <bytecode: 0x13198cfd0>\n## <environment: 0x1318be400>\n\n\n\n3. Apply our partial function to data.\nRather, create a function that would do that, if it were evaluated.\nThis function should a data frame into an iterable collection of groups (like a list), apply means to each element of that collection, and return a final data frame that re-merges the collection. Here’s how we map these steps from type to type.\n\nsplit: pair of (data frame, vector) \\(\\mapsto\\) list\npartial_apply(means): list \\(\\mapsto\\) list\ndplyr::bind_rows: list \\(\\mapsto\\) data frame\n\nThe composition of all these steps creates a function that eats a data frame and returns a data frame.\n\nmeans_by = compose(c(dplyr::bind_rows, partial_apply(group_means), split))\n\n\n\nPut it all together.\nThe code below retraces our steps. Step (1) creates our means function. Step (2) creates some functional infrastructure to apply functions over iterable objects, which isn’t the kind of thing we would ordinarily have to mess with as end-users. And step (3) composes our means function with the iteration tools to make our eventual result.\n\n# 1. small function to be applied\nmeans = compose(c(tibble::as_tibble, as.list, colMeans))\n\n# 2. infrastructure layer:\n# you can see why these would be generally useful for many problems\n# 2a. reimplement *-apply()\napply_on_elements = function(l, f) {\n    v = vector(mode = \"list\", length = length(l))\n    for (x in seq_along(l)) {\n        v[[x]] = f(l[[x]])\n    }\n    return(v)\n}\n# 2b. partial apply\npartial_apply = function(f) {\n    function(x) apply_on_elements(x, f)\n}\n\n\n# 3. interface layer\nmeans_by = compose(c(dplyr::bind_rows, partial_apply(means), split))\n\nAgain, given that step (2) is rebuilding the wheel, it’s pretty impressive how little code goes into steps (1) and (3) to achieve the end results. The interface currently isn’t as succinct as dplyr grouping and summarizing, but remember, functional programming hasn’t been invented yet. But moreover, you can start to imagine how tools like partials can be stacked to create tools that are essentially as powerful as dplyr with nice interfaces too, even if those interfaces are different from the imperative steps you are used to.\nLet’s apply the ultimate function, means_by, to various groups in mtcars.\n\nmeans_by(mtcars, mtcars$cyl)\n## # A tibble: 3 × 11\n##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n##   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1  26.7     4  105.  82.6  4.07  2.29  19.1 0.909 0.727  4.09  1.55\n## 2  19.7     6  183. 122.   3.59  3.12  18.0 0.571 0.429  3.86  3.43\n## 3  15.1     8  353. 209.   3.23  4.00  16.8 0     0.143  3.29  3.5\nmeans_by(mtcars, mtcars$vs)\n## # A tibble: 2 × 11\n##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n##   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1  16.6  7.44  307. 190.   3.39  3.69  16.7     0 0.333  3.56  3.61\n## 2  24.6  4.57  132.  91.4  3.86  2.61  19.3     1 0.5    3.86  1.79\nmeans_by(mtcars, mtcars$am)\n## # A tibble: 2 × 11\n##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n##   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1  17.1  6.95  290.  160.  3.29  3.77  18.2 0.368     0  3.21  2.74\n## 2  24.4  5.08  144.  127.  4.05  2.41  17.4 0.538     1  4.38  2.92\nmeans_by(mtcars, mtcars$gear)\n## # A tibble: 3 × 11\n##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n##   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1  16.1  7.47  326. 176.   3.13  3.89  17.7 0.2   0         3  2.67\n## 2  24.5  4.67  123.  89.5  4.04  2.62  19.0 0.833 0.667     4  2.33\n## 3  21.4  6     202. 196.   3.92  2.63  15.6 0.2   1         5  4.4\nmeans_by(mtcars, mtcars$carb)\n## # A tibble: 6 × 11\n##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n##   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1  25.3  4.57  134.   86   3.68  2.49  19.5   1   0.571  3.57     1\n## 2  22.4  5.6   208.  117.  3.70  2.86  18.2   0.5 0.4    3.8      2\n## 3  16.3  8     276.  180   3.07  3.86  17.7   0   0      3        3\n## 4  15.8  7.2   309.  187   3.60  3.90  17.0   0.2 0.3    3.6      4\n## 5  19.7  6     145   175   3.62  2.77  15.5   0   1      5        6\n## 6  15    8     301   335   3.54  3.57  14.6   0   1      5        8\n\n\n\nNotes on the tidyverse\nNow that we have invented functional programming, we can better appreciate how tidyverse tools leverage functional infrastructure to make nice APIs, even if those APIs feel way less hardcore-functional than the example we just created.\nThe pipe operator. You may have thought to yourself, composing “left” sure is harder to read than composing “right” like the pipe operator. First, functional programming hadn’t been invented yet, so you can’t blame me for not knowing about the pipe operator. Second, creating a composition function that reads more “linearly” is easy with one extra step: reversing the direction of the Reduce.\n\npipe = function(fns) {\n    # reverse the order of fns before composing\n    compose(rev(fns))\n}\n\nNow we can write our means_by function with more linear recipe that reminds us more of tidyverse code.\n\nmeans_by = pipe(c(split, partial_apply(means), dplyr::bind_rows))\nmeans_by(mtcars, mtcars$cyl)\n## # A tibble: 3 × 11\n##     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n##   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n## 1  26.7     4  105.  82.6  4.07  2.29  19.1 0.909 0.727  4.09  1.55\n## 2  19.7     6  183. 122.   3.59  3.12  18.0 0.571 0.429  3.86  3.43\n## 3  15.1     8  353. 209.   3.23  4.00  16.8 0     0.143  3.29  3.5\n\nPartial functions. The partial_apply function might have been the weirdest-feeling step in the earlier example. But if you squint at it, you realize that this is sort of what the tidyverse does with tidy evaluation. Whenever you pipe a data frame to mutate or filter and so on, and you write expressions on unquoted variables, those arguments are (in a way) creating new partial functions. There is also delayed evaluation of that function: the unquoted expressions are not evaluated on the spot, but instead are translated to create the partial function you actually want. It is that translated/partial function that actually is evaluated when you pass it your data frame."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "michael decrescenzo",
    "section": "",
    "text": "— background —\nI have a Ph.D. in political science from the University of Wisconsin–Madison, where I was a graduate affiliate of the Elections Research Center. My graduate work focused on Bayesian + causal methods for studying U.S. elections and public opinion.\n\n\n— lately —\nSome things I have been interested in:\n\nSoftware design, especially functional programming\nCategory theory, in service of functional programming\nFrequentist statistics, which isn’t how I usually do things\nNeovim and lua-based configuration"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\nAug 31, 2022\n\n\nTreating \\(\\mathrm{\\LaTeX}\\) like a programming language: the case of color-coded equations\n\n\n\n\nAug 17, 2022\n\n\nMore than lapply: functional programming adventures in R\n\n\n\n\nMay 15, 2022\n\n\nReplication code should be more usable.\n\n\n\n\nNov 6, 2021\n\n\nHighly modular blogging with Blogdown\n\n\n\n\n\n\nNo matching items"
  }
]